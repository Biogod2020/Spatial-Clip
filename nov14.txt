===== setup.py =====
# Êñá‰ª∂Ë∑ØÂæÑ: diffusion_project/git_repo/yuanspace/setup.py

from setuptools import setup, find_packages

setup(
    name="open-clip-torch",
    version="2.20.0-dev",  # ‰ΩøÁî®‰∏Ä‰∏™Ëá™ÂÆö‰πâÁâàÊú¨Âè∑‰ª•Á§∫Âå∫Âà´
    author="Yuanspace Custom Version",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        # Âú®ËøôÈáåÂèØ‰ª•Ê∑ªÂä†ÊÇ®È°πÁõÆ‰æùËµñÁöÑÂåÖÔºå‰ΩÜÂØπ‰∫éÂèØÁºñËæëÂÆâË£ÖÈÄöÂ∏∏‰∏çÊòØÂøÖÈúÄÁöÑ
    ],
)

===== .pre-commit-config.yaml =====
default_language_version:
  python: python3

repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      # list of supported hooks: https://pre-commit.com/hooks.html
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-docstring-first
      - id: check-yaml
      - id: debug-statements
      - id: detect-private-key
      - id: check-executables-have-shebangs
      - id: check-toml
      - id: check-case-conflict
      - id: check-added-large-files

  # python code formatting
  - repo: https://github.com/psf/black
    rev: 23.1.0
    hooks:
      - id: black
        args: [--line-length, "99"]

  # python import sorting
  - repo: https://github.com/PyCQA/isort
    rev: 5.12.0
    hooks:
      - id: isort
        args: ["--profile", "black", "--filter-files"]

  # python upgrading syntax to newer version
  - repo: https://github.com/asottile/pyupgrade
    rev: v3.3.1
    hooks:
      - id: pyupgrade
        args: [--py38-plus]

  # python docstring formatting
  - repo: https://github.com/myint/docformatter
    rev: v1.7.4
    hooks:
      - id: docformatter
        args:
          [
            --in-place,
            --wrap-summaries=99,
            --wrap-descriptions=99,
            --style=sphinx,
            --black,
          ]

  # python docstring coverage checking
  - repo: https://github.com/econchick/interrogate
    rev: 1.5.0 # or master if you're bold
    hooks:
      - id: interrogate
        args:
          [
            --verbose,
            --fail-under=80,
            --ignore-init-module,
            --ignore-init-method,
            --ignore-module,
            --ignore-nested-functions,
            -vv,
          ]

  # python check (PEP8), programming errors and code complexity
  - repo: https://github.com/PyCQA/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args:
          [
            "--extend-ignore",
            "E203,E402,E501,F401,F841,RST2,RST301",
            "--exclude",
            "logs/*,data/*",
          ]
        additional_dependencies: [flake8-rst-docstrings==0.3.0]

  # python security linter
  - repo: https://github.com/PyCQA/bandit
    rev: "1.7.5"
    hooks:
      - id: bandit
        args: ["-s", "B101"]

  # yaml formatting
  - repo: https://github.com/pre-commit/mirrors-prettier
    rev: v3.0.0-alpha.6
    hooks:
      - id: prettier
        types: [yaml]
        exclude: "environment.yaml"

  # shell scripts linter
  - repo: https://github.com/shellcheck-py/shellcheck-py
    rev: v0.9.0.2
    hooks:
      - id: shellcheck

  # md formatting
  - repo: https://github.com/executablebooks/mdformat
    rev: 0.7.16
    hooks:
      - id: mdformat
        args: ["--number"]
        additional_dependencies:
          - mdformat-gfm
          - mdformat-tables
          - mdformat_frontmatter
          # - mdformat-toc
          # - mdformat-black

  # word spelling linter
  - repo: https://github.com/codespell-project/codespell
    rev: v2.2.4
    hooks:
      - id: codespell
        args:
          - --skip=logs/**,data/**,*.ipynb
          # - --ignore-words-list=abc,def

  # jupyter notebook cell output clearing
  - repo: https://github.com/kynan/nbstripout
    rev: 0.6.1
    hooks:
      - id: nbstripout

  # jupyter notebook linting
  - repo: https://github.com/nbQA-dev/nbQA
    rev: 1.6.3
    hooks:
      - id: nbqa-black
        args: ["--line-length=99"]
      - id: nbqa-isort
        args: ["--profile=black"]
      - id: nbqa-flake8
        args:
          [
            "--extend-ignore=E203,E402,E501,F401,F841",
            "--exclude=logs/*,data/*",
          ]

===== requirements.txt =====
absl-py==2.3.0
aiohappyeyeballs==2.6.1
aiohttp==3.12.12
aiosignal==1.3.2
anndata==0.11.4
anyio==4.9.0
argon2-cffi==25.1.0
argon2-cffi-bindings==21.2.0
array-api-compat==1.12.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.5
attrs==25.3.0
babel==2.17.0
beautifulsoup4==4.13.4
bleach==6.2.0
braceexpand==0.1.7
certifi==2025.4.26
cffi==1.17.1
charset-normalizer==3.4.2
comm==0.2.2
contourpy==1.3.2
cycler==0.12.1
debugpy==1.8.14
decorator==5.2.1
defusedxml==0.7.1
executing==2.2.0
fastjsonschema==2.21.1
filelock==3.18.0
fonttools==4.58.4
fqdn==1.5.1
frozenlist==1.7.0
fsspec==2025.5.1
ftfy==6.3.1
grpcio==1.73.0
h11==0.16.0
h5py==3.14.0
hf-xet==1.1.5
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.33.0
idna==3.10
igraph==0.11.9
iniconfig==2.1.0
ipykernel==6.29.5
ipython==9.3.0
ipython_pygments_lexers==1.1.1
ipywidgets==8.1.7
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.6
joblib==1.5.1
json5==0.12.0
jsonpointer==3.0.0
jsonschema==4.24.0
jsonschema-specifications==2025.4.1
jupyter==1.1.1
jupyter-console==6.6.3
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.8.1
jupyter_server==2.16.0
jupyter_server_terminals==0.5.3
jupyterlab==4.4.6
jupyterlab-indent-guides==0.1.0
jupyterlab_execute_time==3.2.0
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
jupyterlab_widgets==3.0.15
kiwisolver==1.4.8
legacy-api-wrap==1.4.1
leidenalg==0.10.2
llvmlite==0.44.0
Markdown==3.8.2
markdown-it-py==3.0.0
MarkupSafe==3.0.2
matplotlib==3.10.3
matplotlib-inline==0.1.7
mdurl==0.1.2
mistune==3.1.3
mpmath==1.3.0
multidict==6.4.4
natsort==8.4.0
nbclient==0.10.2
nbconvert==7.16.6
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.5
notebook==7.4.3
notebook_shim==0.2.4
numba==0.61.2
numpy==2.2.6
nvidia-cublas-cu12==12.6.4.1
nvidia-cuda-cupti-cu12==12.6.80
nvidia-cuda-nvrtc-cu12==12.6.77
nvidia-cuda-runtime-cu12==12.6.77
nvidia-cudnn-cu12==9.5.1.17
nvidia-cufft-cu12==11.3.0.4
nvidia-cufile-cu12==1.11.1.6
nvidia-curand-cu12==10.3.7.77
nvidia-cusolver-cu12==11.7.1.2
nvidia-cusparse-cu12==12.5.4.2
nvidia-cusparselt-cu12==0.6.3
nvidia-nccl-cu12==2.26.2
nvidia-nvjitlink-cu12==12.6.85
nvidia-nvtx-cu12==12.6.77
overrides==7.7.0
packaging==25.0
pandas==2.3.0
pandocfilters==1.5.1
parso==0.8.4
patsy==1.0.1
pexpect==4.9.0
pillow==11.2.1
platformdirs==4.3.8
pluggy==1.6.0
prometheus_client==0.22.1
prompt_toolkit==3.0.51
propcache==0.3.2
protobuf==6.31.1
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==21.0.0
pycparser==2.22
Pygments==2.19.1
pynndescent==0.5.13
pyparsing==3.2.3
pytest==7.2.0
pytest-split==0.8.0
python-dateutil==2.9.0.post0
python-json-logger==3.3.0
pytz==2025.2
PyYAML==6.0.2
pyzmq==27.0.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.4
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rich==14.1.0
rpds-py==0.25.1
safetensors==0.5.3
scanpy==1.11.2
scikit-learn==1.7.0
scipy==1.16.0
seaborn==0.13.2
Send2Trash==1.8.3
sentencepiece==0.2.0
session-info2==0.1.2
setuptools==80.9.0
six==1.17.0
sniffio==1.3.1
soupsieve==2.7
stack-data==0.6.3
statsmodels==0.14.4
sympy==1.14.0
tensorboard==2.19.0
tensorboard-data-server==0.7.2
terminado==0.18.1
texttable==1.7.0
threadpoolctl==3.6.0
tinycss2==1.4.0
tokenizers==0.21.1
torch==2.7.1
torch-geometric==2.6.1
torchaudio==2.7.1
torchvision==0.22.1
tornado==6.5.1
tqdm==4.67.1
traitlets==5.14.3
transformers==4.52.4
triton==3.3.1
types-python-dateutil==2.9.0.20250516
typing_extensions==4.14.0
tzdata==2025.2
umap-learn==0.5.7
uri-template==1.3.0
urllib3==2.4.0
wcwidth==0.2.13
webcolors==24.11.1
webdataset==0.2.86
webencodings==0.5.1
websocket-client==1.8.0
Werkzeug==3.1.3
wheel==0.45.1
widgetsnbextension==4.0.14
yarl==1.20.1

torch>=2.0
torchvision
webdataset>=0.2.5,<=0.2.86
regex
ftfy
tqdm
pandas
braceexpand
huggingface_hub
safetensors
transformers[sentencepiece]
timm>=1.0.17
fsspec

pytest-split==0.8.0
pytest==7.2.0
transformers[sentencepiece]
timm>=1.0.17

torch>=2.0
torchvision
regex
ftfy
tqdm
huggingface_hub
safetensors
timm
===== environment.yaml =====
name: spatial-clip-env
channels:
  - conda-forge
dependencies:
  - python=3.12   # Ê†πÊçÆÊÇ®ÁöÑmicromamba listÔºåÊÇ®ÁöÑPythonÁâàÊú¨ÊòØ3.12
  - uv            # Êàë‰ª¨ËÆ©condaÊù•ÂÆâË£ÖuvËøô‰∏™Â∑•ÂÖ∑
===== view_result.ipynb =====
# ===== Cell 1: Code (execution_count: 2) =====
from pathlib import Path
import re

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import yaml
from IPython.display import display

# Identify the latest multirun folder, aggregate overrides and tracked metrics, then plot them.
LOG_ROOT = Path("logs/train/multiruns")


def get_latest_multirun(root: Path) -> Path:
    runs = [d for d in root.iterdir() if d.is_dir()]
    if not runs:
        raise FileNotFoundError(f"No multirun directories under {root}")
    return max(runs, key=lambda d: d.stat().st_mtime)


def parse_overrides(run_dir: Path) -> dict:
    override_path = run_dir / ".hydra" / "overrides.yaml"
    overrides: dict[str, object] = {}
    if override_path.exists():
        raw_entries = yaml.safe_load(override_path.read_text()) or []
        for raw in raw_entries:
            if isinstance(raw, str) and "=" in raw:
                key, value = raw.split("=", 1)
                try:
                    overrides[key] = yaml.safe_load(value)
                except yaml.YAMLError:
                    overrides[key] = value
    return overrides


_METRIC_RE = re.compile(r"Retrieved metric value!\s*<([^=]+)=([0-9eE+\.\-]+)>")


def parse_metrics(run_dir: Path) -> dict:
    metrics: dict[str, float] = {}
    log_path = run_dir / "train.log"
    if log_path.exists():
        for line in log_path.read_text().splitlines():
            match = _METRIC_RE.search(line)
            if match:
                metrics[match.group(1)] = float(match.group(2))
    return metrics


latest_multirun = get_latest_multirun(LOG_ROOT)
rows: list[dict[str, object]] = []
for subdir in sorted(latest_multirun.iterdir()):
    if not subdir.is_dir() or not subdir.name.isdigit():
        continue
    overrides = parse_overrides(subdir)
    metrics = parse_metrics(subdir)
    if metrics:
        rows.append({"run": int(subdir.name), "run_dir": str(subdir), **overrides, **metrics})

if not rows:
    raise RuntimeError(f"No metrics captured in {latest_multirun}")

df = pd.DataFrame(rows).sort_values("run").reset_index(drop=True)
display(f"Latest multirun: {latest_multirun.name}")
display(df)

metric_cols = [c for c in df.columns if "/" in c]
if metric_cols:
    sns.set_theme(style="whitegrid")
    id_vars = ["run"] + [col for col in ("loss",) if col in df.columns]
    melted = df.melt(id_vars=id_vars, value_vars=metric_cols, var_name="metric", value_name="value")
    hue = "loss" if "loss" in melted.columns else "run"
    plt.figure(figsize=(8, 5))
    sns.barplot(data=melted, x="metric", y="value", hue=hue)
    plt.title(f"Metrics for multirun {latest_multirun.name}")
    plt.ylabel("Metric value")
    plt.tight_layout()
    plt.show()
else:
    print("No slash-delimited metric columns found in the aggregated dataframe.")


# ===== Cell 2: Code =====



===== README.md =====
<div align="center">

# Lightning-Hydra-Template

[![python](https://img.shields.io/badge/-Python_3.8_%7C_3.9_%7C_3.10-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)
[![black](https://img.shields.io/badge/Code%20Style-Black-black.svg?labelColor=gray)](https://black.readthedocs.io/en/stable/)
[![isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/) <br>
[![tests](https://github.com/ashleve/lightning-hydra-template/actions/workflows/test.yml/badge.svg)](https://github.com/ashleve/lightning-hydra-template/actions/workflows/test.yml)
[![code-quality](https://github.com/ashleve/lightning-hydra-template/actions/workflows/code-quality-main.yaml/badge.svg)](https://github.com/ashleve/lightning-hydra-template/actions/workflows/code-quality-main.yaml)
[![codecov](https://codecov.io/gh/ashleve/lightning-hydra-template/branch/main/graph/badge.svg)](https://codecov.io/gh/ashleve/lightning-hydra-template) <br>
[![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)
[![PRs](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/ashleve/lightning-hydra-template/pulls)
[![contributors](https://img.shields.io/github/contributors/ashleve/lightning-hydra-template.svg)](https://github.com/ashleve/lightning-hydra-template/graphs/contributors)

A clean template to kickstart your deep learning project üöÄ‚ö°üî•<br>
Click on [<kbd>Use this template</kbd>](https://github.com/ashleve/lightning-hydra-template/generate) to initialize new repository.

_Suggestions are always welcome!_

</div>

<br>

## üìå¬†¬†Introduction

**Why you might want to use it:**

‚úÖ Save on boilerplate <br>
Easily add new models, datasets, tasks, experiments, and train on different accelerators, like multi-GPU, TPU or SLURM clusters.

‚úÖ Education <br>
Thoroughly commented. You can use this repo as a learning resource.

‚úÖ Reusability <br>
Collection of useful MLOps tools, configs, and code snippets. You can use this repo as a reference for various utilities.

**Why you might not want to use it:**

‚ùå Things break from time to time <br>
Lightning and Hydra are still evolving and integrate many libraries, which means sometimes things break. For the list of currently known problems visit [this page](https://github.com/ashleve/lightning-hydra-template/labels/bug).

‚ùå Not adjusted for data engineering <br>
Template is not really adjusted for building data pipelines that depend on each other. It's more efficient to use it for model prototyping on ready-to-use data.

‚ùå Overfitted to simple use case <br>
The configuration setup is built with simple lightning training in mind. You might need to put some effort to adjust it for different use cases, e.g. lightning fabric.

‚ùå Might not support your workflow <br>
For example, you can't resume hydra-based multirun or hyperparameter search.

> **Note**: _Keep in mind this is unofficial community project._

<br>

## Main Technologies

[PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) - a lightweight PyTorch wrapper for high-performance AI research. Think of it as a framework for organizing your PyTorch code.

[Hydra](https://github.com/facebookresearch/hydra) - a framework for elegantly configuring complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line.

<br>

## Main Ideas

- [**Rapid Experimentation**](#your-superpowers): thanks to hydra command line superpowers
- [**Minimal Boilerplate**](#how-it-works): thanks to automating pipelines with config instantiation
- [**Main Configs**](#main-config): allow you to specify default training configuration
- [**Experiment Configs**](#experiment-config): allow you to override chosen hyperparameters and version control experiments
- [**Workflow**](#workflow): comes down to 4 simple steps
- [**Experiment Tracking**](#experiment-tracking): Tensorboard, W&B, Neptune, Comet, MLFlow and CSVLogger
- [**Logs**](#logs): all logs (checkpoints, configs, etc.) are stored in a dynamically generated folder structure
- [**Hyperparameter Search**](#hyperparameter-search): simple search is effortless with Hydra plugins like Optuna Sweeper
- [**Tests**](#tests): generic, easy-to-adapt smoke tests for speeding up the development
- [**Continuous Integration**](#continuous-integration): automatically test and lint your repo with Github Actions
- [**Best Practices**](#best-practices): a couple of recommended tools, practices and standards

<br>

## Project Structure

The directory structure of new project looks like this:

```
‚îú‚îÄ‚îÄ .github                   <- Github Actions workflows
‚îÇ
‚îú‚îÄ‚îÄ configs                   <- Hydra configs
‚îÇ   ‚îú‚îÄ‚îÄ callbacks                <- Callbacks configs
‚îÇ   ‚îú‚îÄ‚îÄ data                     <- Data configs
‚îÇ   ‚îú‚îÄ‚îÄ debug                    <- Debugging configs
‚îÇ   ‚îú‚îÄ‚îÄ experiment               <- Experiment configs
‚îÇ   ‚îú‚îÄ‚îÄ extras                   <- Extra utilities configs
‚îÇ   ‚îú‚îÄ‚îÄ hparams_search           <- Hyperparameter search configs
‚îÇ   ‚îú‚îÄ‚îÄ hydra                    <- Hydra configs
‚îÇ   ‚îú‚îÄ‚îÄ local                    <- Local configs
‚îÇ   ‚îú‚îÄ‚îÄ logger                   <- Logger configs
‚îÇ   ‚îú‚îÄ‚îÄ model                    <- Model configs
‚îÇ   ‚îú‚îÄ‚îÄ paths                    <- Project paths configs
‚îÇ   ‚îú‚îÄ‚îÄ trainer                  <- Trainer configs
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ eval.yaml             <- Main config for evaluation
‚îÇ   ‚îî‚îÄ‚îÄ train.yaml            <- Main config for training
‚îÇ
‚îú‚îÄ‚îÄ data                   <- Project data
‚îÇ
‚îú‚îÄ‚îÄ logs                   <- Logs generated by hydra and lightning loggers
‚îÇ
‚îú‚îÄ‚îÄ notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
‚îÇ                             the creator's initials, and a short `-` delimited description,
‚îÇ                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
‚îÇ
‚îú‚îÄ‚îÄ scripts                <- Shell scripts
‚îÇ
‚îú‚îÄ‚îÄ src                    <- Source code
‚îÇ   ‚îú‚îÄ‚îÄ data                     <- Data scripts
‚îÇ   ‚îú‚îÄ‚îÄ models                   <- Model scripts
‚îÇ   ‚îú‚îÄ‚îÄ utils                    <- Utility scripts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ eval.py                  <- Run evaluation
‚îÇ   ‚îî‚îÄ‚îÄ train.py                 <- Run training
‚îÇ
‚îú‚îÄ‚îÄ tests                  <- Tests of any kind
‚îÇ
‚îú‚îÄ‚îÄ .env.example              <- Example of file for storing private environment variables
‚îú‚îÄ‚îÄ .gitignore                <- List of files ignored by git
‚îú‚îÄ‚îÄ .pre-commit-config.yaml   <- Configuration of pre-commit hooks for code formatting
‚îú‚îÄ‚îÄ .project-root             <- File for inferring the position of project root directory
‚îú‚îÄ‚îÄ environment.yaml          <- File for installing conda environment
‚îú‚îÄ‚îÄ Makefile                  <- Makefile with commands like `make train` or `make test`
‚îú‚îÄ‚îÄ pyproject.toml            <- Configuration options for testing and linting
‚îú‚îÄ‚îÄ requirements.txt          <- File for installing python dependencies
‚îú‚îÄ‚îÄ setup.py                  <- File for installing project as a package
‚îî‚îÄ‚îÄ README.md
```

<br>

## üöÄ¬†¬†Quickstart

```bash
# clone project
git clone https://github.com/ashleve/lightning-hydra-template
cd lightning-hydra-template

# [OPTIONAL] create conda environment
conda create -n myenv python=3.9
conda activate myenv

# install pytorch according to instructions
# https://pytorch.org/get-started/

# install requirements
pip install -r requirements.txt
```

Template contains example with MNIST classification.<br>
When running `python src/train.py` you should see something like this:

<div align="center">

![](https://github.com/ashleve/lightning-hydra-template/blob/resources/terminal.png)

</div>

## ‚ö°¬†¬†Your Superpowers

<details>
<summary><b>Override any config parameter from command line</b></summary>

```bash
python train.py trainer.max_epochs=20 model.optimizer.lr=1e-4
```

> **Note**: You can also add new parameters with `+` sign.

```bash
python train.py +model.new_param="owo"
```

</details>

<details>
<summary><b>Train on CPU, GPU, multi-GPU and TPU</b></summary>

```bash
# train on CPU
python train.py trainer=cpu

# train on 1 GPU
python train.py trainer=gpu

# train on TPU
python train.py +trainer.tpu_cores=8

# train with DDP (Distributed Data Parallel) (4 GPUs)
python train.py trainer=ddp trainer.devices=4

# train with DDP (Distributed Data Parallel) (8 GPUs, 2 nodes)
python train.py trainer=ddp trainer.devices=4 trainer.num_nodes=2

# simulate DDP on CPU processes
python train.py trainer=ddp_sim trainer.devices=2

# accelerate training on mac
python train.py trainer=mps
```

> **Warning**: Currently there are problems with DDP mode, read [this issue](https://github.com/ashleve/lightning-hydra-template/issues/393) to learn more.

</details>

<details>
<summary><b>Train with mixed precision</b></summary>

```bash
# train with pytorch native automatic mixed precision (AMP)
python train.py trainer=gpu +trainer.precision=16
```

</details>

<!-- deepspeed support still in beta
<details>
<summary><b>Optimize large scale models on multiple GPUs with Deepspeed</b></summary>

```bash
python train.py +trainer.
```

</details>
 -->

<details>
<summary><b>Train model with any logger available in PyTorch Lightning, like W&B or Tensorboard</b></summary>

```yaml
# set project and entity names in `configs/logger/wandb`
wandb:
  project: "your_project_name"
  entity: "your_wandb_team_name"
```

```bash
# train model with Weights&Biases (link to wandb dashboard should appear in the terminal)
python train.py logger=wandb
```

> **Note**: Lightning provides convenient integrations with most popular logging frameworks. Learn more [here](#experiment-tracking).

> **Note**: Using wandb requires you to [setup account](https://www.wandb.com/) first. After that just complete the config as below.

> **Note**: Click [here](https://wandb.ai/hobglob/template-dashboard/) to see example wandb dashboard generated with this template.

</details>

<details>
<summary><b>Train model with chosen experiment config</b></summary>

```bash
python train.py experiment=example
```

> **Note**: Experiment configs are placed in [configs/experiment/](configs/experiment/).

</details>

<details>
<summary><b>Attach some callbacks to run</b></summary>

```bash
python train.py callbacks=default
```

> **Note**: Callbacks can be used for things such as as model checkpointing, early stopping and [many more](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks).

> **Note**: Callbacks configs are placed in [configs/callbacks/](configs/callbacks/).

</details>

<details>
<summary><b>Use different tricks available in Pytorch Lightning</b></summary>

```yaml
# gradient clipping may be enabled to avoid exploding gradients
python train.py +trainer.gradient_clip_val=0.5

# run validation loop 4 times during a training epoch
python train.py +trainer.val_check_interval=0.25

# accumulate gradients
python train.py +trainer.accumulate_grad_batches=10

# terminate training after 12 hours
python train.py +trainer.max_time="00:12:00:00"
```

> **Note**: PyTorch Lightning provides about [40+ useful trainer flags](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags).

</details>

<details>
<summary><b>Easily debug</b></summary>

```bash
# runs 1 epoch in default debugging mode
# changes logging directory to `logs/debugs/...`
# sets level of all command line loggers to 'DEBUG'
# enforces debug-friendly configuration
python train.py debug=default

# run 1 train, val and test loop, using only 1 batch
python train.py debug=fdr

# print execution time profiling
python train.py debug=profiler

# try overfitting to 1 batch
python train.py debug=overfit

# raise exception if there are any numerical anomalies in tensors, like NaN or +/-inf
python train.py +trainer.detect_anomaly=true

# use only 20% of the data
python train.py +trainer.limit_train_batches=0.2 \
+trainer.limit_val_batches=0.2 +trainer.limit_test_batches=0.2
```

> **Note**: Visit [configs/debug/](configs/debug/) for different debugging configs.

</details>

<details>
<summary><b>Resume training from checkpoint</b></summary>

```yaml
python train.py ckpt_path="/path/to/ckpt/name.ckpt"
```

> **Note**: Checkpoint can be either path or URL.

> **Note**: Currently loading ckpt doesn't resume logger experiment, but it will be supported in future Lightning release.

</details>

<details>
<summary><b>Evaluate checkpoint on test dataset</b></summary>

```yaml
python eval.py ckpt_path="/path/to/ckpt/name.ckpt"
```

> **Note**: Checkpoint can be either path or URL.

</details>

<details>
<summary><b>Create a sweep over hyperparameters</b></summary>

```bash
# this will run 6 experiments one after the other,
# each with different combination of batch_size and learning rate
python train.py -m data.batch_size=32,64,128 model.lr=0.001,0.0005
```

> **Note**: Hydra composes configs lazily at job launch time. If you change code or configs after launching a job/sweep, the final composed configs might be impacted.

</details>

<details>
<summary><b>Create a sweep over hyperparameters with Optuna</b></summary>

```bash
# this will run hyperparameter search defined in `configs/hparams_search/mnist_optuna.yaml`
# over chosen experiment config
python train.py -m hparams_search=mnist_optuna experiment=example
```

> **Note**: Using [Optuna Sweeper](https://hydra.cc/docs/next/plugins/optuna_sweeper) doesn't require you to add any boilerplate to your code, everything is defined in a [single config file](configs/hparams_search/mnist_optuna.yaml).

> **Warning**: Optuna sweeps are not failure-resistant (if one job crashes then the whole sweep crashes).

</details>

<details>
<summary><b>Execute all experiments from folder</b></summary>

```bash
python train.py -m 'experiment=glob(*)'
```

> **Note**: Hydra provides special syntax for controlling behavior of multiruns. Learn more [here](https://hydra.cc/docs/next/tutorials/basic/running_your_app/multi-run). The command above executes all experiments from [configs/experiment/](configs/experiment/).

</details>

<details>
<summary><b>Execute run for multiple different seeds</b></summary>

```bash
python train.py -m seed=1,2,3,4,5 trainer.deterministic=True logger=csv tags=["benchmark"]
```

> **Note**: `trainer.deterministic=True` makes pytorch more deterministic but impacts the performance.

</details>

<details>
<summary><b>Execute sweep on a remote AWS cluster</b></summary>

> **Note**: This should be achievable with simple config using [Ray AWS launcher for Hydra](https://hydra.cc/docs/next/plugins/ray_launcher). Example is not implemented in this template.

</details>

<!-- <details>
<summary><b>Execute sweep on a SLURM cluster</b></summary>

> This should be achievable with either [the right lightning trainer flags](https://pytorch-lightning.readthedocs.io/en/latest/clouds/cluster.html?highlight=SLURM#slurm-managed-cluster) or simple config using [Submitit launcher for Hydra](https://hydra.cc/docs/plugins/submitit_launcher). Example is not yet implemented in this template.

</details> -->

<details>
<summary><b>Use Hydra tab completion</b></summary>

> **Note**: Hydra allows you to autocomplete config argument overrides in shell as you write them, by pressing `tab` key. Read the [docs](https://hydra.cc/docs/tutorials/basic/running_your_app/tab_completion).

</details>

<details>
<summary><b>Apply pre-commit hooks</b></summary>

```bash
pre-commit run -a
```

> **Note**: Apply pre-commit hooks to do things like auto-formatting code and configs, performing code analysis or removing output from jupyter notebooks. See [# Best Practices](#best-practices) for more.

Update pre-commit hook versions in `.pre-commit-config.yaml` with:

```bash
pre-commit autoupdate
```

</details>

<details>
<summary><b>Run tests</b></summary>

```bash
# run all tests
pytest

# run tests from specific file
pytest tests/test_train.py

# run all tests except the ones marked as slow
pytest -k "not slow"
```

</details>

<details>
<summary><b>Use tags</b></summary>

Each experiment should be tagged in order to easily filter them across files or in logger UI:

```bash
python train.py tags=["mnist","experiment_X"]
```

> **Note**: You might need to escape the bracket characters in your shell with `python train.py tags=\["mnist","experiment_X"\]`.

If no tags are provided, you will be asked to input them from command line:

```bash
>>> python train.py tags=[]
[2022-07-11 15:40:09,358][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2022-07-11 15:40:09,359][src.utils.rich_utils][WARNING] - No tags provided in config. Prompting user to input tags...
Enter a list of comma separated tags (dev):
```

If no tags are provided for multirun, an error will be raised:

```bash
>>> python train.py -m +x=1,2,3 tags=[]
ValueError: Specify tags before launching a multirun!
```

> **Note**: Appending lists from command line is currently not supported in hydra :(

</details>

<br>

## ‚ù§Ô∏è¬†¬†Contributions

This project exists thanks to all the people who contribute.

![Contributors](https://readme-contributors.now.sh/ashleve/lightning-hydra-template?extension=jpg&width=400&aspectRatio=1)

Have a question? Found a bug? Missing a specific feature? Feel free to file a new issue, discussion or PR with respective title and description.

Before making an issue, please verify that:

- The problem still exists on the current `main` branch.
- Your python dependencies are updated to recent versions.

Suggestions for improvements are always welcome!

<br>

## How It Works

All PyTorch Lightning modules are dynamically instantiated from module paths specified in config. Example model config:

```yaml
_target_: src.models.mnist_model.MNISTLitModule
lr: 0.001
net:
  _target_: src.models.components.simple_dense_net.SimpleDenseNet
  input_size: 784
  lin1_size: 256
  lin2_size: 256
  lin3_size: 256
  output_size: 10
```

Using this config we can instantiate the object with the following line:

```python
model = hydra.utils.instantiate(config.model)
```

This allows you to easily iterate over new models! Every time you create a new one, just specify its module path and parameters in appropriate config file. <br>

Switch between models and datamodules with command line arguments:

```bash
python train.py model=mnist
```

Example pipeline managing the instantiation logic: [src/train.py](src/train.py).

<br>

## Main Config

Location: [configs/train.yaml](configs/train.yaml) <br>
Main project config contains default training configuration.<br>
It determines how config is composed when simply executing command `python train.py`.<br>

<details>
<summary><b>Show main project config</b></summary>

```yaml
# order of defaults determines the order in which configs override each other
defaults:
  - _self_
  - data: mnist.yaml
  - model: mnist.yaml
  - callbacks: default.yaml
  - logger: null # set logger here or use command line (e.g. `python train.py logger=csv`)
  - trainer: default.yaml
  - paths: default.yaml
  - extras: default.yaml
  - hydra: default.yaml

  # experiment configs allow for version control of specific hyperparameters
  # e.g. best hyperparameters for given model and datamodule
  - experiment: null

  # config for hyperparameter optimization
  - hparams_search: null

  # optional local config for machine/user specific settings
  # it's optional since it doesn't need to exist and is excluded from version control
  - optional local: default.yaml

  # debugging config (enable through command line, e.g. `python train.py debug=default)
  - debug: null

# task name, determines output directory path
task_name: "train"

# tags to help you identify your experiments
# you can overwrite this in experiment configs
# overwrite from command line with `python train.py tags="[first_tag, second_tag]"`
# appending lists from command line is currently not supported :(
# https://github.com/facebookresearch/hydra/issues/1547
tags: ["dev"]

# set False to skip model training
train: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: True

# simply provide checkpoint path to resume training
ckpt_path: null

# seed for random number generators in pytorch, numpy and python.random
seed: null
```

</details>

<br>

## Experiment Config

Location: [configs/experiment](configs/experiment)<br>
Experiment configs allow you to overwrite parameters from main config.<br>
For example, you can use them to version control best hyperparameters for each combination of model and dataset.

<details>
<summary><b>Show example experiment config</b></summary>

```yaml
# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: mnist.yaml
  - override /model: mnist.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["mnist", "simple_dense_net"]

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 10
  gradient_clip_val: 0.5

model:
  optimizer:
    lr: 0.002
  net:
    lin1_size: 128
    lin2_size: 256
    lin3_size: 64

data:
  batch_size: 64

logger:
  wandb:
    tags: ${tags}
    group: "mnist"
```

</details>

<br>

## Workflow

**Basic workflow**

1. Write your PyTorch Lightning module (see [models/mnist_module.py](src/models/mnist_module.py) for example)
2. Write your PyTorch Lightning datamodule (see [data/mnist_datamodule.py](src/data/mnist_datamodule.py) for example)
3. Write your experiment config, containing paths to model and datamodule
4. Run training with chosen experiment config:
   ```bash
   python src/train.py experiment=experiment_name.yaml
   ```

**Experiment design**

_Say you want to execute many runs to plot how accuracy changes in respect to batch size._

1. Execute the runs with some config parameter that allows you to identify them easily, like tags:

   ```bash
   python train.py -m logger=csv data.batch_size=16,32,64,128 tags=["batch_size_exp"]
   ```

2. Write a script or notebook that searches over the `logs/` folder and retrieves csv logs from runs containing given tags in config. Plot the results.

<br>

## Logs

Hydra creates new output directory for every executed run.

Default logging structure:

```
‚îú‚îÄ‚îÄ logs
‚îÇ   ‚îú‚îÄ‚îÄ task_name
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ runs                        # Logs generated by single runs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ YYYY-MM-DD_HH-MM-SS       # Datetime of the run
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ .hydra                  # Hydra logs
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ csv                     # Csv logs
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wandb                   # Weights&Biases logs
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ checkpoints             # Training checkpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...                     # Any other thing saved during training
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multiruns                   # Logs generated by multiruns
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ YYYY-MM-DD_HH-MM-SS       # Datetime of the multirun
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ1                        # Multirun job number
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ2
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ debugs                          # Logs generated when debugging config is attached
‚îÇ       ‚îî‚îÄ‚îÄ ...
```

</details>

You can change this structure by modifying paths in [hydra configuration](configs/hydra).

<br>

## Experiment Tracking

PyTorch Lightning supports many popular logging frameworks: [Weights&Biases](https://www.wandb.com/), [Neptune](https://neptune.ai/), [Comet](https://www.comet.ml/), [MLFlow](https://mlflow.org), [Tensorboard](https://www.tensorflow.org/tensorboard/).

These tools help you keep track of hyperparameters and output metrics and allow you to compare and visualize results. To use one of them simply complete its configuration in [configs/logger](configs/logger) and run:

```bash
python train.py logger=logger_name
```

You can use many of them at once (see [configs/logger/many_loggers.yaml](configs/logger/many_loggers.yaml) for example).

You can also write your own logger.

Lightning provides convenient method for logging custom metrics from inside LightningModule. Read the [docs](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging) or take a look at [MNIST example](src/models/mnist_module.py).

<br>

## Tests

Template comes with generic tests implemented with `pytest`.

```bash
# run all tests
pytest

# run tests from specific file
pytest tests/test_train.py

# run all tests except the ones marked as slow
pytest -k "not slow"
```

Most of the implemented tests don't check for any specific output - they exist to simply verify that executing some commands doesn't end up in throwing exceptions. You can execute them once in a while to speed up the development.

Currently, the tests cover cases like:

- running 1 train, val and test step
- running 1 epoch on 1% of data, saving ckpt and resuming for the second epoch
- running 2 epochs on 1% of data, with DDP simulated on CPU

And many others. You should be able to modify them easily for your use case.

There is also `@RunIf` decorator implemented, that allows you to run tests only if certain conditions are met, e.g. GPU is available or system is not windows. See the [examples](tests/test_train.py).

<br>

## Hyperparameter Search

You can define hyperparameter search by adding new config file to [configs/hparams_search](configs/hparams_search).

<details>
<summary><b>Show example hyperparameter search config</b></summary>

```yaml
# @package _global_

defaults:
  - override /hydra/sweeper: optuna

# choose metric which will be optimized by Optuna
# make sure this is the correct name of some metric logged in lightning module!
optimized_metric: "val/acc_best"

# here we define Optuna hyperparameter search
# it optimizes for value returned from function with @hydra.main decorator
hydra:
  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper

    # 'minimize' or 'maximize' the objective
    direction: maximize

    # total number of runs that will be executed
    n_trials: 20

    # choose Optuna hyperparameter sampler
    # docs: https://optuna.readthedocs.io/en/stable/reference/samplers.html
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 1234
      n_startup_trials: 10 # number of random sampling runs before optimization starts

    # define hyperparameter search space
    params:
      model.optimizer.lr: interval(0.0001, 0.1)
      data.batch_size: choice(32, 64, 128, 256)
      model.net.lin1_size: choice(64, 128, 256)
      model.net.lin2_size: choice(64, 128, 256)
      model.net.lin3_size: choice(32, 64, 128, 256)
```

</details>

Next, execute it with: `python train.py -m hparams_search=mnist_optuna`

Using this approach doesn't require adding any boilerplate to code, everything is defined in a single config file. The only necessary thing is to return the optimized metric value from the launch file.

You can use different optimization frameworks integrated with Hydra, like [Optuna, Ax or Nevergrad](https://hydra.cc/docs/plugins/optuna_sweeper/).

The `optimization_results.yaml` will be available under `logs/task_name/multirun` folder.

This approach doesn't support resuming interrupted search and advanced techniques like prunning - for more sophisticated search and workflows, you should probably write a dedicated optimization task (without multirun feature).

<br>

## Continuous Integration

Template comes with CI workflows implemented in Github Actions:

- `.github/workflows/test.yaml`: running all tests with pytest
- `.github/workflows/code-quality-main.yaml`: running pre-commits on main branch for all files
- `.github/workflows/code-quality-pr.yaml`: running pre-commits on pull requests for modified files only

<br>

## Distributed Training

Lightning supports multiple ways of doing distributed training. The most common one is DDP, which spawns separate process for each GPU and averages gradients between them. To learn about other approaches read the [lightning docs](https://lightning.ai/docs/pytorch/latest/advanced/speed.html).

You can run DDP on mnist example with 4 GPUs like this:

```bash
python train.py trainer=ddp
```

> **Note**: When using DDP you have to be careful how you write your models - read the [docs](https://lightning.ai/docs/pytorch/latest/advanced/speed.html).

<br>

## Accessing Datamodule Attributes In Model

The simplest way is to pass datamodule attribute directly to model on initialization:

```python
# ./src/train.py
datamodule = hydra.utils.instantiate(config.data)
model = hydra.utils.instantiate(config.model, some_param=datamodule.some_param)
```

> **Note**: Not a very robust solution, since it assumes all your datamodules have `some_param` attribute available.

Similarly, you can pass a whole datamodule config as an init parameter:

```python
# ./src/train.py
model = hydra.utils.instantiate(config.model, dm_conf=config.data, _recursive_=False)
```

You can also pass a datamodule config parameter to your model through variable interpolation:

```yaml
# ./configs/model/my_model.yaml
_target_: src.models.my_module.MyLitModule
lr: 0.01
some_param: ${data.some_param}
```

Another approach is to access datamodule in LightningModule directly through Trainer:

```python
# ./src/models/mnist_module.py
def on_train_start(self):
  self.some_param = self.trainer.datamodule.some_param
```

> **Note**: This only works after the training starts since otherwise trainer won't be yet available in LightningModule.

<br>

## Best Practices

<details>
<summary><b>Use Miniconda</b></summary>

It's usually unnecessary to install full anaconda environment, miniconda should be enough (weights around 80MB).

Big advantage of conda is that it allows for installing packages without requiring certain compilers or libraries to be available in the system (since it installs precompiled binaries), so it often makes it easier to install some dependencies e.g. cudatoolkit for GPU support.

It also allows you to access your environments globally which might be more convenient than creating new local environment for every project.

Example installation:

```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
```

Update conda:

```bash
conda update -n base -c defaults conda
```

Create new conda environment:

```bash
conda create -n myenv python=3.10
conda activate myenv
```

</details>

<details>
<summary><b>Use automatic code formatting</b></summary>

Use pre-commit hooks to standardize code formatting of your project and save mental energy.<br>
Simply install pre-commit package with:

```bash
pip install pre-commit
```

Next, install hooks from [.pre-commit-config.yaml](.pre-commit-config.yaml):

```bash
pre-commit install
```

After that your code will be automatically reformatted on every new commit.

To reformat all files in the project use command:

```bash
pre-commit run -a
```

To update hook versions in [.pre-commit-config.yaml](.pre-commit-config.yaml) use:

```bash
pre-commit autoupdate
```

</details>

<details>
<summary><b>Set private environment variables in .env file</b></summary>

System specific variables (e.g. absolute paths to datasets) should not be under version control or it will result in conflict between different users. Your private keys also shouldn't be versioned since you don't want them to be leaked.<br>

Template contains `.env.example` file, which serves as an example. Create a new file called `.env` (this name is excluded from version control in .gitignore).
You should use it for storing environment variables like this:

```
MY_VAR=/home/user/my_system_path
```

All variables from `.env` are loaded in `train.py` automatically.

Hydra allows you to reference any env variable in `.yaml` configs like this:

```yaml
path_to_data: ${oc.env:MY_VAR}
```

</details>

<details>
<summary><b>Name metrics using '/' character</b></summary>

Depending on which logger you're using, it's often useful to define metric name with `/` character:

```python
self.log("train/loss", loss)
```

This way loggers will treat your metrics as belonging to different sections, which helps to get them organised in UI.

</details>

<details>
<summary><b>Use torchmetrics</b></summary>

Use official [torchmetrics](https://github.com/PytorchLightning/metrics) library to ensure proper calculation of metrics. This is especially important for multi-GPU training!

For example, instead of calculating accuracy by yourself, you should use the provided `Accuracy` class like this:

```python
from torchmetrics.classification.accuracy import Accuracy


class LitModel(LightningModule):
    def __init__(self)
        self.train_acc = Accuracy()
        self.val_acc = Accuracy()

    def training_step(self, batch, batch_idx):
        ...
        acc = self.train_acc(predictions, targets)
        self.log("train/acc", acc)
        ...

    def validation_step(self, batch, batch_idx):
        ...
        acc = self.val_acc(predictions, targets)
        self.log("val/acc", acc)
        ...
```

Make sure to use different metric instance for each step to ensure proper value reduction over all GPU processes.

Torchmetrics provides metrics for most use cases, like F1 score or confusion matrix. Read [documentation](https://torchmetrics.readthedocs.io/en/latest/#more-reading) for more.

</details>

<details>
<summary><b>Follow PyTorch Lightning style guide</b></summary>

The style guide is available [here](https://pytorch-lightning.readthedocs.io/en/latest/starter/style_guide.html).<br>

1. Be explicit in your init. Try to define all the relevant defaults so that the user doesn‚Äôt have to guess. Provide type hints. This way your module is reusable across projects!

   ```python
   class LitModel(LightningModule):
       def __init__(self, layer_size: int = 256, lr: float = 0.001):
   ```

2. Preserve the recommended method order.

   ```python
   class LitModel(LightningModule):

       def __init__():
           ...

       def forward():
           ...

       def training_step():
           ...

       def training_step_end():
           ...

       def on_train_epoch_end():
           ...

       def validation_step():
           ...

       def validation_step_end():
           ...

       def on_validation_epoch_end():
           ...

       def test_step():
           ...

       def test_step_end():
           ...

       def on_test_epoch_end():
           ...

       def configure_optimizers():
           ...

       def any_extra_hook():
           ...
   ```

</details>

<details>
<summary><b>Support installing project as a package</b></summary>

It allows other people to easily use your modules in their own projects.
Change name of the `src` folder to your project name and complete the `setup.py` file.

Now your project can be installed from local files:

```bash
pip install -e .
```

Or directly from git repository:

```bash
pip install git+git://github.com/YourGithubName/your-repo-name.git --upgrade
```

So any file can be easily imported into any other file like so:

```python
from project_name.models.mnist_module import MNISTLitModule
from project_name.data.mnist_datamodule import MNISTDataModule
```

</details>

<details>
<summary><b>Keep local configs out of code versioning</b></summary>

Some configurations are user/machine/installation specific (e.g. configuration of local cluster, or harddrive paths on a specific machine). For such scenarios, a file [configs/local/default.yaml](configs/local/) can be created which is automatically loaded but not tracked by Git.

For example, you can use it for a SLURM cluster config:

```yaml
# @package _global_

defaults:
  - override /hydra/launcher@_here_: submitit_slurm

data_dir: /mnt/scratch/data/

hydra:
  launcher:
    timeout_min: 1440
    gpus_per_task: 1
    gres: gpu:1
  job:
    env_set:
      MY_VAR: /home/user/my/system/path
      MY_KEY: asdgjhawi8y23ihsghsueity23ihwd
```

</details>

<br>

## Resources

This template was inspired by:

- [PyTorchLightning/deep-learning-project-template](https://github.com/PyTorchLightning/deep-learning-project-template)
- [drivendata/cookiecutter-data-science](https://github.com/drivendata/cookiecutter-data-science)
- [lucmos/nn-template](https://github.com/lucmos/nn-template)

Other useful repositories:

- [jxpress/lightning-hydra-template-vertex-ai](https://github.com/jxpress/lightning-hydra-template-vertex-ai) - lightning-hydra-template integration with Vertex AI hyperparameter tuning and custom training job

</details>

<br>

## License

Lightning-Hydra-Template is licensed under the MIT License.

```
MIT License

Copyright (c) 2021 ashleve

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

<br>
<br>
<br>
<br>

**DELETE EVERYTHING ABOVE FOR YOUR PROJECT**

______________________________________________________________________

<div align="center">

# Your Project Name

<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a><br>
[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/paper/2020)

</div>

## Description

What it does

## Installation

#### Pip

```bash
# clone project
git clone https://github.com/YourGithubName/your-repo-name
cd your-repo-name

# [OPTIONAL] create conda environment
conda create -n myenv python=3.9
conda activate myenv

# install pytorch according to instructions
# https://pytorch.org/get-started/

# install requirements
pip install -r requirements.txt
```

#### Conda

```bash
# clone project
git clone https://github.com/YourGithubName/your-repo-name
cd your-repo-name

# create conda environment and install dependencies
conda env create -f environment.yaml -n myenv

# activate conda environment
conda activate myenv
```

## How to run

Train model with default configuration

```bash
# train on CPU
python src/train.py trainer=cpu

# train on GPU
python src/train.py trainer=gpu
```

Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/)

```bash
python src/train.py experiment=experiment_name.yaml
```

You can override any parameter from command line like this

```bash
python src/train.py trainer.max_epochs=20 data.batch_size=64
```

===== pyproject.toml =====
[tool.pytest.ini_options]
addopts = [
  "--color=yes",
  "--durations=0",
  "--strict-markers",
  "--doctest-modules",
]
filterwarnings = [
  "ignore::DeprecationWarning",
  "ignore::UserWarning",
]
log_cli = "True"
markers = [
  "slow: slow tests",
]
minversion = "6.0"
testpaths = "tests/"

[tool.coverage.report]
exclude_lines = [
    "pragma: nocover",
    "raise NotImplementedError",
    "raise NotImplementedError()",
    "if __name__ == .__main__.:",
]

===== Preprocessing pipeline =====
Run the Typer CLI directly (the previous repro flow is fully retired):

```
make preprocess-hest-v1                   # canonical human release
make preprocess CFG=preprocess/hest_mouse.yaml STAGE=stage-1
make preprocess STAGE=stage-2             # continue from intermediates
```

All configs derive their paths from the standard layout:

```
data/
    raw/<dataset_key>/
    processed_intermediate/<dataset_key>/
    processed/<dataset_key>/
```

Use `dataset.key` in each `configs/preprocess/*.yaml` file to isolate new sources.

Manifest files now live in `data/processed/<dataset_key>/manifest.json` and capture the resolved Hydra config, git SHA, hashes of the HVG/HGNC inputs, plus shard counts. Inspect them with `python scripts/inspect_manifest.py data/processed/<dataset_key>` before handing a dataset to the training team.
===== add_new_metric_guide.html =====
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SOTA Guide: Adding New Metrics</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&family=Fira+Code&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-color: #f8f9fa;
            --primary-text: #212529;
            --secondary-text: #495057;
            --card-bg: #ffffff;
            --border-color: #dee2e6;
            --primary-color: #4263eb;
            --code-bg: #2c3e50;
            --code-text: #ecf0f1;
            --note-bg: #e7f5ff;
            --note-border: #339af0;
            --success-bg: #e6fcf5;
            --success-border: #20c997;
        }
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.7;
            color: var(--primary-text);
            background-color: var(--bg-color);
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 900px;
            margin: 40px auto;
            padding: 40px;
            background-color: var(--card-bg);
            border-radius: 12px;
            box-shadow: 0 8px 30px rgba(0,0,0,0.08);
        }
        h1, h2, h3 {
            color: var(--primary-text);
            font-weight: 700;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }
        h1 {
            font-size: 2.8em;
            text-align: center;
            margin-bottom: 0.5em;
            background: linear-gradient(45deg, var(--primary-color), #a61e4d);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        h2 {
            font-size: 2em;
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 10px;
        }
        h3 {
            font-size: 1.5em;
        }
        code {
            font-family: 'Fira Code', "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
            background-color: rgba(0,0,0,0.05);
            padding: 3px 6px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        pre {
            background-color: var(--code-bg);
            color: var(--code-text);
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        }
        pre code {
            background-color: transparent;
            padding: 0;
            font-size: 0.95em;
            line-height: 1.5;
        }
        .info-box {
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
            display: flex;
            align-items: flex-start;
            gap: 15px;
        }
        .info-box svg {
            flex-shrink: 0;
            margin-top: 5px;
        }
        .note {
            background-color: var(--note-bg);
            border-left: 5px solid var(--note-border);
        }
        .success {
            background-color: var(--success-bg);
            border-left: 5px solid var(--success-border);
        }
        .file-path {
            font-style: italic;
            color: var(--secondary-text);
            display: block;
            margin-bottom: -10px;
            font-size: 0.9em;
        }
        .step {
            display: flex;
            align-items: center;
            gap: 15px;
            font-size: 1.5em;
            font-weight: 700;
            color: var(--primary-color);
        }
        .step-circle {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background-color: var(--primary-color);
            color: white;
            display: grid;
            place-items: center;
            font-size: 0.8em;
        }
        ul {
            padding-left: 20px;
        }
        li {
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>SOTA Guide to Adding Metrics</h1>
        <p>A developer-friendly guide to integrating new metrics into your PyTorch Lightning project with elegance and modularity.</p>

        <h2>The Big Picture: How Metrics Flow</h2>
        <p>Our project uses a clean, decoupled architecture. Understanding this flow is key:</p>
        <ul>
            <li><strong>YAML Configs:</strong> Define *what* metrics to use.</li>
            <li><strong>Hydra:</strong> Reads configs and *instantiates* metric objects.</li>
            <li><strong>MetricCollection:</strong> Bundles individual metrics into a single, callable object.</li>
            <li><strong>LightningModule:</strong> Receives the collection and uses it, without knowing the details.</li>
            <li><strong>Logger:</strong> Automatically receives and logs the results.</li>
        </ul>
        <div class="info-box note">
            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 8A6 6 0 0 0 6 8c0 7-3 9-3 9h18s-3-2-3-9"/><path d="M13.73 21a2 2 0 0 1-3.46 0"/></svg>
            <div><strong>Key Principle:</strong> The training logic in <code>LightningModule</code> is completely agnostic to *which* metrics are being run. This makes adding new ones a breeze‚Äîno changes to the core training code needed!</div>
        </div>

        <h2>Your Step-by-Step Mission</h2>

        <div class="step"><div class="step-circle">1</div><h3>Implement the Metric Class</h3></div>
        <p>Your custom metric should be a class inheriting from <code>torchmetrics.Metric</code>. This gives you superpowers like automatic distributed training support.</p>
        <p>Let's build a <strong>Mean Reciprocal Rank (MRR)</strong> metric.</p>
        <p class="file-path">File to edit: <code>src/models/components/metrics.py</code></p>
        <pre><code># src/models/components/metrics.py

# ... existing code for RecallAtK ...

class MeanReciprocalRank(Metric):
    """
    Mean Reciprocal Rank (MRR) for in-batch retrieval.
    """
    is_differentiable = False
    higher_is_better = True
    full_state_update = False

    def __init__(self):
        super().__init__()
        # `add_state` registers buffers to track the metric's state.
        # `dist_reduce_fx="sum"` ensures correct aggregation in distributed settings.
        self.add_state("reciprocal_sum", default=torch.tensor(0.0), dist_reduce_fx="sum")
        self.add_state("count", default=torch.tensor(0), dist_reduce_fx="sum")

    def update(self, logits: torch.Tensor, target: torch.Tensor) -> None:
        """Called on each batch to update the metric's state."""
        # Get descending sort indices for each row of logits
        preds_desc = torch.argsort(logits, dim=1, descending=True)
        
        # Find the rank of the target for each item in the batch
        matches = (preds_desc == target.view(-1, 1))
        ranks = torch.nonzero(matches)[:, 1].float() + 1.0
        
        # Update state
        self.reciprocal_sum += torch.sum(1.0 / ranks)
        self.count += target.numel()

    def compute(self) -> torch.Tensor:
        """Computes the final metric value from the state."""
        return (self.reciprocal_sum / self.count).float()

# ... existing ContrastiveMetrics class ...
</code></pre>

        <div class="step"><div class="step-circle">2</div><h3>Register in a Metric Collection</h3></div>
        <p>The simplest way to get your metric into the training loop is by adding it to the existing <code>ContrastiveMetrics</code> collection.</p>
        <p class="file-path">File to edit: <code>src/models/components/metrics.py</code></p>
        <pre><code># src/models/components/metrics.py

class ContrastiveMetrics(torchmetrics.MetricCollection):
    """A collection of robust metrics for contrastive learning tasks."""
    def __init__(self, prefix: str):
        super().__init__(
            {
                "R@1": RecallAtK(k=1),
                "R@5": RecallAtK(k=5),
                "R@10": RecallAtK(k=10),
                "MRR": MeanReciprocalRank(),  # &lt;-- Your new metric is now active!
            },
            prefix=prefix,
        )
</code></pre>

        <div class="step"><div class="step-circle">3</div><h3>Write a Rock-Solid Unit Test</h3></div>
        <p>Good code is tested code. A unit test ensures your metric is correct now and in the future.</p>
        <div class="info-box success">
             <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"/><polyline points="22 4 12 14.01 9 11.01"/></svg>
            <div><strong>Best Practice:</strong> Create a small, predictable input tensor where you can calculate the expected output by hand. This makes your test clear and reliable.</div>
        </div>
        <p class="file-path">New file: <code>tests/test_custom_metrics.py</code></p>
        <pre><code># tests/test_custom_metrics.py
import torch
import pytest
from src.models.components.metrics import MeanReciprocalRank, ContrastiveMetrics

def test_mrr_metric_perfect_score():
    """Test MRR with perfect predictions (all rank 1)."""
    mrr = MeanReciprocalRank()
    logits = torch.tensor([[10., 1., 0.], [0., 10., 1.], [1., 0., 10.]])
    targets = torch.tensor([0, 1, 2])
    mrr.update(logits, targets)
    assert torch.isclose(mrr.compute(), torch.tensor(1.0))

def test_mrr_metric_mixed_ranks():
    """Test MRR with a mix of ranks."""
    mrr = MeanReciprocalRank()
    # Ranks: 1, 3, 2
    logits = torch.tensor([[10., 1., 0.], [1., 0., 10.], [1., 10., 0.]])
    targets = torch.tensor([0, 1, 2])
    mrr.update(logits, targets)
    
    # Expected MRR = (1/1 + 1/3 + 1/2) / 3 = 11/18
    expected = (1.0/1.0 + 1.0/3.0 + 1.0/2.0) / 3.0
    assert torch.isclose(mrr.compute(), torch.tensor(expected))

def test_full_metric_collection():
    """Test that the full metric collection computes all metrics."""
    metrics = ContrastiveMetrics(prefix="test/")
    logits = torch.eye(5) * 10  # Perfect predictions for a batch of 5
    targets = torch.arange(5)

    metrics.update(logits, targets)
    results = metrics.compute()

    assert "test/MRR" in results
    assert "test/R@1" in results
    assert torch.isclose(results["test/MRR"], torch.tensor(1.0))
    assert torch.isclose(results["test/R@1"], torch.tensor(1.0))
</code></pre>
        <p>Run your new test from the terminal:</p>
        <pre><code>pytest tests/test_custom_metrics.py</code></pre>

        <h2>Next Level: Configuration with Hydra</h2>
        <p>For ultimate flexibility, you can modify your <code>MetricCollection</code> to accept parameters from your YAML files. This lets you switch metrics on or off without touching the code.</p>
        <p>First, update the collection to conditionally include metrics:</p>
        <pre><code># src/models/components/metrics.py
class ContrastiveMetrics(torchmetrics.MetricCollection):
    def __init__(self, prefix: str, include_mrr: bool = False):
        metrics_dict = {
            "R@1": RecallAtK(k=1),
            "R@5": RecallAtK(k=5),
            "R@10": RecallAtK(k=10),
        }
        if include_mrr:
            metrics_dict["MRR"] = MeanReciprocalRank()
        
        super().__init__(metrics_dict, prefix=prefix)
</code></pre>
        <p>Now, create a new config file to enable it:</p>
        <p class="file-path">New file: <code>configs/metrics/contrastive_with_mrr.yaml</code></p>
        <pre><code># @package _group_
_target_: src.models.components.metrics.ContrastiveMetrics
include_mrr: True
</code></pre>
        <p>You can now run an experiment with MRR enabled via a simple command-line override:</p>
        <pre><code># The 'prefix' will be automatically set by the main model config
python src/train.py model/metrics/val_metrics=contrastive_with_mrr
</code></pre>

        <h2>Conclusion</h2>
        <p>That's it! Your new metric is now fully integrated. It will be automatically computed, logged, and can be controlled via configuration. This clean, modular design makes maintaining and extending your project's evaluation capabilities straightforward and robust.</p>
    </div>
</body>
</html>

===== tests/test_train.py =====
import os
from pathlib import Path

import pytest
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, open_dict

from src.train import train
from tests.helpers.run_if import RunIf


def test_train_fast_dev_run(cfg_train: DictConfig) -> None:
    """Run for 1 train, val and test step.

    :param cfg_train: A DictConfig containing a valid training configuration.
    """
    HydraConfig().set_config(cfg_train)
    with open_dict(cfg_train):
        cfg_train.trainer.fast_dev_run = True
        cfg_train.trainer.accelerator = "cpu"
    train(cfg_train)


@RunIf(min_gpus=1)
def test_train_fast_dev_run_gpu(cfg_train: DictConfig) -> None:
    """Run for 1 train, val and test step on GPU.

    :param cfg_train: A DictConfig containing a valid training configuration.
    """
    HydraConfig().set_config(cfg_train)
    with open_dict(cfg_train):
        cfg_train.trainer.fast_dev_run = True
        cfg_train.trainer.accelerator = "gpu"
    train(cfg_train)


@RunIf(min_gpus=1)
@pytest.mark.slow
def test_train_epoch_gpu_amp(cfg_train: DictConfig) -> None:
    """Train 1 epoch on GPU with mixed-precision.

    :param cfg_train: A DictConfig containing a valid training configuration.
    """
    HydraConfig().set_config(cfg_train)
    with open_dict(cfg_train):
        cfg_train.trainer.max_epochs = 1
        cfg_train.trainer.accelerator = "gpu"
        cfg_train.trainer.precision = 16
    train(cfg_train)


@pytest.mark.slow
def test_train_epoch_double_val_loop(cfg_train: DictConfig) -> None:
    """Train 1 epoch with validation loop twice per epoch.

    :param cfg_train: A DictConfig containing a valid training configuration.
    """
    HydraConfig().set_config(cfg_train)
    with open_dict(cfg_train):
        cfg_train.trainer.max_epochs = 1
        cfg_train.trainer.val_check_interval = 0.5
    train(cfg_train)


@pytest.mark.slow
def test_train_ddp_sim(cfg_train: DictConfig) -> None:
    """Simulate DDP (Distributed Data Parallel) on 2 CPU processes.

    :param cfg_train: A DictConfig containing a valid training configuration.
    """
    HydraConfig().set_config(cfg_train)
    with open_dict(cfg_train):
        cfg_train.trainer.max_epochs = 2
        cfg_train.trainer.accelerator = "cpu"
        cfg_train.trainer.devices = 2
        cfg_train.trainer.strategy = "ddp_spawn"
    train(cfg_train)


@pytest.mark.slow
def test_train_resume(tmp_path: Path, cfg_train: DictConfig) -> None:
    """Run 1 epoch, finish, and resume for another epoch.

    :param tmp_path: The temporary logging path.
    :param cfg_train: A DictConfig containing a valid training configuration.
    """
    with open_dict(cfg_train):
        cfg_train.trainer.max_epochs = 1

    HydraConfig().set_config(cfg_train)
    metric_dict_1, _ = train(cfg_train)

    files = os.listdir(tmp_path / "checkpoints")
    assert "last.ckpt" in files
    assert "epoch_000.ckpt" in files

    with open_dict(cfg_train):
        cfg_train.ckpt_path = str(tmp_path / "checkpoints" / "last.ckpt")
        cfg_train.trainer.max_epochs = 2

    metric_dict_2, _ = train(cfg_train)

    files = os.listdir(tmp_path / "checkpoints")
    assert "epoch_001.ckpt" in files
    assert "epoch_002.ckpt" not in files

    assert metric_dict_1["train/acc"] < metric_dict_2["train/acc"]
    assert metric_dict_1["val/acc"] < metric_dict_2["val/acc"]

===== tests/test_configs.py =====
import hydra
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig


def test_train_config(cfg_train: DictConfig) -> None:
    """Tests the training configuration provided by the `cfg_train` pytest fixture.

    :param cfg_train: A DictConfig containing a valid training configuration.
    """
    assert cfg_train
    assert cfg_train.data
    assert cfg_train.model
    assert cfg_train.trainer

    HydraConfig().set_config(cfg_train)

    hydra.utils.instantiate(cfg_train.data)
    hydra.utils.instantiate(cfg_train.model)
    hydra.utils.instantiate(cfg_train.trainer)


def test_eval_config(cfg_eval: DictConfig) -> None:
    """Tests the evaluation configuration provided by the `cfg_eval` pytest fixture.

    :param cfg_train: A DictConfig containing a valid evaluation configuration.
    """
    assert cfg_eval
    assert cfg_eval.data
    assert cfg_eval.model
    assert cfg_eval.trainer

    HydraConfig().set_config(cfg_eval)

    hydra.utils.instantiate(cfg_eval.data)
    hydra.utils.instantiate(cfg_eval.model)
    hydra.utils.instantiate(cfg_eval.trainer)

===== tests/test_eval.py =====
import os
from pathlib import Path

import pytest
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, open_dict

from src.eval import evaluate
from src.train import train


@pytest.mark.slow
def test_train_eval(tmp_path: Path, cfg_train: DictConfig, cfg_eval: DictConfig) -> None:
    """Tests training and evaluation by training for 1 epoch with `train.py` then evaluating with
    `eval.py`.

    :param tmp_path: The temporary logging path.
    :param cfg_train: A DictConfig containing a valid training configuration.
    :param cfg_eval: A DictConfig containing a valid evaluation configuration.
    """
    assert str(tmp_path) == cfg_train.paths.output_dir == cfg_eval.paths.output_dir

    with open_dict(cfg_train):
        cfg_train.trainer.max_epochs = 1
        cfg_train.test = True

    HydraConfig().set_config(cfg_train)
    train_metric_dict, _ = train(cfg_train)

    assert "last.ckpt" in os.listdir(tmp_path / "checkpoints")

    with open_dict(cfg_eval):
        cfg_eval.ckpt_path = str(tmp_path / "checkpoints" / "last.ckpt")

    HydraConfig().set_config(cfg_eval)
    test_metric_dict, _ = evaluate(cfg_eval)

    assert test_metric_dict["test/acc"] > 0.0
    assert abs(train_metric_dict["test/acc"].item() - test_metric_dict["test/acc"].item()) < 0.001

===== tests/__init__.py =====

===== tests/conftest.py =====
"""This file prepares config fixtures for other tests."""

from pathlib import Path

import pytest
import rootutils
from hydra import compose, initialize
from hydra.core.global_hydra import GlobalHydra
from omegaconf import DictConfig, open_dict


@pytest.fixture(scope="package")
def cfg_train_global() -> DictConfig:
    """A pytest fixture for setting up a default Hydra DictConfig for training.

    :return: A DictConfig object containing a default Hydra configuration for training.
    """
    with initialize(version_base="1.3", config_path="../configs"):
        cfg = compose(config_name="train.yaml", return_hydra_config=True, overrides=[])

        # set defaults for all tests
        with open_dict(cfg):
            cfg.paths.root_dir = str(rootutils.find_root(indicator=".project-root"))
            cfg.trainer.max_epochs = 1
            cfg.trainer.limit_train_batches = 0.01
            cfg.trainer.limit_val_batches = 0.1
            cfg.trainer.limit_test_batches = 0.1
            cfg.trainer.accelerator = "cpu"
            cfg.trainer.devices = 1
            cfg.data.num_workers = 0
            cfg.data.pin_memory = False
            cfg.extras.print_config = False
            cfg.extras.enforce_tags = False
            cfg.logger = None

    return cfg


@pytest.fixture(scope="package")
def cfg_eval_global() -> DictConfig:
    """A pytest fixture for setting up a default Hydra DictConfig for evaluation.

    :return: A DictConfig containing a default Hydra configuration for evaluation.
    """
    with initialize(version_base="1.3", config_path="../configs"):
        cfg = compose(config_name="eval.yaml", return_hydra_config=True, overrides=["ckpt_path=."])

        # set defaults for all tests
        with open_dict(cfg):
            cfg.paths.root_dir = str(rootutils.find_root(indicator=".project-root"))
            cfg.trainer.max_epochs = 1
            cfg.trainer.limit_test_batches = 0.1
            cfg.trainer.accelerator = "cpu"
            cfg.trainer.devices = 1
            cfg.data.num_workers = 0
            cfg.data.pin_memory = False
            cfg.extras.print_config = False
            cfg.extras.enforce_tags = False
            cfg.logger = None

    return cfg


@pytest.fixture(scope="function")
def cfg_train(cfg_train_global: DictConfig, tmp_path: Path) -> DictConfig:
    """A pytest fixture built on top of the `cfg_train_global()` fixture, which accepts a temporary
    logging path `tmp_path` for generating a temporary logging path.

    This is called by each test which uses the `cfg_train` arg. Each test generates its own temporary logging path.

    :param cfg_train_global: The input DictConfig object to be modified.
    :param tmp_path: The temporary logging path.

    :return: A DictConfig with updated output and log directories corresponding to `tmp_path`.
    """
    cfg = cfg_train_global.copy()

    with open_dict(cfg):
        cfg.paths.output_dir = str(tmp_path)
        cfg.paths.log_dir = str(tmp_path)

    yield cfg

    GlobalHydra.instance().clear()


@pytest.fixture(scope="function")
def cfg_eval(cfg_eval_global: DictConfig, tmp_path: Path) -> DictConfig:
    """A pytest fixture built on top of the `cfg_eval_global()` fixture, which accepts a temporary
    logging path `tmp_path` for generating a temporary logging path.

    This is called by each test which uses the `cfg_eval` arg. Each test generates its own temporary logging path.

    :param cfg_train_global: The input DictConfig object to be modified.
    :param tmp_path: The temporary logging path.

    :return: A DictConfig with updated output and log directories corresponding to `tmp_path`.
    """
    cfg = cfg_eval_global.copy()

    with open_dict(cfg):
        cfg.paths.output_dir = str(tmp_path)
        cfg.paths.log_dir = str(tmp_path)

    yield cfg

    GlobalHydra.instance().clear()

===== tests/test_sweeps.py =====
from pathlib import Path

import pytest

from tests.helpers.run_if import RunIf
from tests.helpers.run_sh_command import run_sh_command

startfile = "src/train.py"
overrides = ["logger=[]"]


@RunIf(sh=True)
@pytest.mark.slow
def test_experiments(tmp_path: Path) -> None:
    """Test running all available experiment configs with `fast_dev_run=True.`

    :param tmp_path: The temporary logging path.
    """
    command = [
        startfile,
        "-m",
        "experiment=glob(*)",
        "hydra.sweep.dir=" + str(tmp_path),
        "++trainer.fast_dev_run=true",
    ] + overrides
    run_sh_command(command)


@RunIf(sh=True)
@pytest.mark.slow
def test_hydra_sweep(tmp_path: Path) -> None:
    """Test default hydra sweep.

    :param tmp_path: The temporary logging path.
    """
    command = [
        startfile,
        "-m",
        "hydra.sweep.dir=" + str(tmp_path),
        "model.optimizer.lr=0.005,0.01",
        "++trainer.fast_dev_run=true",
    ] + overrides

    run_sh_command(command)


@RunIf(sh=True)
@pytest.mark.slow
def test_hydra_sweep_ddp_sim(tmp_path: Path) -> None:
    """Test default hydra sweep with ddp sim.

    :param tmp_path: The temporary logging path.
    """
    command = [
        startfile,
        "-m",
        "hydra.sweep.dir=" + str(tmp_path),
        "trainer=ddp_sim",
        "trainer.max_epochs=3",
        "+trainer.limit_train_batches=0.01",
        "+trainer.limit_val_batches=0.1",
        "+trainer.limit_test_batches=0.1",
        "model.optimizer.lr=0.005,0.01,0.02",
    ] + overrides
    run_sh_command(command)


@RunIf(sh=True)
@pytest.mark.slow
def test_optuna_sweep(tmp_path: Path) -> None:
    """Test Optuna hyperparam sweeping.

    :param tmp_path: The temporary logging path.
    """
    command = [
        startfile,
        "-m",
        "hparams_search=mnist_optuna",
        "hydra.sweep.dir=" + str(tmp_path),
        "hydra.sweeper.n_trials=10",
        "hydra.sweeper.sampler.n_startup_trials=5",
        "++trainer.fast_dev_run=true",
    ] + overrides
    run_sh_command(command)


@RunIf(wandb=True, sh=True)
@pytest.mark.slow
def test_optuna_sweep_ddp_sim_wandb(tmp_path: Path) -> None:
    """Test Optuna sweep with wandb logging and ddp sim.

    :param tmp_path: The temporary logging path.
    """
    command = [
        startfile,
        "-m",
        "hparams_search=mnist_optuna",
        "hydra.sweep.dir=" + str(tmp_path),
        "hydra.sweeper.n_trials=5",
        "trainer=ddp_sim",
        "trainer.max_epochs=3",
        "+trainer.limit_train_batches=0.01",
        "+trainer.limit_val_batches=0.1",
        "+trainer.limit_test_batches=0.1",
        "logger=wandb",
    ]
    run_sh_command(command)

===== tests/test_datamodules.py =====
from pathlib import Path

import pytest
import torch

from src.data.mnist_datamodule import MNISTDataModule


@pytest.mark.parametrize("batch_size", [32, 128])
def test_mnist_datamodule(batch_size: int) -> None:
    """Tests `MNISTDataModule` to verify that it can be downloaded correctly, that the necessary
    attributes were created (e.g., the dataloader objects), and that dtypes and batch sizes
    correctly match.

    :param batch_size: Batch size of the data to be loaded by the dataloader.
    """
    data_dir = "data/"

    dm = MNISTDataModule(data_dir=data_dir, batch_size=batch_size)
    dm.prepare_data()

    assert not dm.data_train and not dm.data_val and not dm.data_test
    assert Path(data_dir, "MNIST").exists()
    assert Path(data_dir, "MNIST", "raw").exists()

    dm.setup()
    assert dm.data_train and dm.data_val and dm.data_test
    assert dm.train_dataloader() and dm.val_dataloader() and dm.test_dataloader()

    num_datapoints = len(dm.data_train) + len(dm.data_val) + len(dm.data_test)
    assert num_datapoints == 70_000

    batch = next(iter(dm.train_dataloader()))
    x, y = batch
    assert len(x) == batch_size
    assert len(y) == batch_size
    assert x.dtype == torch.float32
    assert y.dtype == torch.int64

===== tests/helpers/package_available.py =====
import platform

import pkg_resources
from lightning.fabric.accelerators import TPUAccelerator


def _package_available(package_name: str) -> bool:
    """Check if a package is available in your environment.

    :param package_name: The name of the package to be checked.

    :return: `True` if the package is available. `False` otherwise.
    """
    try:
        return pkg_resources.require(package_name) is not None
    except pkg_resources.DistributionNotFound:
        return False


_TPU_AVAILABLE = TPUAccelerator.is_available()

_IS_WINDOWS = platform.system() == "Windows"

_SH_AVAILABLE = not _IS_WINDOWS and _package_available("sh")

_DEEPSPEED_AVAILABLE = not _IS_WINDOWS and _package_available("deepspeed")
_FAIRSCALE_AVAILABLE = not _IS_WINDOWS and _package_available("fairscale")

_WANDB_AVAILABLE = _package_available("wandb")
_NEPTUNE_AVAILABLE = _package_available("neptune")
_COMET_AVAILABLE = _package_available("comet_ml")
_MLFLOW_AVAILABLE = _package_available("mlflow")

===== tests/helpers/__init__.py =====

===== tests/helpers/run_if.py =====
"""Adapted from:

https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/helpers/runif.py
"""

import sys
from typing import Any, Dict, Optional

import pytest
import torch
from packaging.version import Version
from pkg_resources import get_distribution
from pytest import MarkDecorator

from tests.helpers.package_available import (
    _COMET_AVAILABLE,
    _DEEPSPEED_AVAILABLE,
    _FAIRSCALE_AVAILABLE,
    _IS_WINDOWS,
    _MLFLOW_AVAILABLE,
    _NEPTUNE_AVAILABLE,
    _SH_AVAILABLE,
    _TPU_AVAILABLE,
    _WANDB_AVAILABLE,
)


class RunIf:
    """RunIf wrapper for conditional skipping of tests.

    Fully compatible with `@pytest.mark`.

    Example:

    ```python
        @RunIf(min_torch="1.8")
        @pytest.mark.parametrize("arg1", [1.0, 2.0])
        def test_wrapper(arg1):
            assert arg1 > 0
    ```
    """

    def __new__(
        cls,
        min_gpus: int = 0,
        min_torch: Optional[str] = None,
        max_torch: Optional[str] = None,
        min_python: Optional[str] = None,
        skip_windows: bool = False,
        sh: bool = False,
        tpu: bool = False,
        fairscale: bool = False,
        deepspeed: bool = False,
        wandb: bool = False,
        neptune: bool = False,
        comet: bool = False,
        mlflow: bool = False,
        **kwargs: Dict[Any, Any],
    ) -> MarkDecorator:
        """Creates a new `@RunIf` `MarkDecorator` decorator.

        :param min_gpus: Min number of GPUs required to run test.
        :param min_torch: Minimum pytorch version to run test.
        :param max_torch: Maximum pytorch version to run test.
        :param min_python: Minimum python version required to run test.
        :param skip_windows: Skip test for Windows platform.
        :param tpu: If TPU is available.
        :param sh: If `sh` module is required to run the test.
        :param fairscale: If `fairscale` module is required to run the test.
        :param deepspeed: If `deepspeed` module is required to run the test.
        :param wandb: If `wandb` module is required to run the test.
        :param neptune: If `neptune` module is required to run the test.
        :param comet: If `comet` module is required to run the test.
        :param mlflow: If `mlflow` module is required to run the test.
        :param kwargs: Native `pytest.mark.skipif` keyword arguments.
        """
        conditions = []
        reasons = []

        if min_gpus:
            conditions.append(torch.cuda.device_count() < min_gpus)
            reasons.append(f"GPUs>={min_gpus}")

        if min_torch:
            torch_version = get_distribution("torch").version
            conditions.append(Version(torch_version) < Version(min_torch))
            reasons.append(f"torch>={min_torch}")

        if max_torch:
            torch_version = get_distribution("torch").version
            conditions.append(Version(torch_version) >= Version(max_torch))
            reasons.append(f"torch<{max_torch}")

        if min_python:
            py_version = (
                f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
            )
            conditions.append(Version(py_version) < Version(min_python))
            reasons.append(f"python>={min_python}")

        if skip_windows:
            conditions.append(_IS_WINDOWS)
            reasons.append("does not run on Windows")

        if tpu:
            conditions.append(not _TPU_AVAILABLE)
            reasons.append("TPU")

        if sh:
            conditions.append(not _SH_AVAILABLE)
            reasons.append("sh")

        if fairscale:
            conditions.append(not _FAIRSCALE_AVAILABLE)
            reasons.append("fairscale")

        if deepspeed:
            conditions.append(not _DEEPSPEED_AVAILABLE)
            reasons.append("deepspeed")

        if wandb:
            conditions.append(not _WANDB_AVAILABLE)
            reasons.append("wandb")

        if neptune:
            conditions.append(not _NEPTUNE_AVAILABLE)
            reasons.append("neptune")

        if comet:
            conditions.append(not _COMET_AVAILABLE)
            reasons.append("comet")

        if mlflow:
            conditions.append(not _MLFLOW_AVAILABLE)
            reasons.append("mlflow")

        reasons = [rs for cond, rs in zip(conditions, reasons) if cond]
        return pytest.mark.skipif(
            condition=any(conditions),
            reason=f"Requires: [{' + '.join(reasons)}]",
            **kwargs,
        )

===== tests/helpers/run_sh_command.py =====
from typing import List

import pytest

from tests.helpers.package_available import _SH_AVAILABLE

if _SH_AVAILABLE:
    import sh


def run_sh_command(command: List[str]) -> None:
    """Default method for executing shell commands with `pytest` and `sh` package.

    :param command: A list of shell commands as strings.
    """
    msg = None
    try:
        sh.python(command)
    except sh.ErrorReturnCode as e:
        msg = e.stderr.decode()
    if msg:
        pytest.fail(msg=msg)

===== .vscode/settings.json =====
{
    "python-envs.defaultEnvManager": "ms-python.python:system",
    "python-envs.pythonProjects": []
}



{
  "python.analysis.typeCheckingMode": "strict",
  "python.analysis.diagnosticSeverityOverrides": {
    "reportMissingImports": "error",
    "reportUndefinedVariable": "error",
    "reportUnusedVariable": "warning",
    "reportGeneralTypeIssues": "error",
    "reportPrivateUsage": "information"
  }
}

===== .github/dependabot.yml =====
# To get started with Dependabot version updates, you'll need to specify which
# package ecosystems to update and where the package manifests are located.
# Please see the documentation for all configuration options:
# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates

version: 2
updates:
  - package-ecosystem: "pip" # See documentation for possible values
    directory: "/" # Location of package manifests
    schedule:
      interval: "daily"
    ignore:
      - dependency-name: "pytorch-lightning"
        update-types: ["version-update:semver-patch"]
      - dependency-name: "torchmetrics"
        update-types: ["version-update:semver-patch"]

===== .github/release-drafter.yml =====
name-template: "v$RESOLVED_VERSION"
tag-template: "v$RESOLVED_VERSION"

categories:
  - title: "üöÄ Features"
    labels:
      - "feature"
      - "enhancement"
  - title: "üêõ Bug Fixes"
    labels:
      - "fix"
      - "bugfix"
      - "bug"
  - title: "üßπ Maintenance"
    labels:
      - "maintenance"
      - "dependencies"
      - "refactoring"
      - "cosmetic"
      - "chore"
  - title: "üìùÔ∏è Documentation"
    labels:
      - "documentation"
      - "docs"

change-template: "- $TITLE @$AUTHOR (#$NUMBER)"
change-title-escapes: '\<*_&' # You can add # and @ to disable mentions

version-resolver:
  major:
    labels:
      - "major"
  minor:
    labels:
      - "minor"
  patch:
    labels:
      - "patch"
  default: patch

template: |
  ## Changes

  $CHANGES

===== .github/codecov.yml =====
coverage:
  status:
    # measures overall project coverage
    project:
      default:
        threshold: 100% # how much decrease in coverage is needed to not consider success

    # measures PR or single commit coverage
    patch:
      default:
        threshold: 100% # how much decrease in coverage is needed to not consider success


    # project: off
    # patch: off

===== .github/PULL_REQUEST_TEMPLATE.md =====
## What does this PR do?

<!--
Please include a summary of the change and which issue is fixed.
Please also include relevant motivation and context.
List any dependencies that are required for this change.
List all the breaking changes introduced by this pull request.
-->

Fixes #\<issue_number>

## Before submitting

- [ ] Did you make sure **title is self-explanatory** and **the description concisely explains the PR**?
- [ ] Did you make sure your **PR does only one thing**, instead of bundling different changes together?
- [ ] Did you list all the **breaking changes** introduced by this pull request?
- [ ] Did you **test your PR locally** with `pytest` command?
- [ ] Did you **run pre-commit hooks** with `pre-commit run -a` command?

## Did you have fun?

Make sure you had fun coding üôÉ

===== .github/workflows/code-quality-pr.yaml =====
# This workflow finds which files were changed, prints them,
# and runs `pre-commit` on those files.

# Inspired by the sktime library:
# https://github.com/alan-turing-institute/sktime/blob/main/.github/workflows/test.yml

name: Code Quality PR

on:
  pull_request:
    branches: [main, "release/*", "dev"]

jobs:
  code-quality:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2

      - name: Find modified files
        id: file_changes
        uses: trilom/file-changes-action@v1.2.4
        with:
          output: " "

      - name: List modified files
        run: echo '${{ steps.file_changes.outputs.files}}'

      - name: Run pre-commits
        uses: pre-commit/action@v2.0.3
        with:
          extra_args: --files ${{ steps.file_changes.outputs.files}}

===== .github/workflows/release-drafter.yml =====
name: Release Drafter

on:
  push:
    # branches to consider in the event; optional, defaults to all
    branches:
      - main

permissions:
  contents: read

jobs:
  update_release_draft:
    permissions:
      # write permission is required to create a github release
      contents: write
      # write permission is required for autolabeler
      # otherwise, read permission is required at least
      pull-requests: write

    runs-on: ubuntu-latest

    steps:
      # Drafts your next Release notes as Pull Requests are merged into "master"
      - uses: release-drafter/release-drafter@v5
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

===== .github/workflows/test.yml =====
name: Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main, "release/*", "dev"]

jobs:
  run_tests_ubuntu:
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-latest"]
        python-version: ["3.8", "3.9", "3.10"]

    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v3
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
          pip install sh

      - name: List dependencies
        run: |
          python -m pip list

      - name: Run pytest
        run: |
          pytest -v

  run_tests_macos:
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        os: ["macos-latest"]
        python-version: ["3.8", "3.9", "3.10"]

    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v3
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
          pip install sh

      - name: List dependencies
        run: |
          python -m pip list

      - name: Run pytest
        run: |
          pytest -v

  run_tests_windows:
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        os: ["windows-latest"]
        python-version: ["3.8", "3.9", "3.10"]

    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v3
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest

      - name: List dependencies
        run: |
          python -m pip list

      - name: Run pytest
        run: |
          pytest -v

  # upload code coverage report
  code-coverage:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Set up Python 3.10
        uses: actions/setup-python@v2
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
          pip install pytest-cov[toml]
          pip install sh

      - name: Run tests and collect coverage
        run: pytest --cov src # NEEDS TO BE UPDATED WHEN CHANGING THE NAME OF "src" FOLDER

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3

===== .github/workflows/code-quality-main.yaml =====
# Same as `code-quality-pr.yaml` but triggered on commit to main branch
# and runs on all files (instead of only the changed ones)

name: Code Quality Main

on:
  push:
    branches: [main]

jobs:
  code-quality:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2

      - name: Run pre-commits
        uses: pre-commit/action@v2.0.3

===== notebooks/d1_dataset_construct_cw.ipynb =====
# ===== Cell 1: Markdown =====
# Cell 1: ÂØºÂÖ• (Imports)

# ===== Cell 2: Code (execution_count: 15) =====
import logging
import re
import sys
from pathlib import Path
from typing import List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from sklearn.neighbors import NearestNeighbors
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

# (Review #1) ÂØºÂÖ• PIL Âπ∂ËÆæÁΩÆ‰ª•Â§ÑÁêÜÊà™Êñ≠ÂõæÂÉè
from PIL import Image, ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

# (Review #3) ÂØºÂÖ• pyarrow Áî®‰∫éÊõ¥Á®≥ÂÅ•ÁöÑ Parquet ÂÜôÂÖ•
import pyarrow as pa
import pyarrow.parquet as pq

# Á°Æ‰øù open_clip Âú® Python Ë∑ØÂæÑ‰∏≠
# sys.path.append('/path/to/your/open_clip/src')
import open_clip

# ÈÖçÁΩÆÊó•ÂøóËÆ∞ÂΩï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

# ===== Cell 3: Markdown =====
# Cell 2: ÈÖçÁΩÆ (Configuration)

# ===== Cell 4: Code (execution_count: 42) =====
# --- Ê†∏ÂøÉÈÖçÁΩÆ ---
# üõ°Ô∏è ÂÆâÂÖ®Á¨¨‰∏Ä: ÈªòËÆ§Âú®È¢ÑÊºîÊ®°Âºè‰∏ãËøêË°å
DRYRUN = False

# (Review #12) ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê‰ª•‰øùËØÅÂèØÂ§çÁé∞ÊÄß
def set_seed(seed=2025):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
set_seed(2025)

# Ë∑ØÂæÑÈÖçÁΩÆ
BASE_DIR = Path("/cwStorage/nodecw_group/jijh/trained_models/omiclip_base_model/")
TRAIN_MANIFEST = BASE_DIR / "train_manifest.csv"
VALID_MANIFEST = BASE_DIR / "validation_manifest.csv"
MODEL_CHECKPOINT = BASE_DIR / "omiclip_epoch_50.pt"

# (Review #8) Ê£ÄÊü•ÂÖ≥ÈîÆÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
for p in [TRAIN_MANIFEST, VALID_MANIFEST, MODEL_CHECKPOINT]:
    assert p.exists(), f"ÂÖ≥ÈîÆÊñá‰ª∂Áº∫Â§±: {p}"

# (Review #4) ÂÆö‰πâÊõ¥È≤ÅÊ£íÁöÑÊ≠£ÂàôË°®ËææÂºè
FILENAME_REGEX = re.compile(r'(.+?)_(-?\d+)_(-?\d+)\.(png|jpg|jpeg|tif|tiff)$', re.IGNORECASE)

# Ê®°ÂûãÈÖçÁΩÆ
MODEL_NAME = "ViT-B-32"

# ÈÇªÂüüËÆ°ÁÆóÈÖçÁΩÆ
K_NEIGHBORS = 6

# ÊâπÂ§ÑÁêÜÂíåËÆæÂ§áÈÖçÁΩÆ
BATCH_SIZE = 128  # (Review #9) Èôç‰ΩéÈªòËÆ§ÊâπÊ¨°Â§ßÂ∞è‰ª•ÊèêÈ´òÂÖºÂÆπÊÄß

# (Review #2) Á®≥ÂÅ•ÁöÑËÆæÂ§áÈÄâÊã©ÈÄªËæë
def pick_device(prefer_cuda_index: int = 2) -> str:
    if torch.cuda.is_available():
        n = torch.cuda.device_count()
        idx = prefer_cuda_index if prefer_cuda_index < n else 0
        return f"cuda:{idx}"
    elif torch.backends.mps.is_available():
        return "mps"
    return "cpu"
DEVICE = pick_device(prefer_cuda_index=2)

# ËæìÂá∫ÈÖçÁΩÆ
ARTIFACTS_DIR = Path("/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts")
OUTPUT_NODES_PATH = ARTIFACTS_DIR / "nodes.parquet"
OUTPUT_EDGES_PATH = ARTIFACTS_DIR / "edges.parquet"
OUTPUT_IMG_EMBED_PATH = ARTIFACTS_DIR / "image_embeds.npy"
OUTPUT_TXT_EMBED_PATH = ARTIFACTS_DIR / "text_embeds.npy"

# (Review #13) ÊòæÂºèÂàõÂª∫ÁõÆÂΩï
if not DRYRUN:
    ARTIFACTS_DIR.mkdir(exist_ok=True)
    (ARTIFACTS_DIR / "clip_cache").mkdir(exist_ok=True, parents=True)
    
logging.info(f"ËÆæÂ§á: {DEVICE}")
logging.info(f"È¢ÑÊºîÊ®°Âºè (DRYRUN): {DRYRUN}")

# ===== Cell 5: Markdown =====
# Cell 3: Ê†∏ÂøÉÈÄªËæë - Êï∞ÊçÆÂä†ËΩΩ‰∏éÂ§ÑÁêÜ (Core Logic - Data Loading & Processing)

# ===== Cell 6: Code (execution_count: 37) =====
import os
from concurrent.futures import ProcessPoolExecutor
from tqdm.contrib.concurrent import process_map

# (Review #1) ÂØºÂÖ• PIL Âπ∂ËÆæÁΩÆ‰ª•Â§ÑÁêÜÊà™Êñ≠ÂõæÂÉè
from PIL import Image, ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

# --- Êñ∞Â¢ûÔºöÂÅ•Â£ÆÁöÑÂõæÂÉèÈ™åËØÅÂáΩÊï∞ ---
def is_image_valid(filepath: str) -> bool:
    """
    Ê£ÄÊü•Âçï‰∏™ÂõæÂÉèÊñá‰ª∂ÊòØÂê¶ÊúâÊïà„ÄÇ
    ËøîÂõû True Â¶ÇÊûúÊñá‰ª∂Â≠òÂú®„ÄÅÂ§ßÂ∞è‰∏ç‰∏∫Èõ∂‰∏îÂèØ‰ª•Ë¢´PILËØÜÂà´„ÄÇ
    """
    try:
        p = Path(filepath)
        # 1. Âø´ÈÄüÊ£ÄÊü•ÔºöÊòØÂê¶Â≠òÂú®ÂíåÂ§ßÂ∞èÊòØÂê¶‰∏∫0
        if not p.exists() or p.stat().st_size == 0:
            return False
        # 2. Ê∑±Â∫¶Ê£ÄÊü•ÔºöPILËÉΩÂê¶ÊâìÂºÄÂπ∂È™åËØÅ
        with Image.open(p) as img:
            img.verify()  # Âø´ÈÄüÊ£ÄÊü•Êñá‰ª∂Â§¥ÂíåÂÖÉÊï∞ÊçÆ
        return True
    except Exception:
        # ÊçïËé∑ÊâÄÊúâÂºÇÂ∏∏ (FileNotFoundError, PIL.UnidentifiedImageError, etc.)
        return False
# ------------------------------------

def parse_path(path_str: str) -> Tuple[str, int, int]:
    """‰ªéÂõæÂÉèË∑ØÂæÑ‰∏≠Ëß£Êûê sample_id, x, y ÂùêÊ†á„ÄÇ"""
    m = FILENAME_REGEX.search(Path(path_str).name)
    if not m:
        raise ValueError(f"Êó†Ê≥ïËß£ÊûêË∑ØÂæÑÊ†ºÂºè: {path_str}")
    sample_id, x, y, _ = m.groups()
    return sample_id, int(x), int(y)

def find_malformed_paths(paths: pd.Series) -> List[str]:
    """Âø´ÈÄüÊü•ÊâæÊâÄÊúâ‰∏çÁ¨¶ÂêàÈ¢ÑÊúüÊ†ºÂºèÁöÑË∑ØÂæÑ„ÄÇ"""
    return [p for p in paths if not FILENAME_REGEX.search(Path(p).name)]

def load_and_prepare_data(manifest_paths: List[Path]) -> pd.DataFrame:
    """Âä†ËΩΩ„ÄÅÂêàÂπ∂„ÄÅÈ™åËØÅ„ÄÅËß£ÊûêÂπ∂Á¥¢ÂºïÊ∏ÖÂçïÊñá‰ª∂„ÄÇ"""
    logging.info(f"‰ªé {manifest_paths} Âä†ËΩΩÊ∏ÖÂçï...")
    df = pd.concat([pd.read_csv(p) for p in manifest_paths], ignore_index=True)

    required_cols = {"image_path", "gene_sentence"}
    assert required_cols.issubset(df.columns), f"Ê∏ÖÂçïÁº∫Â∞ëÂàó: {required_cols - set(df.columns)}"

    # --- BUG FIX & ROBUSTNESS ---
    # Âú®Âä†ËΩΩ‰ªª‰ΩïÊï∞ÊçÆÂâçÔºåÂπ∂Ë°åÈ™åËØÅÊâÄÊúâÂõæÂÉèÊñá‰ª∂ÁöÑÊúâÊïàÊÄß
    logging.info(f"ÂºÄÂßãÂπ∂Ë°åÈ™åËØÅ {len(df)} ‰∏™ÂõæÂÉèÊñá‰ª∂...")
    image_paths = df['image_path'].tolist()
    # ‰ΩøÁî® process_map Âπ∂Ë°åÂ§ÑÁêÜÔºåÂπ∂ÊòæÁ§∫ËøõÂ∫¶Êù°
    # max_workers=None ‰ºö‰ΩøÁî®ÊâÄÊúâÂèØÁî®ÁöÑCPUÊ†∏ÂøÉ
    is_valid_mask = process_map(is_image_valid, image_paths, max_workers=None, chunksize=100)
    
    num_original = len(df)
    df_valid = df[is_valid_mask].copy() # ‰ΩøÁî® .copy() ÈÅøÂÖç SettingWithCopyWarning
    num_valid = len(df_valid)
    num_removed = num_original - num_valid
    
    if num_removed > 0:
        logging.warning(f"È™åËØÅÂÆåÊàê„ÄÇÁßªÈô§‰∫Ü {num_removed} ‰∏™Êó†ÊïàÊàñÊçüÂùèÁöÑÂõæÂÉèÊù°ÁõÆ„ÄÇ")
    else:
        logging.info("È™åËØÅÂÆåÊàê„ÄÇÊâÄÊúâÂõæÂÉèÊñá‰ª∂ÂùáÊúâÊïà„ÄÇ")
    # ----------------------------

    logging.info("È¢ÑÊ£ÄÊü•ÊâÄÊúâË∑ØÂæÑÊ†ºÂºè...")
    malformed = find_malformed_paths(df_valid['image_path'])
    if malformed:
        logging.error(f"ÂèëÁé∞ {len(malformed)} ‰∏™Ê†ºÂºèÈîôËØØÁöÑË∑ØÂæÑ„ÄÇÁ§∫‰æã: {malformed[:5]}")
    else:
        logging.info("ÊâÄÊúâË∑ØÂæÑÊ†ºÂºèÂùáÊúâÊïà„ÄÇ")

    logging.info("Ëß£ÊûêÊñá‰ª∂Âêç‰ª•ÊèêÂèñÂùêÊ†áÂíåÊ†∑Êú¨ID...")
    path_data = df_valid['image_path'].apply(parse_path)
    
    parsed_info_df = pd.DataFrame(path_data.tolist(), columns=['sample_id', 'x', 'y'], index=df_valid.index)
    df_final = df_valid.join(parsed_info_df)
    df_final = df_final.astype({'sample_id': 'object', 'x': 'int32', 'y': 'int32'})
    
    df_final.reset_index(drop=True, inplace=True) # ‰ΩøÁî® drop=True ÈÅøÂÖçÊóßÁ¥¢ÂºïÊàê‰∏∫Êñ∞Âàó
    df_final.reset_index(inplace=True)
    df_final.rename(columns={'index': 'tile_id'}, inplace=True)
    df_final['tile_id'] = df_final['tile_id'].astype('int64')
    
    logging.info(f"Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê„ÄÇÊúâÊïàÂõæÂùóÊÄªÊï∞ {len(df_final)}ÔºåÂàÜÂ∏ÉÂú® {df_final['sample_id'].nunique()} ‰∏™Ê†∑Êú¨‰∏≠„ÄÇ")
    return df_final

def compute_neighborhoods(df: pd.DataFrame, k: int) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """ÊåâÊ†∑Êú¨ÂàÜÁªÑËÆ°ÁÆók-ÊúÄËøëÈÇªÔºåËøîÂõûËäÇÁÇπÂíåËæπDataFrame„ÄÇ"""
    logging.info(f"ÂºÄÂßãËÆ°ÁÆó k={k} ÁöÑÊúÄËøëÈÇª...")
    
    edges = []
    
    for sample_id, group in tqdm(df.groupby('sample_id'), desc="Â§ÑÁêÜÊ†∑Êú¨"):
        coords = group[['x', 'y']].values
        n_neighbors = min(k + 1, len(coords))
        if n_neighbors <= 1:
            continue

        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(coords)
        distances, indices = nbrs.kneighbors(coords)
        
        distances, indices = distances[:, 1:], indices[:, 1:]
        
        non_zero_distances = distances[distances > 0]
        sigma = np.percentile(non_zero_distances, 50) if non_zero_distances.size > 0 else 1.0
        sigma = max(sigma, 1e-6)

        weights = np.exp(- (distances ** 2) / (2 * sigma ** 2))
        
        sum_w = weights.sum(axis=1, keepdims=True)
        alpha = np.divide(weights, np.maximum(sum_w, 1e-8))

        src_tile_ids = group['tile_id'].values
        neighbor_tile_ids = src_tile_ids[indices]

        for i in range(len(group)):
            src_id = src_tile_ids[i]
            for j in range(indices.shape[1]):
                edges.append((
                    src_id,
                    neighbor_tile_ids[i, j],
                    distances[i, j],
                    weights[i, j],
                    alpha[i, j]
                ))

    logging.info("ÈÇªÂüüËÆ°ÁÆóÂÆåÊàê„ÄÇ")
    edges_df = pd.DataFrame(edges, columns=[
        "src_tile_id", "nbr_tile_id", "distance", "weight", "alpha"
    ]).astype({
        "src_tile_id": "int64", "nbr_tile_id": "int64",
        "distance": "float32", "weight": "float32", "alpha": "float32"
    })

    nodes_df = df[['tile_id', 'sample_id', 'x', 'y', 'image_path', 'gene_sentence']].copy()
    return nodes_df, edges_df

# ===== Cell 7: Markdown =====
# Cell 4: Ê†∏ÂøÉÈÄªËæë - ÂµåÂÖ•ÁºìÂ≠ò (Core Logic - Embedding Caching)

# ===== Cell 8: Code (execution_count: 38) =====
class ClipDataset(Dataset):
    """Áî®‰∫éÊâπÈáèÊèêÂèñÁâπÂæÅÁöÑ PyTorch Dataset„ÄÇ"""
    def __init__(self, df: pd.DataFrame, preprocess_fn, tokenizer):
        self.df = df
        self.preprocess = preprocess_fn
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = self.preprocess(Image.open(row['image_path']).convert("RGB"))
        # (Review #5) Á°Æ‰øùÂßãÁªà‰º†ÈÄíÂàóË°®Áªô tokenizer
        text = self.tokenizer([row['gene_sentence']])[0]
        return image, text

def cache_embeddings(df: pd.DataFrame, model_name: str, checkpoint_path: Path, device: str, batch_size: int) -> Tuple[np.ndarray, np.ndarray]:
    """‰∏∫ÊâÄÊúâÂõæÂùóÁîüÊàêÂõæÂÉèÂíåÊñáÊú¨ÂµåÂÖ•„ÄÇ"""
    logging.info(f"Âä†ËΩΩÊ®°Âûã {model_name} ‰ªéÊ£ÄÊü•ÁÇπ {checkpoint_path}...")
    
    model, _, preprocess_val = open_clip.create_model_and_transforms(
        model_name, pretrained=None, cache_dir=str(ARTIFACTS_DIR / "clip_cache")
    )
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint.get('state_dict', checkpoint)
    if all(key.startswith('module.') for key in state_dict.keys()):
        state_dict = {k[7:]: v for k, v in state_dict.items()}
    model.load_state_dict(state_dict)
    
    model.to(device)
    model.eval()
    
    tokenizer = open_clip.get_tokenizer(model_name)
    dataset = ClipDataset(df, preprocess_val, tokenizer)
    
    # (Review #9) ‰ºòÂåñ DataLoader
    pin_memory = device.startswith("cuda")
    num_workers = 16
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers > 0)
    )
    
    image_embeds, text_embeds = [], []
    # (Review #9) ‰ΩøÁî® torch.inference_mode()
    with torch.inference_mode():
        for images, texts in tqdm(dataloader, desc="ÊèêÂèñÂµåÂÖ•"):
            images = images.to(device, non_blocking=True)
            texts = texts.to(device, non_blocking=True)
            
            image_features = model.encode_image(images)
            text_features = model.encode_text(texts)
            
            image_features = F.normalize(image_features, p=2, dim=-1)
            text_features = F.normalize(text_features, p=2, dim=-1)
            
            image_embeds.append(image_features.cpu().numpy())
            text_embeds.append(text_features.cpu().numpy())

    img_emb_array = np.vstack(image_embeds).astype("float32")
    txt_emb_array = np.vstack(text_embeds).astype("float32")
    
    logging.info("ÂµåÂÖ•ÊèêÂèñÂÆåÊàê„ÄÇ")
    return img_emb_array, txt_emb_array

# ===== Cell 9: Markdown =====
# Cell 5: ‰∏ªÊµÅÁ®ã‰∏é‰∫§‰ªò (Main Orchestration & Delivery)

# ===== Cell 10: Code (execution_count: 40) =====
def run_step0_preprocessing():
    """ÊâßË°åStep 0ÁöÑÂÆåÊï¥ÊµÅÁ®ã„ÄÇ"""
    # 1. Âä†ËΩΩÂíåÂáÜÂ§áÊï∞ÊçÆ
    base_df = load_and_prepare_data([TRAIN_MANIFEST, VALID_MANIFEST])
    
    # 2. ËÆ°ÁÆóÈÇªÂüüÔºåÂæóÂà∞ËäÇÁÇπÂíåËæπË°®
    nodes_df, edges_df = compute_neighborhoods(base_df, k=K_NEIGHBORS)
    
    # 3. ÁºìÂ≠òÂµåÂÖ•
    img_embeds, txt_embeds = cache_embeddings(
        nodes_df, # ‰ªÖÈúÄËäÇÁÇπ‰ø°ÊÅØÊù•ÊèêÂèñÂµåÂÖ•
        model_name=MODEL_NAME,
        checkpoint_path=MODEL_CHECKPOINT,
        device=DEVICE,
        batch_size=BATCH_SIZE
    )
    
    # 4. (Review #3, #10) ÊãÜÂàÜÂ≠òÂÇ®Âà∞ Parquet Âíå Numpy Êñá‰ª∂
    if not DRYRUN:
        logging.info("ÂºÄÂßãÂ∞ÜÊï∞ÊçÆÂÜôÂÖ•Á£ÅÁõò...")
        
        # ‰øùÂ≠òÂµåÂÖ•
        np.save(OUTPUT_IMG_EMBED_PATH, img_embeds)
        np.save(OUTPUT_TXT_EMBED_PATH, txt_embeds)
        logging.info(f"ÂµåÂÖ•Â∑≤‰øùÂ≠òÂà∞ .npy Êñá‰ª∂: {OUTPUT_IMG_EMBED_PATH}, {OUTPUT_TXT_EMBED_PATH}")
        
        # ‰ΩøÁî® PyArrow ‰øùÂ≠ò Parquet Êñá‰ª∂ÔºåÊõ¥Á®≥ÂÅ•
        pq.write_table(pa.Table.from_pandas(nodes_df, preserve_index=False), OUTPUT_NODES_PATH)
        logging.info(f"ËäÇÁÇπÂÖÉÊï∞ÊçÆÂ∑≤‰øùÂ≠òÂà∞: {OUTPUT_NODES_PATH}")
        
        pq.write_table(pa.Table.from_pandas(edges_df, preserve_index=False), OUTPUT_EDGES_PATH)
        logging.info(f"Ëæπ(ÈÇªÂüü)Êï∞ÊçÆÂ∑≤‰øùÂ≠òÂà∞: {OUTPUT_EDGES_PATH}")
        
        logging.info("ÊâÄÊúâÊñá‰ª∂‰øùÂ≠òÊàêÂäüÔºÅ")
    else:
        logging.warning("Â§Ñ‰∫éÈ¢ÑÊºîÊ®°ÂºèÔºåÊñá‰ª∂Êú™Ë¢´ÂÜôÂÖ•„ÄÇ")
        logging.info("ËäÇÁÇπË°®È¢ÑËßà:\n" + str(nodes_df.head()))
        logging.info("ËæπË°®È¢ÑËßà:\n" + str(edges_df.head()))
        logging.info(f"ÂõæÂÉèÂµåÂÖ•Êï∞ÁªÑÂΩ¢Áä∂: {img_embeds.shape}")
        logging.info(f"ÊñáÊú¨ÂµåÂÖ•Êï∞ÁªÑÂΩ¢Áä∂: {txt_embeds.shape}")

    return nodes_df, edges_df, img_embeds, txt_embeds

# ÊâßË°å‰∏ªÊµÅÁ®ã
nodes_df, edges_df, img_embeds, txt_embeds = run_step0_preprocessing()

# ===== Cell 11: Code (execution_count: 43) =====
def validate_output_revised():
    """Âä†ËΩΩÂπ∂È™åËØÅÊñ∞ÁîüÊàêÁöÑÊâÄÊúâÊñá‰ª∂„ÄÇ"""
    logging.info("ÂºÄÂßãÈ™åËØÅÊãÜÂàÜÂêéÁöÑÊñá‰ª∂...")
    
    paths_to_check = [
        OUTPUT_NODES_PATH, OUTPUT_EDGES_PATH, 
        OUTPUT_IMG_EMBED_PATH, OUTPUT_TXT_EMBED_PATH
    ]
    
    for p in paths_to_check:
        if not p.exists():
            logging.error(f"È™åËØÅÂ§±Ë¥•: Êñá‰ª∂‰∏çÂ≠òÂú®ÔºÅ{p}„ÄÇËØ∑Á°Æ‰øù DRYRUN=False Âπ∂ÈáçÊñ∞ËøêË°å„ÄÇ")
            return
    logging.info("ÊâÄÊúâÂøÖÈúÄÁöÑÊñá‰ª∂ÈÉΩÂ∑≤ÊâæÂà∞„ÄÇ")

    # 1. È™åËØÅËäÇÁÇπË°® (nodes.parquet)
    nodes = pd.read_parquet(OUTPUT_NODES_PATH)
    assert not nodes.empty, "ËäÇÁÇπË°®‰∏çÂ∫î‰∏∫Á©∫"
    assert nodes['tile_id'].is_unique, "ËäÇÁÇπË°®‰∏≠ÁöÑ tile_id Â∫îÂîØ‰∏Ä"
    assert nodes['tile_id'].dtype == 'int64'
    assert nodes['x'].dtype == 'int32'
    logging.info("‚úÖ ËäÇÁÇπË°®Âü∫Êú¨ÁªìÊûÑÊ≠£Á°Æ„ÄÇ")

    # 2. È™åËØÅËæπË°® (edges.parquet)
    edges = pd.read_parquet(OUTPUT_EDGES_PATH)
    assert not edges.empty, "ËæπË°®‰∏çÂ∫î‰∏∫Á©∫"
    assert 'src_tile_id' in edges.columns and 'nbr_tile_id' in edges.columns
    assert edges['src_tile_id'].isin(nodes['tile_id']).all(), "ËæπË°®‰∏≠ÁöÑÊ∫êIDÂ∫îÂú®ËäÇÁÇπË°®‰∏≠"
    assert edges['nbr_tile_id'].isin(nodes['tile_id']).all(), "ËæπË°®‰∏≠ÁöÑÈÇªÂ±ÖIDÂ∫îÂú®ËäÇÁÇπË°®‰∏≠"
    assert edges['alpha'].dtype == 'float32'
    max_neighbors = edges.groupby('src_tile_id').size().max()
    assert max_neighbors <= K_NEIGHBORS, f"Âçï‰∏™ËäÇÁÇπÁöÑÊúÄÂ§ßÈÇªÂ±ÖÊï∞ ({max_neighbors}) Ë∂ÖËøá k={K_NEIGHBORS}"
    logging.info("‚úÖ ËæπË°®Âü∫Êú¨ÁªìÊûÑÂíåÂºïÁî®ÂÆåÊï¥ÊÄßÊ≠£Á°Æ„ÄÇ")

    # 3. È™åËØÅÂµåÂÖ•Êï∞ÁªÑ (.npy)
    img_e = np.load(OUTPUT_IMG_EMBED_PATH)
    txt_e = np.load(OUTPUT_TXT_EMBED_PATH)
    assert img_e.shape[0] == len(nodes), "ÂõæÂÉèÂµåÂÖ•Êï∞ÈáèÂ∫î‰∏éËäÇÁÇπÊï∞ÈáèÂåπÈÖç"
    assert txt_e.shape[0] == len(nodes), "ÊñáÊú¨ÂµåÂÖ•Êï∞ÈáèÂ∫î‰∏éËäÇÁÇπÊï∞ÈáèÂåπÈÖç"
    
    # ÂÅáËÆæ ViT-B-32 ÁöÑÂµåÂÖ•Áª¥Â∫¶ÊòØ 512
    embed_dim = 512
    assert img_e.shape[1] == embed_dim, f"ÂõæÂÉèÂµåÂÖ•Áª¥Â∫¶ÈîôËØØ (Â∫î‰∏∫ {embed_dim})"
    assert txt_e.shape[1] == embed_dim, f"ÊñáÊú¨ÂµåÂÖ•Áª¥Â∫¶ÈîôËØØ (Â∫î‰∏∫ {embed_dim})"
    logging.info("‚úÖ ÂµåÂÖ•Êï∞ÁªÑÂΩ¢Áä∂ÂíåÁª¥Â∫¶Ê≠£Á°Æ„ÄÇ")

    logging.info("üéâ ÂÖ®ÈÉ®È™åËØÅÊàêÂäüÔºÅÊï∞ÊçÆÂ∑≤ÂáÜÂ§áÂ∞±Áª™ÔºåÂèØ‰ª•ËøõÂÖ•‰∏ã‰∏ÄÊ≠•„ÄÇ")

# ËøêË°åÈ™åËØÅ
if not DRYRUN:
    validate_output_revised()
else:
    logging.info("Â§Ñ‰∫éÈ¢ÑÊºîÊ®°ÂºèÔºåË∑≥ËøáÈ™åËØÅ„ÄÇ")

# ===== Cell 12: Markdown =====
# Cell 6 Visualization Validation

# ===== Cell 13: Code (execution_count: 45) =====
import random
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_neighborhood(nodes_df: pd.DataFrame, edges_df: pd.DataFrame):
    """
    Randomly select a tile and visualize its position and neighbors in the sample space.
    """
    # Ensure data is loaded
    if 'nodes_df' not in locals() or 'edges_df' not in locals():
        logging.info("Loading data from disk for visualization...")
        nodes_df = pd.read_parquet(OUTPUT_NODES_PATH)
        edges_df = pd.read_parquet(OUTPUT_EDGES_PATH)

    # 1. Random sampling
    # Randomly select a sample ID
    sample_ids_with_neighbors = edges_df['src_tile_id'].map(nodes_df.set_index('tile_id')['sample_id']).unique()
    if len(sample_ids_with_neighbors) == 0:
        logging.warning("No samples found in edge table, cannot perform visualization.")
        return
        
    random_sample_id = random.choice(sample_ids_with_neighbors)
    
    # Get all tiles from this sample
    sample_nodes = nodes_df[nodes_df['sample_id'] == random_sample_id]
    
    # Randomly select an anchor tile from this sample
    anchor_tile = sample_nodes.sample(1).iloc[0]
    anchor_id = anchor_tile['tile_id']
    
    logging.info(f"Visualizing anchor tile ID: {anchor_id} from sample '{random_sample_id}'")

    # 2. Data preparation
    # Get neighbor information for the anchor
    anchor_edges = edges_df[edges_df['src_tile_id'] == anchor_id]
    neighbor_ids = anchor_edges['nbr_tile_id'].tolist()
    
    # Get neighbor coordinates and weights
    neighbor_nodes = nodes_df[nodes_df['tile_id'].isin(neighbor_ids)].merge(
        anchor_edges[['nbr_tile_id', 'alpha']],
        left_on='tile_id',
        right_on='nbr_tile_id'
    )

    # 3. Visualization
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(12, 12))
    
    # Draw background: all other points in this sample
    ax.scatter(
        sample_nodes['x'], sample_nodes['y'],
        s=5, c='lightgray', alpha=0.5, label='Background (same sample)'
    )
    
    # Draw neighbors
    ax.scatter(
        neighbor_nodes['x'], neighbor_nodes['y'],
        s=50, c='royalblue', alpha=0.8, label=f'Neighbors (k={len(neighbor_nodes)})'
    )
    
    # Draw anchor point
    ax.scatter(
        anchor_tile['x'], anchor_tile['y'],
        s=200, c='red', marker='*', edgecolors='black', label='Anchor'
    )
    
    # Draw connection lines, transparency determined by alpha
    for _, neighbor in neighbor_nodes.iterrows():
        ax.plot(
            [anchor_tile['x'], neighbor['x']],
            [anchor_tile['y'], neighbor['y']],
            color='salmon',
            linewidth=1.5,
            alpha=neighbor['alpha'] * 0.8 + 0.2  # Ensure weakest lines are still visible
        )
        
    # 4. Chart styling
    ax.set_title(f"Neighborhood Visualization (Sample: {random_sample_id}, Anchor ID: {anchor_id})", fontsize=16)
    ax.set_xlabel("X Coordinate", fontsize=12)
    ax.set_ylabel("Y Coordinate", fontsize=12)
    ax.legend(loc='best')
    ax.set_aspect('equal', adjustable='box') # Ensure equal x, y axis ratios
    plt.gca().invert_yaxis() # Image coordinate system Y-axis usually points down
    plt.show()

# --- Run visualization ---
# Assume nodes_df and edges_df are already in memory
if not DRYRUN:
    try:
        visualize_neighborhood(nodes_df, edges_df)
    except NameError:
        # If variables are not in memory, load from files
        nodes_df_viz = pd.read_parquet(OUTPUT_NODES_PATH)
        edges_df_viz = pd.read_parquet(OUTPUT_EDGES_PATH)
        visualize_neighborhood(nodes_df_viz, edges_df_viz)
else:
    logging.info("In dry run mode, skipping visualization.")

# ===== Cell 14: Markdown =====
# Cell 6.1 Visualization Validation 2

# ===== Cell 15: Code (execution_count: 47) =====
import random
import textwrap
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image

def load_image_from_path(path_str: str) -> Image.Image:
    """Safely loads an image from a path."""
    try:
        return Image.open(path_str).convert("RGB")
    except Exception as e:
        logging.error(f"Failed to load image at {path_str}: {e}")
        # Return a placeholder image on failure
        return Image.new('RGB', (224, 224), color = 'red')

def visualize_retrieval(nodes_df: pd.DataFrame = None, edges_df: pd.DataFrame = None):
    """
    Randomly selects an anchor tile, retrieves its raw data (image, text), 
    finds its neighbors, retrieves their raw data, and displays them all.
    """
    # 1. Âä†ËΩΩÊï∞ÊçÆ (Â¶ÇÊûúÂ∞öÊú™Âä†ËΩΩ)
    if nodes_df is None or edges_df is None:
        logging.info("Visualizer is loading data from disk...")
        if not OUTPUT_NODES_PATH.exists() or not OUTPUT_EDGES_PATH.exists():
            logging.error("Parquet files not found. Please run the main pipeline first.")
            return
        nodes_df = pd.read_parquet(OUTPUT_NODES_PATH)
        edges_df = pd.read_parquet(OUTPUT_EDGES_PATH)

    # 2. ÈöèÊú∫ÊäΩÊ†∑‰∏Ä‰∏™ÊúâÊÑè‰πâÁöÑÈîöÁÇπ
    sample_ids_with_neighbors = edges_df['src_tile_id'].map(nodes_df.set_index('tile_id')['sample_id']).unique()
    if len(sample_ids_with_neighbors) == 0:
        logging.warning("No edges found in the dataset. Cannot visualize a neighborhood.")
        return
        
    random_sample_id = random.choice(sample_ids_with_neighbors)
    
    possible_anchors = nodes_df[
        (nodes_df['sample_id'] == random_sample_id) &
        (nodes_df['tile_id'].isin(edges_df['src_tile_id']))
    ]
    if possible_anchors.empty:
        logging.warning(f"Could not find an anchor with neighbors in sample '{random_sample_id}'. Retrying might help.")
        return
        
    anchor_tile_series = possible_anchors.sample(1).iloc[0]
    anchor_id = anchor_tile_series['tile_id']
    
    # 3. Ê£ÄÁ¥¢Êï∞ÊçÆ
    anchor_image = load_image_from_path(anchor_tile_series['image_path'])
    anchor_text = anchor_tile_series['gene_sentence']
    
    anchor_edges = edges_df[edges_df['src_tile_id'] == anchor_id].sort_values('distance')
    neighbor_ids = anchor_edges['nbr_tile_id'].tolist()
    
    # --- BUG FIX ---
    # ÈîôËØØÂéüÂõ†Ôºö‰πãÂâçÁöÑ merge Êìç‰ΩúÈÅóÊºè‰∫Ü 'distance' Âàó„ÄÇ
    # ‰øÆÂ§çÊñπÊ°àÔºöÂú® merge ÁöÑÂàóÈÄâÊã©‰∏≠Âä†ÂÖ• 'distance'„ÄÇ
    neighbor_nodes_df = nodes_df[nodes_df['tile_id'].isin(neighbor_ids)].merge(
        anchor_edges[['nbr_tile_id', 'distance', 'alpha']], # <-- Ê†∏ÂøÉ‰øÆÂ§çÁÇπ
        left_on='tile_id',
        right_on='nbr_tile_id'
    ).set_index('tile_id').loc[neighbor_ids].reset_index() # ‰øùÊåÅÊéíÂ∫è
    # --- END FIX ---

    neighbor_images = [load_image_from_path(p) for p in neighbor_nodes_df['image_path']]
    neighbor_texts = neighbor_nodes_df['gene_sentence'].tolist()
    
    logging.info(f"Visualizing anchor tile {anchor_id} and its {len(neighbor_ids)} neighbors from sample '{random_sample_id}'.")

    # 4. ÂèØËßÜÂåñ
    num_plots = 1 + len(neighbor_ids)
    fig, axes = plt.subplots(1, num_plots, figsize=(num_plots * 4, 4.5))
    
    def wrap_text(text, width=40):
        return textwrap.fill(text, width)

    axes[0].imshow(anchor_image)
    axes[0].set_title(f"Anchor Tile: {anchor_id}\n\n{wrap_text(anchor_text)}", fontsize=10)
    axes[0].axis('off')

    for i, row in enumerate(neighbor_nodes_df.itertuples(index=False)):
        ax_neighbor = axes[i + 1]
        ax_neighbor.imshow(neighbor_images[i])
        title = (
            f"Neighbor {i+1} (ID: {row.tile_id})\n"
            f"Dist: {row.distance:.2f}, Alpha: {row.alpha:.2f}\n\n" # Áé∞Âú®ÂèØ‰ª•ÂÆâÂÖ®ËÆøÈóÆ row.distance
            f"{wrap_text(row.gene_sentence)}"
        )
        ax_neighbor.set_title(title, fontsize=10)
        ax_neighbor.axis('off')
        
    plt.suptitle(f"Data Retrieval for Sample '{random_sample_id}'", fontsize=16, y=1.1)
    plt.tight_layout()
    plt.show()

# --- ËøêË°åÂèØËßÜÂåñ ---
if not DRYRUN:
    try:
        # ÂÅáËÆæ nodes_df Âíå edges_df ÂèØËÉΩÂ∑≤Âú®ÂÜÖÂ≠ò‰∏≠
        visualize_retrieval(nodes_df, edges_df)
    except NameError:
        # Â¶ÇÊûúÂèòÈáè‰∏çÂú®ÂÜÖÂ≠ò‰∏≠ÔºåÂàô‰ªéÊñá‰ª∂Âä†ËΩΩ
        visualize_retrieval()
else:
    logging.info("Â§Ñ‰∫éÈ¢ÑÊºîÊ®°ÂºèÔºåË∑≥ËøáÂèØËßÜÂåñ„ÄÇ")

# ===== Cell 16: Code =====


# ===== Cell 17: Code =====



===== notebooks/test2_multipositive_effect.ipynb =====
# ‚ö†Ô∏è WARNING: File near/over threshold (size=890939 bytes, chars=97701)
# ===== Cell 1: Code (execution_count: 15) =====
# %% [markdown]
# # Á©∫Èó¥ CLIP Ê®°ÂûãÁªºÂêàËØÑ‰º∞ÊñπÊ°à
#
# **ÁõÆÊ†á:** ÂÖ®Èù¢„ÄÅÁßëÂ≠¶Âú∞ÊØîËæÉÂü∫Á∫ø OmiCLIP Ê®°Âûã‰∏é‰ΩøÁî®Â§öÊ≠£Ê†∑Êú¨ÊçüÂ§± (Multi-Positive Loss) ËÆ≠ÁªÉÁöÑÊñ∞ "Spatial CLIP" Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ
#
# **ËØÑ‰º∞ÊµÅÁ®ã:**
# 1.  **ÂÆöÊÄßËØÑ‰º∞:** ÂØπ HEST ÁªÑÁªáÂàáÁâáËøõË°åÈõ∂Ê†∑Êú¨ÁªÜËÉûÁ±ªÂûãÊ†áÊ≥®ÔºåÂπ∂ÂèØËßÜÂåñÊØîËæÉ‰∏§‰∏™Ê®°ÂûãÁöÑÊ†áÊ≥®ÁªìÊûú„ÄÇ
# 2.  **ÂÆöÈáèËØÑ‰º∞ (Ê£ÄÁ¥¢):** ËÆ°ÁÆó‰º†ÁªüÁöÑÂõæÊñá‰∫íÊ£ÄÁ¥¢ÊåáÊ†á (Recall@k) ÂíåÁõ∏‰ººÂ∫¶ÁÉ≠ÂäõÂõæÔºåËØÑ‰º∞Ê®°ÂûãÂü∫Á°ÄÁöÑÂØπÈΩêËÉΩÂäõ„ÄÇ
# 3.  **ÂÆöÈáèËØÑ‰º∞ (ÂàÜÁ±ª):** Âü∫‰∫éÈ´òË°®ËææÂü∫Âõ†ÂàõÂª∫‰º™Ê†áÁ≠æÔºåËØÑ‰º∞‰∏§‰∏™Ê®°ÂûãÂú®Èõ∂Ê†∑Êú¨ÂàÜÁ±ª‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÁéá„ÄÅF1ÂàÜÊï∞ÂíåÊ∑∑Ê∑ÜÁü©Èòµ„ÄÇ

# %% [markdown]
# ### 1. ÂØºÂÖ•‰∏éÁéØÂ¢ÉÈÖçÁΩÆ (Imports & Configuration)

# %%
# --- Ê†∏ÂøÉÂéüÂàô ---
# üõ°Ô∏è ÂÆâÂÖ®Á¨¨‰∏Ä: ÈªòËÆ§Âú®È¢ÑÊºîÊ®°Âºè‰∏ãËøêË°å‰ª•Ê£ÄÊü•Ë∑ØÂæÑÂíåÈÖçÁΩÆ
DRYRUN = False

# --- È°πÁõÆË∑ØÂæÑÈÖçÁΩÆ (Ê†∏ÂøÉ‰øÆÊ≠£) ---
import sys
from pathlib import Path

# Â∞ÜÊÇ®ÁöÑÈ°πÁõÆÊ†πÁõÆÂΩïÊ∑ªÂä†Âà∞ Python ÊêúÁ¥¢Ë∑ØÂæÑ
PROJECT_ROOT = Path("/home1/jijh/diffusion_project/git_repo/yuanspace")
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))
    print(f"Â∑≤Â∞Ü '{PROJECT_ROOT}' Ê∑ªÂä†Âà∞ sys.path")

from src.spaglam_preproc.utils.hest_loading import HESTDataset

    
# --- Ê†áÂáÜÂ∫ì ---
import os
import sys
import logging
from pathlib import Path
from typing import Dict, Any, Tuple, List

# --- Á¨¨‰∏âÊñπÂ∫ì ---
import torch
import numpy as np
import pandas as pd
import scanpy as sc
from PIL import Image
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
from torch.utils.data import DataLoader, Dataset, Subset

# --- Êú¨Âú∞È°πÁõÆÂ∫ì ---
# Á°Æ‰øù open_clip Âú® Python Ë∑ØÂæÑ‰∏≠
# ËØ∑Ê†πÊçÆÊÇ®ÁöÑÁéØÂ¢ÉË∞ÉÊï¥Ê≠§Ë∑ØÂæÑ
try:
    import open_clip
except ImportError:
    # ÂÅáËÆæÊ≠§ notebook ‰Ωç‰∫éÈ°πÁõÆÊ†πÁõÆÂΩï
    sys.path.append('./src')
    import open_clip

# --- ÈÖçÁΩÆÊó•Âøó ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout)

# %%
# --- Ë∑ØÂæÑ‰∏éÊ®°ÂûãÈÖçÁΩÆ ---
# (Review #8) ‰ΩøÁî® Path ÂØπË±°ËøõË°åÁ®≥ÂÅ•ÁöÑË∑ØÂæÑÁÆ°ÁêÜ
# --- Êï∞ÊçÆË∑ØÂæÑ ---
HEST_DATA_DIR = Path("/cwStorage/nodecw_group/jijh/hest_1k")
ARTIFACTS_DIR = Path("/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts")
NODES_PATH = ARTIFACTS_DIR / "nodes.parquet"
IMG_EMBED_PATH = ARTIFACTS_DIR / "image_embeds.npy" # Âü∫Á∫øÊ®°ÂûãÁöÑÈ¢ÑËÆ°ÁÆóÂµåÂÖ•

OUTPUT_DIR = Path("/cwStorage/nodecw_group/jijh/trained_models/all_comparisons/multipositive_vs_basline")

# --- Ê®°ÂûãÊ£ÄÊü•ÁÇπË∑ØÂæÑ ---
BASELINE_MODEL_CKPT = Path("/cwStorage/nodecw_group/jijh/trained_models/omiclip_base_model/omiclip_epoch_50.pt")
SPATIAL_MODEL_CKPT = Path("/cwStorage/nodecw_group/jijh/trained_models/spatial_clip_base_model/multi_positice_loss50.pt")

# --- Ê®°ÂûãÊû∂ÊûÑ ---
MODEL_NAME = "ViT-B-32"

# --- Êé®ÁêÜÈÖçÁΩÆ ---
BATCH_SIZE = 512
NUM_WORKERS = 16
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
DEVICE_COUNT = torch.cuda.device_count()

# --- ÂæÖÊµãËØïÊ®°ÂûãÊ∏ÖÂçï ---
MODELS_TO_TEST = {
    "OmiCLIP (Baseline)": {
        "path": BASELINE_MODEL_CKPT,
        "model_name": MODEL_NAME,
    },
    "Spatial CLIP (Ours)": {
        "path": SPATIAL_MODEL_CKPT,
        "model_name": MODEL_NAME,
    },
}

# --- Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â≠òÂú® ---
for name, config in MODELS_TO_TEST.items():
    assert config['path'].exists(), f"Ê®°Âûã '{name}' ÁöÑÊ£ÄÊü•ÁÇπÊñá‰ª∂‰∏çÂ≠òÂú®: {config['path']}"
assert HEST_DATA_DIR.exists(), f"HEST Êï∞ÊçÆÁõÆÂΩï‰∏çÂ≠òÂú®: {HEST_DATA_DIR}"

if not DRYRUN:
    # Âè™ÊúâÂú®ÈùûÈ¢ÑÊºîÊ®°Âºè‰∏ãÊâçÊ£ÄÊü•Ëøô‰∫õÁîüÊàêÁöÑÊñá‰ª∂
    assert NODES_PATH.exists(), f"È¢ÑÂ§ÑÁêÜÁîüÊàêÁöÑ nodes.parquet ‰∏çÂ≠òÂú®: {NODES_PATH}"
    assert IMG_EMBED_PATH.exists(), f"È¢ÑÂ§ÑÁêÜÁîüÊàêÁöÑ image_embeds.npy ‰∏çÂ≠òÂú®: {IMG_EMBED_PATH}"
else:
    logging.warning("Â§Ñ‰∫éÈ¢ÑÊºîÊ®°Âºè (DRYRUN=True)„ÄÇ‰∏ç‰ºöÊâßË°åÊñá‰ª∂ÂÜôÂÖ•ÂíåÈÉ®ÂàÜÊï∞ÊçÆÂä†ËΩΩ„ÄÇ")

logging.info(f"ÈÖçÁΩÆÂÆåÊàê„ÄÇ‰ΩøÁî®ËÆæÂ§á: {DEVICE}")
logging.info(f"ÂæÖËØÑ‰º∞Ê®°ÂûãÊï∞Èáè: {len(MODELS_TO_TEST)}")



# ===== Cell 2: Code (execution_count: 12) =====
# %% [markdown]
# ### 2. Ê†∏ÂøÉÂáΩÊï∞ÔºöÊ®°ÂûãÂä†ËΩΩ‰∏éÊï∞ÊçÆÂáÜÂ§á (Core Functions: Model Loading & Data Prep)

# %%
def load_clip_model(checkpoint_path: Path, model_name: str, device: str) -> torch.nn.Module:
    """
    ‰ªéÁªôÂÆöÁöÑÊ£ÄÊü•ÁÇπË∑ØÂæÑÂä†ËΩΩ CLIP Ê®°Âûã„ÄÇ
    - Ëá™Âä®Â§ÑÁêÜ 'module.' ÂâçÁºÄ„ÄÇ
    - Â∞ÜÊ®°ÂûãËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°ÂºèÂπ∂ÁßªÂä®Âà∞ÊåáÂÆöËÆæÂ§á„ÄÇ
    """
    logging.info(f"Ê≠£Âú®Âä†ËΩΩÊ®°Âûã '{model_name}' ‰ªé: {checkpoint_path}")
    
    # 1. ÂàõÂª∫Ê®°ÂûãÁªìÊûÑ
    model, _, _ = open_clip.create_model_and_transforms(model_name, pretrained=None)
    
    # 2. Âä†ËΩΩÁä∂ÊÄÅÂ≠óÂÖ∏
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    # 3. Ê∏ÖÁêÜ 'module.' ÂâçÁºÄÔºàÊù•Ëá™ DDP ËÆ≠ÁªÉÔºâ
    if all(key.startswith('module.') for key in state_dict.keys()):
        state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}
        
    # 4. Âä†ËΩΩÊùÉÈáç
    model.load_state_dict(state_dict)
    
    # 5. ÂáÜÂ§áÊé®ÁêÜ
    model.to(device)
    model.eval()
    
    logging.info(f"Ê®°Âûã '{model_name}' Âä†ËΩΩÊàêÂäü„ÄÇ")
    return model

def get_hest_sample(sample_id: str = 'TENX156') -> Any:
    """Âä†ËΩΩÂπ∂ÂáÜÂ§á‰∏Ä‰∏™ HEST Ê†∑Êú¨Áî®‰∫éÂàÜÊûê„ÄÇ"""
    from src.spaglam_preproc.utils.hest_loading import HESTDataset
    
    logging.info(f"Âä†ËΩΩ HEST Ê†∑Êú¨: {sample_id}...")
    hest_dataset = HESTDataset(data_dir=HEST_DATA_DIR)
    sample = hest_dataset.get_samples(sample_ids=[sample_id])[0]
    sample.load_st_data(lazy=False)
    sample.load_wsi()
    
    # ‰øÆÊ≠£ scanpy ÊúüÊúõÁöÑ spatial ÁªìÊûÑ
    spatial_data = sample.adata.uns['spatial']['ST']
    library_id = sample.sample_id
    sample.adata.uns['spatial'] = {library_id: spatial_data}
    
    logging.info(f"Ê†∑Êú¨ '{sample.sample_id}' Âä†ËΩΩÂπ∂ÂáÜÂ§áÂÆåÊØï„ÄÇ")
    return sample

# --- ‰∏∫Êé®ÁêÜÂÆö‰πâ PyTorch Dataset ---
class WSISpotDataset(Dataset):
    """‰∏Ä‰∏™Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜÔºåÁî®‰∫é‰ªéWSI‰∏≠ÊåâÈúÄÂä†ËΩΩspotÂØπÂ∫îÁöÑÂõæÂùó„ÄÇ"""
    def __init__(self, wsi, coords, transform):
        self.wsi = wsi
        self.coords = coords
        self.transform = transform
        self.patch_size = (224, 224)
        self.patch_radius = self.patch_size[0] // 2

    def __len__(self):
        return len(self.coords)

    def __getitem__(self, idx):
        c = self.coords[idx]
        top_left_coord = (int(c[0] - self.patch_radius), int(c[1] - self.patch_radius))
        tile = self.wsi.read_region(top_left_coord, 0, self.patch_size).convert("RGB")
        if self.transform:
            tile = self.transform(tile)
        return tile



# ===== Cell 3: Code (execution_count: 13) =====
# %% [markdown]
# ### 3. Èò∂ÊÆµ‰∏ÄÔºöÂÆöÊÄßËØÑ‰º∞ - Èõ∂Ê†∑Êú¨Á©∫Èó¥Ê†áÊ≥® (Qualitative Evaluation)
#
# Âú®Ê≠§Èò∂ÊÆµÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®‰∏§‰∏™Ê®°ÂûãÂØπÂêå‰∏Ä‰∏™ÁªÑÁªáÂàáÁâáËøõË°åÈõ∂Ê†∑Êú¨ÁªÜËÉûÁ±ªÂûãÊ†áÊ≥®ÔºåÂπ∂ÂèØËßÜÂåñÁªìÊûú‰ª•ËøõË°åÁõ¥ËßÇÊØîËæÉ„ÄÇ

# %%
# --- ÂÆö‰πâÁîüÁâ©Â≠¶Êü•ËØ¢ ---
text_queries = {
    "Tumor Epithelium": "EPCAM MKI67 TOP2A PCNA KRT8 KRT18 KRT19 TACSTD2 CENPF UBE2C BIRC5 MCM2 MCM3 MCM4 MCM5 MCM6 MCM7 CDK1 CCNB1 CCNB2 AURKA AURKB CEACAM5 CEACAM6 MUC1 CLDN4 CLDN7 CD44 SLC2A1 S100P S100A6 SOX4 SOX9 TFF3 AGR2 RRM2 TYMS TPX2 BUB1B KIF2C KIF11 PLK1 GPC3 MDK PTTG1 LAMC2 LAMA3 ITGA6 ITGB4 SLPI",
    "T Cells": "CD3D CD3E CD3G CD2 CD28 CD247 PTPRC CD4 CD8A CD8B LAT GZMA GZMB GZMK PRF1 IFNG CCL5 CXCL9 CXCL10 ICOS LAG3 PDCD1 CTLA4 TIGIT HAVCR2 ENTPD1 FOXP3 IL2RA IKZF2 CCR7 SELL LEF1 TCF7 IL7R KLRB1 KLRG1 NKG7 ZAP70 LCK FYN THEMIS TRAT1 ITK CD27 CD5 CD6 CD7 SH2D1A",
    "B Cells": "MS4A1 CD19 CD79A CD79B CD22 PAX5 EBF1 BANK1 BLK CD24 FCRL5 TCL1A CD38 SDC1 XBP1 IRF4 PRDM1 IGHG1 IGHG2 IGHG3 IGHG4 IGHA1 IGHA2 IGHM IGHD IGHE IGKC IGKV1-5 JCHAIN DERL3 FCRL4 SLAMF7 POU2F2 BACH2 SPIB VPREB1 VPREB3 IGLL5 LTB CD27 TNFRSF17 TNFRSF13B MZB1",
    "Macrophages": "CD68 CD163 CD14 CSF1R MRC1 CD86 CD80 FCGR3A FCGR1A FCGR2A ITGAM ITGAX AIF1 C1QA C1QB C1QC APOE LYZ MSR1 CD209 CLEC7A CLEC10A HLA-DPA1 HLA-DPB1 HLA-DQA1 HLA-DQB1 HLA-DRA HLA-DRB1 HLA-DRB5 CD74 MAF MARCO STAB1 SPP1 TREM2 LGALS3 S100A8 S100A9 MKI67 FCN1 VCAN",
    "Fibroblasts": "FAP ACTA2 COL1A1 COL1A2 COL3A1 COL5A1 DCN LUM BGN POSTN FN1 SPARC MMP2 MMP9 MMP11 MMP14 TIMP1 PDGFRA PDGFRB TGFB1 TGFBI THBS1 THBS2 VIM S100A4 FBLN1 FBLN2 FBLN5 TNC MFAP5 CTHRC1 GREM1 RGS5 CDH11 ASPN PLAU CXCL12 WNT2 WNT5A SULF1 LOX LOXL1 LOXL2 ADAM12",
    "Endothelial Cells": "PECAM1 CDH5 KDR FLT1 VWF CLDN5 ENG TIE1 TEK ICAM1 ICAM2 VCAM1 SELE SELP ACKR1 EMCN ESAM STAB1 STAB2 RAMP2 RAMP3 ADGRF5 GJA4 GJA5 PLVAP AQP1 SEMA3G PTPRB ROBO4 CD34 CD93 ERG FLI1 SOX18 EGFL7 ECSCR PLXND1 MMRN2 CD36 NRP1 NRP2 IGFBP7",
    "Smooth Muscle": "ACTA2 MYH11 TAGLN TPM1 TPM2 CNN1 CNN2 DES CALD1 MYLK MYL9 LMOD1 ACTG2 MYOCD SRF MEF2C MEF2D PLN PTPRB CASQ2 PPP1R12A PPP1R12B MYL6 ACTB GSN FLNA ADIRF CAV1 CAV2 ITGA7 ITGA8 PDLIM7 PDLIM5 SYNM SYNPO2 SLMAP RGS5 NOTCH3 HEYL HES4 PLN TRPC6 ANO1 KCNMA1"
}

tokenizer = open_clip.get_tokenizer(MODEL_NAME)
query_labels = list(text_queries.keys())

# --- Âä†ËΩΩÊï∞ÊçÆ ---
sample = get_hest_sample()
all_results = {}

# --- ËøêË°åÊé®ÁêÜ ---
for name, config in MODELS_TO_TEST.items():
    model = load_clip_model(config['path'], config['model_name'], DEVICE)
    _, _, image_preprocessor = open_clip.create_model_and_transforms(config['model_name'])

    spot_dataset = WSISpotDataset(wsi=sample.wsi, coords=sample.adata.obsm['spatial'], transform=image_preprocessor)
    dataloader = DataLoader(spot_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

    all_spot_embeddings = []
    with torch.inference_mode():
        tokenized_queries = tokenizer(list(text_queries.values())).to(DEVICE)
        text_features = model.encode_text(tokenized_queries, normalize=True)

        for image_batch in tqdm(dataloader, desc=f"Annotating with {name}"):
            image_batch = image_batch.to(DEVICE, non_blocking=True)
            image_features = model.encode_image(image_batch, normalize=True)
            all_spot_embeddings.append(image_features.cpu())
    
    # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶Âπ∂Ëé∑ÂèñÈ¢ÑÊµãÁªìÊûú
    all_spot_embeddings_tensor = torch.cat(all_spot_embeddings)
    similarity = all_spot_embeddings_tensor.to(DEVICE) @ text_features.T
    confidence, predictions = torch.max(torch.softmax(similarity, dim=1), dim=1)

    predicted_labels = pd.Series([query_labels[i] for i in predictions.cpu().numpy()], index=sample.adata.obs.index)
    confidence_scores = pd.Series(confidence.cpu().numpy(), index=sample.adata.obs.index)

    all_results[name] = {"labels": predicted_labels, "confidences": confidence_scores}

    # Ê∏ÖÁêÜ GPU ÂÜÖÂ≠ò
    del model, text_features
    torch.cuda.empty_cache()

# --- Â∞ÜÁªìÊûúÊ∑ªÂä†Âà∞ AnnData ÂØπË±°‰∏≠ ---
for exp_name, results in all_results.items():
    sample.adata.obs[f"anno_{exp_name}"] = pd.Categorical(results['labels'], categories=query_labels)
    sample.adata.obs[f"conf_{exp_name}"] = results['confidences']



# ===== Cell 4: Code (execution_count: 16) =====
# %% [markdown]
# #### 1.1 ÂèØËßÜÂåñÊ†áÊ≥®ÁªìÊûú

# %%
# --- ÂáÜÂ§áË∞ÉËâ≤ÊùøÂíåÁªòÂõæ ---
categories = list(text_queries.keys())
base_colors = plt.cm.tab20(np.linspace(0, 1, len(categories)))
celltype_palette = dict(zip(categories, base_colors))

fig, axes = plt.subplots(1, len(MODELS_TO_TEST), figsize=(7 * len(MODELS_TO_TEST), 12), squeeze=False)
fig.suptitle(f"Zero-Shot Annotation Comparison ‚Äì Sample {sample.sample_id}", fontsize=20, y=0.9)

plot_params = {
    "show": False, "legend_loc": None, "frameon": False,
    "library_id": sample.sample_id, "img_key": 'downscaled_fullres',
    "size": 1.2, "s": 3,
}

for i, (name, _) in enumerate(MODELS_TO_TEST.items()):
    ax = axes[0, i]
    sc.pl.spatial(
        sample.adata,
        color=f"anno_{name}",
        title=f"{name}\n(Predicted Cell Type)",
        ax=ax,
        palette=celltype_palette,
        **plot_params
    )

handles = [mpatches.Patch(color=celltype_palette[cat], label=cat) for cat in categories]
fig.legend(handles=handles, title="Cell Types", loc='center right', bbox_to_anchor=(1.05, 0.5))

plt.tight_layout(rect=[0, 0, 0.90, 0.96])

if not DRYRUN:
    plt.savefig(OUTPUT_DIR / f"spatial_annotation_comparison_{sample.sample_id}.png", dpi=300, bbox_inches='tight')
plt.show()



# ===== Cell 5: Code (execution_count: 19) =====
sample.adata

# ===== Cell 6: Code (execution_count: 27) =====
sc.pl.spatial(
    sample.adata,
    color=f"total_counts",
    title=f"total_counts",
    s = 0,
    **plot_params_new
)

# ===== Cell 7: Code (execution_count: 21) =====
sc.pl.spatial(
    sample.adata,
    color=f"total_counts",
    title=f"total_counts",
    **plot_params
)

# ===== Cell 8: Code (execution_count: 17) =====
# %% [markdown]
# ### 4. Èò∂ÊÆµ‰∫åÔºöÂÆöÈáèËØÑ‰º∞ - ÂõæÊñáÊ£ÄÁ¥¢ (Quantitative Evaluation - Retrieval)
#
# Êàë‰ª¨ËØÑ‰º∞Ê®°ÂûãÂ∞ÜÂõæÂùó‰∏éÂÖ∂ÂØπÂ∫îÁöÑÂü∫Âõ†Ë°®ËææÊñáÊú¨Ê≠£Á°ÆÂåπÈÖçÁöÑËÉΩÂäõ„ÄÇËøôÂèçÊò†‰∫ÜÊ®°ÂûãÂü∫Á°ÄÁöÑÂõæÊñáÂØπÈΩêÊÄßËÉΩ„ÄÇ

# %%
class GeneSpotDataset(Dataset):
    """Áî®‰∫éÊ£ÄÁ¥¢ËØÑ‰º∞ÁöÑÊï∞ÊçÆÈõÜÔºåËøîÂõûÂõæÂùóÂíåÂØπÂ∫îÁöÑÂü∫Âõ†Âè•Â≠ê„ÄÇ"""
    def __init__(self, df, image_preprocessor, tokenizer):
        self.df = df
        self.image_preprocessor = image_preprocessor
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = self.image_preprocessor(Image.open(row['image_path']).convert("RGB"))
        # Ê≥®ÊÑèÔºöËøôÈáåÊàë‰ª¨‰∏çËøõË°å [0] Á¥¢ÂºïÔºåÂõ†‰∏∫ tokenizer ‰ºöÂ§ÑÁêÜÂàóË°®
        text = row['gene_sentence']
        return image, text

def calculate_retrieval_metrics(image_features: torch.Tensor, text_features: torch.Tensor) -> Dict[str, float]:
    """ËÆ°ÁÆóÂõæÊñá‰∫íÊ£ÄÁ¥¢ÁöÑ Recall@k ÊåáÊ†á„ÄÇ"""
    metrics = {}
    
    # Image-to-Text Retrieval
    logits_per_image = image_features @ text_features.T
    ground_truth = torch.arange(len(image_features)).to(logits_per_image.device)
    
    ranks = torch.argsort(logits_per_image, descending=True)
    preds = torch.where(ranks == ground_truth.unsqueeze(1))[1]
    
    for k in [1, 5, 10]:
        metrics[f"image_to_text_R@{k}"] = (preds < k).float().mean().item()
        
    # Text-to-Image Retrieval
    logits_per_text = logits_per_image.T
    ranks = torch.argsort(logits_per_text, descending=True)
    preds = torch.where(ranks == ground_truth.unsqueeze(1))[1]
    
    for k in [1, 5, 10]:
        metrics[f"text_to_image_R@{k}"] = (preds < k).float().mean().item()
        
    return metrics, logits_per_image.cpu().numpy()

# --- ËøêË°åÊ£ÄÁ¥¢ËØÑ‰º∞ ---
retrieval_results = []
nodes_df = pd.read_parquet(NODES_PATH)
# ‰∏∫ËäÇÁúÅÊó∂Èó¥ÔºåÊàë‰ª¨Âè™Âú® 2048 ‰∏™ÈöèÊú∫Ê†∑Êú¨‰∏äËøõË°åËØÑ‰º∞
subset_indices = np.random.choice(len(nodes_df), size=min(2048, len(nodes_df)), replace=False)
eval_df = nodes_df.iloc[subset_indices]

# Ëé∑ÂèñÈ¢ÑÂ§ÑÁêÜÂô®
_, _, image_preprocessor = open_clip.create_model_and_transforms(MODEL_NAME)
eval_dataset = GeneSpotDataset(eval_df, image_preprocessor, tokenizer)
eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)

final_similarity_matrices = {}

for name, config in MODELS_TO_TEST.items():
    model = load_clip_model(config['path'], config['model_name'], DEVICE)
    
    all_image_features, all_text_features = [], []
    with torch.inference_mode():
        for images, texts in tqdm(eval_dataloader, desc=f"Retrieval eval for {name}"):
            images = images.to(DEVICE)
            texts = tokenizer(texts).to(DEVICE)
            
            image_features = model.encode_image(images, normalize=True)
            text_features = model.encode_text(texts, normalize=True)
            
            all_image_features.append(image_features)
            all_text_features.append(text_features)
            
    img_f = torch.cat(all_image_features)
    txt_f = torch.cat(all_text_features)
    
    metrics, sim_matrix = calculate_retrieval_metrics(img_f, txt_f)
    metrics['model'] = name
    retrieval_results.append(metrics)
    final_similarity_matrices[name] = sim_matrix
    
    del model
    torch.cuda.empty_cache()

retrieval_df = pd.DataFrame(retrieval_results).set_index('model')
print("\n--- Retrieval Performance ---")
print(retrieval_df)

# %% [markdown]
# #### 4.1 ÂèØËßÜÂåñÁõ∏‰ººÂ∫¶ÁÉ≠ÂäõÂõæ

# %%
fig, axes = plt.subplots(1, len(MODELS_TO_TEST), figsize=(8 * len(MODELS_TO_TEST), 7))
if len(MODELS_TO_TEST) == 1: axes = [axes] # Á°Æ‰øù axes ÊòØÂèØËø≠‰ª£ÁöÑ

for ax, (name, sim_matrix) in zip(axes, final_similarity_matrices.items()):
    sns.heatmap(sim_matrix, ax=ax, cmap='viridis')
    ax.set_title(f"Similarity Matrix - {name}")
    ax.set_xlabel("Text Samples")
    ax.set_ylabel("Image Samples")

plt.tight_layout()
if not DRYRUN:
    plt.savefig(OUTPUT_DIR / "retrieval_similarity_heatmaps.png", dpi=300)
plt.show()



# ===== Cell 9: Code (execution_count: 18) =====
# %% [markdown]
# ### 5. Èò∂ÊÆµ‰∏âÔºöÂÆöÈáèËØÑ‰º∞ - Èõ∂Ê†∑Êú¨ÂàÜÁ±ª (Quantitative Evaluation - Classification)
#
# Êàë‰ª¨ÈÄöËøáÂü∫Âõ†Ë°®ËææÊï∞ÊçÆÂàõÂª∫‰º™Ê†áÁ≠æÔºå‰ª•ËØÑ‰º∞Ê®°ÂûãÂú®ÁâπÂÆöÁîüÁâ©Â≠¶Á±ªÂà´‰∏äÁöÑÂàÜÁ±ªÂáÜÁ°ÆÊÄß„ÄÇ

# %%
def create_pseudo_labels(adata: sc.AnnData, queries: Dict[str, str], top_n_spots: int = 50) -> pd.DataFrame:
    """Âü∫‰∫éÂçï‰∏™Ê†áÂøóÊÄßÂü∫Âõ†ÁöÑÈ´òË°®ËææÂå∫ÂüüÂàõÂª∫‰º™Ê†áÁ≠æ„ÄÇ"""
    pseudo_labels = pd.Series(index=adata.obs.index, dtype=str)
    
    # ‰∏∫ÊØè‰∏™Á±ªÂà´ÈÄâÊã©‰∏Ä‰∏™‰ª£Ë°®ÊÄßÂº∫„ÄÅÈÄöÂ∏∏Â∑ÆÂºÇË°®ËææÁöÑÂü∫Âõ†
    marker_map = {
        "Tumor Epithelium": "EPCAM", "T Cells": "CD3D", "B Cells": "MS4A1",
        "Macrophages": "CD68", "Fibroblasts": "FAP", "Endothelial Cells": "PECAM1",
        "Smooth Muscle": "ACTA2"
    }
    
    available_genes = adata.var_names
    
    for label, marker in marker_map.items():
        if marker in available_genes:
            gene_expression = adata[:, marker].X.toarray().flatten()
            top_indices = np.argsort(gene_expression)[-top_n_spots:]
            pseudo_labels.iloc[top_indices] = label
            
    return pseudo_labels.dropna()

# --- ËøêË°åÂàÜÁ±ªËØÑ‰º∞ ---
pseudo_ground_truth = create_pseudo_labels(sample.adata, text_queries)
logging.info(f"ÂàõÂª∫‰∫Ü {len(pseudo_ground_truth)} ‰∏™‰º™Ê†áÁ≠æÁî®‰∫éÂàÜÁ±ªËØÑ‰º∞„ÄÇ")
classification_results = []

for name, results in all_results.items():
    predictions = results['labels'].loc[pseudo_ground_truth.index]
    
    acc = accuracy_score(pseudo_ground_truth, predictions)
    f1 = f1_score(pseudo_ground_truth, predictions, average='weighted')
    
    classification_results.append({"model": name, "accuracy": acc, "f1_weighted": f1})
    
    # ÁªòÂà∂Ê∑∑Ê∑ÜÁü©Èòµ
    cm = confusion_matrix(pseudo_ground_truth, predictions, labels=query_labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=query_labels)
    
    fig, ax = plt.subplots(figsize=(8, 8))
    disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
    ax.set_title(f"Confusion Matrix - {name}")
    if not DRYRUN:
        plt.savefig(OUTPUT_DIR / f"confusion_matrix_{name.replace(' ', '_')}.png", dpi=300, bbox_inches='tight')
    plt.show()

classification_df = pd.DataFrame(classification_results).set_index('model')
print("\n--- Zero-Shot Classification Performance (on Pseudo-Labels) ---")
print(classification_df)

# %% [markdown]
# ### 6. ÁªìËÆ∫‰∏éÊÄªÁªì
#
# Âú®Ëøô‰∏™ Notebook ‰∏≠ÔºåÊàë‰ª¨‰ªé‰∏â‰∏™ÊñπÈù¢ÂØπÂü∫Á∫ø OmiCLIP ÂíåÊñ∞ÁöÑ Spatial CLIP Ê®°ÂûãËøõË°å‰∫ÜËØÑ‰º∞Ôºö
#
# 1.  **ÂÆöÊÄßÁ©∫Èó¥Ê†áÊ≥®:** ËßÜËßâÂØπÊØî‰∫Ü‰∏§‰∏™Ê®°ÂûãÂú® HEST ÂàáÁâá‰∏äÁöÑÊ†áÊ≥®ÁªìÊûú„ÄÇSpatial CLIP ÊòØÂê¶Âú®ÁîüÁâ©Â≠¶‰∏äÊõ¥ÂêàÁêÜÂú∞ËÅöÈõÜ‰∫ÜÁªÜËÉûÁ±ªÂûãÔºüÂÆÉÊòØÂê¶Êõ¥Â•ΩÂú∞ËØÜÂà´‰∫ÜËÇøÁò§ËæπÁïåÊàñÂÖçÁñ´Êµ∏Ê∂¶Âå∫ÂüüÔºü
# 2.  **ÂÆöÈáèÊ£ÄÁ¥¢ÊÄßËÉΩ:** ÈÄöËøá Recall@k ÊåáÊ†áÔºåÊàë‰ª¨Ë°°Èáè‰∫ÜÊ®°ÂûãÁöÑÂü∫Á°ÄÂõæÊñáÂåπÈÖçËÉΩÂäõ„ÄÇÂ¶ÇÊûú Spatial CLIP ÁöÑÊåáÊ†áÊòæËëó‰Ωé‰∫éÂü∫Á∫øÔºåÂèØËÉΩÊÑèÂë≥ÁùÄÂú®Â≠¶‰π†Á©∫Èó¥ÂÖ≥Á≥ªÊó∂Áâ∫Áâ≤‰∫Ü‰∏ÄÈÉ®ÂàÜÊ†∏ÂøÉÂØπÈΩêËÉΩÂäõ„ÄÇ
# 3.  **ÂÆöÈáèÂàÜÁ±ªÊÄßËÉΩ:** ÈÄöËøáÂú®‰º™Ê†áÁ≠æ‰∏äÁöÑÂáÜÁ°ÆÁéáÂíå F1 ÂàÜÊï∞ÔºåÊàë‰ª¨ÂæóÂà∞‰∫Ü‰∏Ä‰∏™Êõ¥ÂÆ¢ËßÇÁöÑÊÄßËÉΩÂ∫¶Èáè„ÄÇSpatial CLIP Âú®Ëøô‰∏™‰ªªÂä°‰∏äÁöÑÊèêÂçáÂ∞ÜÊòØÂÖ∂ÊúâÊïàÊÄßÁöÑÊúâÂäõËØÅÊçÆ„ÄÇ
#
# ÁªºÂêà‰ª•‰∏ä‰∏âÁÇπÔºåÊàë‰ª¨ÂèØ‰ª•ÂæóÂá∫ÂÖ≥‰∫éÂ§öÊ≠£Ê†∑Êú¨ÊçüÂ§±ÊúâÊïàÊÄßÁöÑÂàùÊ≠•ÁªìËÆ∫„ÄÇ

# ===== Cell 10: Code (execution_count: 28) =====
# %% [markdown]
# ### 6. ÂèØÈáçÁî®ÁöÑÂàÜÁ±ª‰∏éÂèØËßÜÂåñÂ∑•‰ΩúÊµÅ (Reusable Classification Workflow)
#
# Ëøô‰∏™ÂáΩÊï∞Â∞ÜÊï¥‰∏™Èõ∂Ê†∑Êú¨ÂàÜÁ±ª„ÄÅÁªìÊûúÂ≠òÂÇ®ÂíåÂèØËßÜÂåñÁöÑËøáÁ®ãÊâìÂåÖÂú®‰∏ÄËµ∑„ÄÇÊÇ®ÂèØ‰ª•‰∏∫‰∏çÂêåÁöÑÁîüÁâ©Â≠¶ÈóÆÈ¢òÂÆö‰πâ‰∏çÂêåÁöÑ `text_queries` Â≠óÂÖ∏ÔºåÁÑ∂ÂêéË∞ÉÁî®Ê≠§ÂáΩÊï∞Êù•Âø´ÈÄüÁîüÊàêÁªìÊûú„ÄÇ

# %%
def run_and_visualize_classification(
    models_to_test: Dict[str, Dict[str, Any]],
    hest_sample: Any, # HESTSample object
    text_queries: Dict[str, str],
    tokenizer: Any, # open_clip tokenizer
    experiment_name: str,
    device: str = "cuda:0",
    batch_size: int = 512,
    num_workers: int = 16,
):
    """
    ÂØπÁªôÂÆöÁöÑÊ®°ÂûãÂíåÊñáÊú¨Êü•ËØ¢ÔºåÊâßË°åÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÔºåÊõ¥Êñ∞AnnDataÂØπË±°ÔºåÂπ∂ÁîüÊàêÂèØËßÜÂåñÂõæË°®„ÄÇ

    Args:
        models_to_test (Dict): ÂåÖÂê´Ê®°ÂûãÂêçÁß∞„ÄÅË∑ØÂæÑÂíåÈÖçÁΩÆÁöÑÂ≠óÂÖ∏„ÄÇ
        hest_sample (HESTSample): Â∑≤Âä†ËΩΩÁöÑHESTÊ†∑Êú¨ÂØπË±°„ÄÇ
        text_queries (Dict): Áî®‰∫éÂàÜÁ±ªÁöÑÊñáÊú¨Êü•ËØ¢Â≠óÂÖ∏ (key=Á±ªÂà´Âêç, value=Âü∫Âõ†Â≠óÁ¨¶‰∏≤)„ÄÇ
        tokenizer (Any): OpenCLIP ÁöÑÂàÜËØçÂô®„ÄÇ
        experiment_name (str): ÂÆûÈ™åÁöÑÂîØ‰∏ÄÂêçÁß∞ÔºåÁî®‰∫éÂëΩÂêçadataÂàóÂíåËæìÂá∫Êñá‰ª∂„ÄÇ
        device (str): Áî®‰∫éÊé®ÁêÜÁöÑPyTorchËÆæÂ§á„ÄÇ
        batch_size (int): Êé®ÁêÜÊó∂ÁöÑÊâπÊ¨°Â§ßÂ∞è„ÄÇ
        num_workers (int): DataLoaderÁöÑÂ∑•‰ΩúËøõÁ®ãÊï∞„ÄÇ
    """
    logging.info(f"--- üöÄ ÂºÄÂßãÂÆûÈ™å: {experiment_name} ---")
    
    query_labels = list(text_queries.keys())
    
    # --- ËøêË°åÊâÄÊúâÊ®°ÂûãÁöÑÊé®ÁêÜ ---
    for model_name, config in models_to_test.items():
        logging.info(f"Ê≠£Âú®Â§ÑÁêÜÊ®°Âûã: {model_name}...")
        
        # 1. Âä†ËΩΩÊ®°ÂûãÂíåÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂô®
        model = load_clip_model(config['path'], config['model_name'], device)
        _, _, image_preprocessor = open_clip.create_model_and_transforms(config['model_name'])

        # 2. ÂàõÂª∫ DataLoader
        spot_dataset = WSISpotDataset(
            wsi=hest_sample.wsi, 
            coords=hest_sample.adata.obsm['spatial'], 
            transform=image_preprocessor
        )
        dataloader = DataLoader(
            spot_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=num_workers, 
            pin_memory=True
        )

        # 3. ÁºñÁ†ÅÊñáÊú¨Êü•ËØ¢ (ÊØè‰∏™Ê®°ÂûãÂè™ÈúÄ‰∏ÄÊ¨°)
        with torch.inference_mode():
            tokenized_queries = tokenizer(list(text_queries.values())).to(device)
            text_features = model.encode_text(tokenized_queries, normalize=True)

            # 4. ÊâπÈáèÁºñÁ†ÅÊâÄÊúâÂõæÂÉèÂõæÂùó
            all_spot_embeddings = []
            for image_batch in tqdm(dataloader, desc=f"Êé®ÁêÜ: {model_name}", leave=False):
                image_batch = image_batch.to(device, non_blocking=True)
                image_features = model.encode_image(image_batch, normalize=True)
                all_spot_embeddings.append(image_features.cpu())
        
        # 5. ËÆ°ÁÆóÁõ∏‰ººÂ∫¶Âπ∂Ëé∑ÂèñÈ¢ÑÊµãÁªìÊûú
        all_spot_embeddings_tensor = torch.cat(all_spot_embeddings)
        similarity = all_spot_embeddings_tensor.to(device) @ text_features.T
        confidence, predictions = torch.max(torch.softmax(similarity, dim=1), dim=1)

        # 6. Â∞ÜÁªìÊûúÊ∑ªÂä†Âà∞ AnnData ÂØπË±°
        anno_col = f"anno_{experiment_name}_{model_name.replace(' ', '_')}"
        conf_col = f"conf_{experiment_name}_{model_name.replace(' ', '_')}"
        
        hest_sample.adata.obs[anno_col] = pd.Categorical(
            [query_labels[i] for i in predictions.cpu().numpy()], 
            categories=query_labels
        )
        hest_sample.adata.obs[conf_col] = confidence.cpu().numpy()
        logging.info(f"ÁªìÊûúÂ∑≤Ê∑ªÂä†Âà∞ AnnData: '{anno_col}'")

        # 7. Ê∏ÖÁêÜ GPU ÂÜÖÂ≠ò
        del model, text_features, all_spot_embeddings_tensor
        torch.cuda.empty_cache()

    # --- ÁîüÊàêÂèØËßÜÂåñÂõæË°® ---
    logging.info("Ê≠£Âú®ÁîüÊàêÂèØËßÜÂåñÂõæË°®...")
    
    # Âä®ÊÄÅËÆæÁΩÆË∞ÉËâ≤Êùø
    categories = list(text_queries.keys())
    if len(categories) <= 10:
        colors = plt.cm.tab10(np.linspace(0, 1, len(categories)))
    elif len(categories) <= 20:
        colors = plt.cm.tab20(np.linspace(0, 1, len(categories)))
    else: # ÂØπ‰∫éÊõ¥Â§öÁ±ªÂà´Ôºå‰ΩøÁî®ËøûÁª≠Ëâ≤Ë∞±
        colors = plt.cm.viridis(np.linspace(0, 1, len(categories)))
    palette = dict(zip(categories, colors))

    # ÂàõÂª∫Â≠êÂõæ
    fig, axes = plt.subplots(
        1, len(models_to_test), 
        figsize=(7 * len(models_to_test), 12), 
        squeeze=False
    )
    fig.suptitle(f"{experiment_name} ‚Äì Sample {hest_sample.sample_id}", fontsize=20, y=0.9)

    plot_params = {
        "show": False, "legend_loc": None, "frameon": False,
        "library_id": hest_sample.sample_id, "img_key": 'downscaled_fullres',
        "size": 1.2, "s": 3,
    }

    for i, (model_name, _) in enumerate(models_to_test.items()):
        ax = axes[0, i]
        anno_col = f"anno_{experiment_name}_{model_name.replace(' ', '_')}"
        sc.pl.spatial(
            hest_sample.adata,
            color=anno_col,
            title=f"{model_name}\n({experiment_name})",
            ax=ax,
            palette=palette,
            **plot_params
        )

    # ÂàõÂª∫Áªü‰∏ÄÁöÑÂõæ‰æã
    handles = [mpatches.Patch(color=palette[cat], label=cat) for cat in categories]
    fig.legend(handles=handles, title="Categories", loc='center right', bbox_to_anchor=(1.05, 0.5))
    
    plt.tight_layout(rect=[0, 0, 0.90, 0.96])
    
    if not DRYRUN:
        save_path = OUTPUT_DIR / f"vis_{experiment_name}_comparison_{hest_sample.sample_id}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"ÂèØËßÜÂåñÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞: {save_path}")
    plt.show()

    logging.info(f"--- ‚úÖ ÂÆûÈ™å '{experiment_name}' ÂÆåÊàê ---")


# %% [markdown]
# ### 7. ËøêË°å‰∫åÂàÜÁ±ª‰ªªÂä° (Tumor vs. Normal)
#
# Áé∞Âú®ÔºåÊàë‰ª¨ÂÆö‰πâ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫é‚ÄúËÇøÁò§‚Äù‰∏é‚ÄúÊ≠£Â∏∏ÁªÑÁªá‚Äù‰∫åÂàÜÁ±ªÁöÑÊü•ËØ¢Â≠óÂÖ∏ÔºåÂπ∂Ë∞ÉÁî®‰∏äÈù¢ÂàõÂª∫ÁöÑÂáΩÊï∞„ÄÇ

# %%
# --- 1. ÂÆö‰πâ‰∫åÂàÜÁ±ªÊü•ËØ¢Â≠óÂÖ∏ ---
# non-tumor Êü•ËØ¢ÁªìÂêà‰∫ÜÂ§öÁßçÂü∫Ë¥®ÂíåÁªìÊûÑÁªÜËÉûÁ±ªÂûãÁöÑÊ†áÂøóÊÄß‰ΩéË°®ËææÂü∫Âõ†
binary_tumor_queries = {
    "Tumor": "EPCAM MKI67 TOP2A PCNA KRT8 KRT18 KRT19 TACSTD2 CENPF UBE2C BIRC5 MCM2 MCM3 MCM4 MCM5 MCM6 MCM7 CDK1 CCNB1 CCNB2 AURKA AURKB CEACAM5 CEACAM6 MUC1 CLDN4 CLDN7 CD44",
    "Non-Tumor Stroma": "TP53 COL1A1 COL1A2 DCN LUM POSTN FN1 SPARC ACTA2 MYH11 TAGLN CNN1 DES MYLK PECAM1 VWF CDH5 KDR CLDN5 TIE1 TEK"
}

# --- 2. ËøêË°åÂÆûÈ™å ---
# Á°Æ‰øù sample Âíå tokenizer Â∑≤ÁªèÂä†ËΩΩ
if 'sample' not in locals():
    sample = get_hest_sample()
if 'tokenizer' not in locals():
    tokenizer = open_clip.get_tokenizer(MODEL_NAME)
    
run_and_visualize_classification(
    models_to_test=MODELS_TO_TEST,
    hest_sample=sample,
    text_queries=binary_tumor_queries,
    tokenizer=tokenizer,
    experiment_name="Binary_Tumor_vs_Normal",
    device=DEVICE,
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS
)


# %% [markdown]
# ### 8. (ÂèØÈÄâ) ‰ΩøÁî®ÂéüÂßãÂ§öÂàÜÁ±ªÊü•ËØ¢ÂÜçÊ¨°ËøêË°å
#
# ‰∏∫‰∫ÜÂ±ïÁ§∫ÂáΩÊï∞ÁöÑÁÅµÊ¥ªÊÄßÔºåÊÇ®ÂèØ‰ª•ËΩªÊùæÂú∞‰ΩøÁî®ÂéüÂßãÁöÑÂ§öÂàÜÁ±ªÊü•ËØ¢Â≠óÂÖ∏ÂÜçÊ¨°ËøêË°åÂÆÉÔºåÁîüÊàêÂè¶‰∏ÄÁªÑÁã¨Á´ãÁöÑÂàÜÊûêÁªìÊûú„ÄÇ

# %%
# --- 1. ‰ΩøÁî®ÂéüÂßãÁöÑÂ§öÂàÜÁ±ªÊü•ËØ¢Â≠óÂÖ∏ ---
# (Ëøô‰∏™Â≠óÂÖ∏Â∑≤Âú®‰πãÂâçÁöÑÂçïÂÖÉÊ†º‰∏≠ÂÆö‰πâ)

# --- 2. ËøêË°åÂ§öÂàÜÁ±ªÂÆûÈ™å ---
run_and_visualize_classification(
    models_to_test=MODELS_TO_TEST,
    hest_sample=sample,
    text_queries=text_queries, # ‰ΩøÁî®ÂéüÂßãÁöÑÂ§öÂàÜÁ±ªÂ≠óÂÖ∏
    tokenizer=tokenizer,
    experiment_name="Multi_Class_Annotation",
    device=DEVICE,
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS
)

# ===== Cell 11: Markdown =====
# New Sample

# ===== Cell 12: Code (execution_count: 37) =====
# %% [markdown]
# # ÂèØÂ§çÁî®ÁöÑÁ©∫Èó¥CLIPÊ®°ÂûãËØÑ‰º∞Â∑•‰ΩúÊµÅ
#
# **ÁõÆÊ†á:** Êèê‰æõ‰∏Ä‰∏™Ê®°ÂùóÂåñÁöÑÂáΩÊï∞ÔºåÂèØ‰ª•ÂØπ‰ªªÊÑèHESTÊ†∑Êú¨Âíå‰ªªÊÑèÁîüÁâ©Â≠¶Êü•ËØ¢ÈõÜÔºåÊâßË°åÂ§öÊ®°ÂûãÁöÑÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÊØîËæÉÔºåÂπ∂ÁîüÊàêÊ†áÂáÜÂåñÁöÑÂèØËßÜÂåñÊä•Âëä„ÄÇ

# %% [markdown]
# ### 1. ÂØºÂÖ•‰∏éÁéØÂ¢ÉÈÖçÁΩÆ (Imports & Configuration)

# %%
# --- Ê†∏ÂøÉÂéüÂàô ---
# üõ°Ô∏è ÂÆâÂÖ®Á¨¨‰∏Ä: ÈªòËÆ§Âú®È¢ÑÊºîÊ®°Âºè‰∏ãËøêË°å‰ª•Ê£ÄÊü•Ë∑ØÂæÑÂíåÈÖçÁΩÆ
DRYRUN = False

# --- Ê†áÂáÜÂ∫ì ---
import os
import sys
import logging
from pathlib import Path
from typing import Dict, Any, Tuple, List

# --- Á¨¨‰∏âÊñπÂ∫ì ---
import torch
import numpy as np
import pandas as pd
import scanpy as sc
from PIL import Image, ImageFile
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
from torch.utils.data import DataLoader, Dataset

# --- È°πÁõÆË∑ØÂæÑÈÖçÁΩÆ ---
PROJECT_ROOT = Path("/home1/jijh/diffusion_project/git_repo/yuanspace")
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))
    print(f"Â∑≤Â∞Ü '{PROJECT_ROOT}' Ê∑ªÂä†Âà∞ sys.path")

# --- Êú¨Âú∞È°πÁõÆÂ∫ì ---
try:
    import open_clip
    from src.spaglam_preproc.utils.hest_loading import HESTDataset, HESTSample
except ImportError as e:
    print(f"ÂØºÂÖ•ÈîôËØØ: {e}\nËØ∑Á°Æ‰øùÊÇ®ÁöÑÈ°πÁõÆÁªìÊûÑÂíåË∑ØÂæÑÈÖçÁΩÆÊ≠£Á°Æ„ÄÇ")
    raise

# --- ÈÖçÁΩÆÊó•Âøó ---
ImageFile.LOAD_TRUNCATED_IMAGES = True
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout)


# %%
# --- Ë∑ØÂæÑ‰∏éÊ®°ÂûãÈÖçÁΩÆ ---
# --- Êï∞ÊçÆË∑ØÂæÑ ---
HEST_DATA_DIR = Path("/cwStorage/nodecw_group/jijh/hest_1k")

# --- ËæìÂá∫ÁõÆÂΩï (Êñ∞) ---
OUTPUT_DIR = Path("/cwStorage/nodecw_group/jijh/trained_models/all_comparisons/multipositive_vs_basline")

# --- Ê®°ÂûãÊ£ÄÊü•ÁÇπË∑ØÂæÑ ---
BASELINE_MODEL_CKPT = Path("/cwStorage/nodecw_group/jijh/trained_models/omiclip_base_model/omiclip_epoch_50.pt")
SPATIAL_MODEL_CKPT = Path("/cwStorage/nodecw_group/jijh/trained_models/spatial_clip_base_model/multi_positice_loss50.pt")

# --- Ê®°ÂûãÊû∂ÊûÑ ---
MODEL_NAME = "ViT-B-32"

# --- Êé®ÁêÜÈÖçÁΩÆ ---
BATCH_SIZE = 512
NUM_WORKERS = 16
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

# --- ÂæÖÊµãËØïÊ®°ÂûãÊ∏ÖÂçï ---
MODELS_TO_TEST = {
    "OmiCLIP (Baseline)": {
        "path": BASELINE_MODEL_CKPT,
        "model_name": MODEL_NAME,
    },
    "Spatial CLIP (Ours)": {
        "path": SPATIAL_MODEL_CKPT,
        "model_name": MODEL_NAME,
    },
}

# --- Ê£ÄÊü•‰∏éÂáÜÂ§á ---
if not DRYRUN:
    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
    logging.info(f"ËæìÂá∫Â∞Ü‰øùÂ≠òÂà∞: {OUTPUT_DIR}")

for name, config in MODELS_TO_TEST.items():
    assert config['path'].exists(), f"Ê®°Âûã '{name}' ÁöÑÊ£ÄÊü•ÁÇπÊñá‰ª∂‰∏çÂ≠òÂú®: {config['path']}"
assert HEST_DATA_DIR.exists(), f"HEST Êï∞ÊçÆÁõÆÂΩï‰∏çÂ≠òÂú®: {HEST_DATA_DIR}"

logging.info(f"ÈÖçÁΩÆÂÆåÊàê„ÄÇ‰ΩøÁî®ËÆæÂ§á: {DEVICE}")
logging.info(f"È¢ÑÊºîÊ®°Âºè (DRYRUN): {DRYRUN}")


# %% [markdown]
# ### 2. Ê†∏ÂøÉÂáΩÊï∞ÔºöÊ®°ÂûãÂä†ËΩΩ‰∏éÊï∞ÊçÆÂáÜÂ§á (Core Functions)

# %%
def load_clip_model(checkpoint_path: Path, model_name: str, device: str) -> torch.nn.Module:
    """‰ªéÁªôÂÆöÁöÑÊ£ÄÊü•ÁÇπË∑ØÂæÑÂä†ËΩΩ CLIP Ê®°Âûã„ÄÇ"""
    logging.info(f"Ê≠£Âú®Âä†ËΩΩÊ®°Âûã '{model_name}' ‰ªé: {checkpoint_path}")
    model, _, _ = open_clip.create_model_and_transforms(model_name, pretrained=None)
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint.get('state_dict', checkpoint)
    if all(key.startswith('module.') for key in state_dict.keys()):
        state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}
    try:
        model.load_state_dict(state_dict)
    except RuntimeError as e:
        logging.warning(f"Âä†ËΩΩ state_dict ‰∏•Ê†ºÊ®°ÂºèÂ§±Ë¥•: {e}„ÄÇÂ∞ùËØïÈùû‰∏•Ê†ºÊ®°Âºè...")
        model.load_state_dict(state_dict, strict=False)
    model.to(device)
    model.eval()
    logging.info(f"Ê®°Âûã '{model_name}' Âä†ËΩΩÊàêÂäü„ÄÇ")
    return model

def get_hest_sample(sample_id: str) -> HESTSample:
    """Âä†ËΩΩÂπ∂ÂáÜÂ§á‰∏Ä‰∏™ HEST Ê†∑Êú¨Áî®‰∫éÂàÜÊûê„ÄÇ"""
    logging.info(f"Âä†ËΩΩ HEST Ê†∑Êú¨: {sample_id}...")
    hest_dataset = HESTDataset(data_dir=HEST_DATA_DIR)
    sample = hest_dataset.get_samples(sample_ids=[sample_id])[0]
    sample.load_st_data(lazy=False)
    sample.load_wsi()
    
    spatial_data = sample.adata.uns['spatial']['ST']
    library_id = sample.sample_id
    sample.adata.uns['spatial'] = {library_id: spatial_data}
    logging.info(f"Ê†∑Êú¨ '{sample.sample_id}' Âä†ËΩΩÂπ∂ÂáÜÂ§áÂÆåÊØï„ÄÇ")
    return sample

class WSISpotDataset(Dataset):
    """‰∏Ä‰∏™Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜÔºåÁî®‰∫é‰ªéWSI‰∏≠ÊåâÈúÄÂä†ËΩΩspotÂØπÂ∫îÁöÑÂõæÂùó„ÄÇ"""
    def __init__(self, wsi, coords, transform):
        self.wsi = wsi
        self.coords = coords
        self.transform = transform
        self.patch_size = (224, 224)
        self.patch_radius = self.patch_size[0] // 2

    def __len__(self):
        return len(self.coords)

    def __getitem__(self, idx):
        c = self.coords[idx]
        top_left_coord = (int(c[0] - self.patch_radius), int(c[1] - self.patch_radius))
        tile = self.wsi.read_region(top_left_coord, 0, self.patch_size).convert("RGB")
        if self.transform:
            tile = self.transform(tile)
        return tile

# %% [markdown]
# ### 3. ‰∏ªÂ∑•‰ΩúÊµÅÂáΩÊï∞ (Main Workflow Function)
#
# ËøôÊòØÊàë‰ª¨Â∞ÅË£ÖÂ•ΩÁöÑÊ†∏ÂøÉËØÑ‰º∞ÂáΩÊï∞„ÄÇ

# %%
def run_and_visualize_experiment(
    sample_id: str,
    text_queries: Dict[str, str],
    experiment_name: str,
    models_to_test: Dict[str, Dict[str, Any]] = MODELS_TO_TEST,
    output_dir: Path = OUTPUT_DIR,
    device: str = DEVICE,
    batch_size: int = BATCH_SIZE,
    num_workers: int = NUM_WORKERS,
) -> HESTSample:
    """
    ÂØπÊåáÂÆöÁöÑÊ†∑Êú¨ÂíåÊñáÊú¨Êü•ËØ¢ÔºåÊâßË°åÂ§öÊ®°ÂûãÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÊØîËæÉÔºåÂπ∂ÁîüÊàêÂèØËßÜÂåñÊä•Âëä„ÄÇ

    Args:
        sample_id (str): Ë¶ÅÂàÜÊûêÁöÑ HEST Ê†∑Êú¨ID (‰æãÂ¶Ç 'TENX99')„ÄÇ
        text_queries (Dict): Áî®‰∫éÂàÜÁ±ªÁöÑÊñáÊú¨Êü•ËØ¢Â≠óÂÖ∏ (key=Á±ªÂà´Âêç, value=Âü∫Âõ†Â≠óÁ¨¶‰∏≤)„ÄÇ
        experiment_name (str): ÂÆûÈ™åÁöÑÂîØ‰∏ÄÂêçÁß∞ÔºåÁî®‰∫éÂëΩÂêçadataÂàóÂíåËæìÂá∫Êñá‰ª∂„ÄÇ
        models_to_test (Dict): ÂåÖÂê´ÂæÖÊµãÊ®°ÂûãÈÖçÁΩÆÁöÑÂ≠óÂÖ∏„ÄÇ
        output_dir (Path): ‰øùÂ≠òÂèØËßÜÂåñÁªìÊûúÁöÑÁõÆÂΩï„ÄÇ
        device (str): PyTorchËÆæÂ§á„ÄÇ
        batch_size (int): Êé®ÁêÜÊâπÊ¨°Â§ßÂ∞è„ÄÇ
        num_workers (int): DataLoaderÂ∑•‰ΩúËøõÁ®ãÊï∞„ÄÇ
        
    Returns:
        HESTSample: ÂåÖÂê´Ê†áÊ≥®ÁªìÊûúÁöÑ HEST Ê†∑Êú¨ÂØπË±°„ÄÇ
    """
    logging.info(f"--- üöÄ ÂºÄÂßãÂÆûÈ™å: '{experiment_name}' on sample '{sample_id}' ---")
    
    # 1. Âä†ËΩΩÊï∞ÊçÆ
    sample = get_hest_sample(sample_id)
    tokenizer = open_clip.get_tokenizer(MODEL_NAME)
    query_labels = list(text_queries.keys())
    
    # --- 2. ËøêË°åÊâÄÊúâÊ®°ÂûãÁöÑÊé®ÁêÜ ---
    for model_name, config in models_to_test.items():
        logging.info(f"Ê≠£Âú®Â§ÑÁêÜÊ®°Âûã: {model_name}...")
        
        model = load_clip_model(config['path'], config['model_name'], device)
        _, _, image_preprocessor = open_clip.create_model_and_transforms(config['model_name'])

        dataloader = DataLoader(
            WSISpotDataset(wsi=sample.wsi, coords=sample.adata.obsm['spatial'], transform=image_preprocessor),
            batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True
        )

        with torch.inference_mode():
            tokenized_queries = tokenizer(list(text_queries.values())).to(device)
            text_features = model.encode_text(tokenized_queries, normalize=True)

            all_spot_embeddings = []
            for image_batch in tqdm(dataloader, desc=f"Êé®ÁêÜ: {model_name}", leave=False):
                image_features = model.encode_image(image_batch.to(device), normalize=True)
                all_spot_embeddings.append(image_features.cpu())
        
        all_spot_embeddings_tensor = torch.cat(all_spot_embeddings)
        similarity = all_spot_embeddings_tensor.to(device) @ text_features.T
        confidence, predictions = torch.max(torch.softmax(similarity, dim=1), dim=1)

        # --- 3. Â∞ÜÁªìÊûúÊ∑ªÂä†Âà∞ AnnData ÂØπË±° ---
        model_name_safe = model_name.replace(' ', '_').replace('(', '').replace(')', '')
        anno_col = f"anno_{experiment_name}_{model_name_safe}"
        conf_col = f"conf_{experiment_name}_{model_name_safe}"
        
        sample.adata.obs[anno_col] = pd.Categorical(
            [query_labels[i] for i in predictions.cpu().numpy()], 
            categories=query_labels
        )
        sample.adata.obs[conf_col] = confidence.cpu().numpy()
        logging.info(f"ÁªìÊûúÂ∑≤Ê∑ªÂä†Âà∞ AnnData: '{anno_col}'")

        del model, text_features, all_spot_embeddings_tensor
        torch.cuda.empty_cache()

    # --- 4. ÁîüÊàêÂèØËßÜÂåñÂõæË°® ---
    logging.info("Ê≠£Âú®ÁîüÊàêÂèØËßÜÂåñÂõæË°®...")
    
    categories = list(text_queries.keys())
    colors = plt.cm.tab10(np.linspace(0, 1, len(categories))) if len(categories) <= 10 else plt.cm.viridis(np.linspace(0, 1, len(categories)))
    palette = dict(zip(categories, sns.color_palette("Set1", len(categories))))

    fig, axes = plt.subplots(1, len(models_to_test), figsize=(7 * len(models_to_test), 12), squeeze=False)
    fig.suptitle(f"'{experiment_name}' on Sample '{sample_id}'", fontsize=20, y=0.9)

    plot_params = {
        "show": False, "legend_loc": None, "frameon": False,
        "library_id": sample.sample_id, "img_key": 'downscaled_fullres', "size": 1.2, "s": 1.5,
    }

    for i, (model_name, _) in enumerate(models_to_test.items()):
        ax = axes[0, i]
        model_name_safe = model_name.replace(' ', '_').replace('(', '').replace(')', '')
        anno_col = f"anno_{experiment_name}_{model_name_safe}"
        sc.pl.spatial(sample.adata, color=anno_col, title=f"{model_name}", ax=ax, palette=palette, **plot_params)

    handles = [mpatches.Patch(color=palette[cat], label=cat) for cat in categories]
    fig.legend(handles=handles, title="Categories", loc='center right', bbox_to_anchor=(1.05, 0.5))
    
    plt.tight_layout(rect=[0, 0, 0.90, 0.96])
    
    if not DRYRUN:
        save_path = output_dir / f"vis_{experiment_name}_comparison_{sample_id}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"ÂèØËßÜÂåñÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞: {save_path}")
    plt.show()

    logging.info(f"--- ‚úÖ ÂÆûÈ™å '{experiment_name}' ÂÆåÊàê ---")
    return sample


# %% [markdown]
# ### 4. ÊâßË°å‰∫åÂàÜÁ±ª‰π≥ËÖ∫ÁôåÂÆûÈ™å (Run Binary Breast Cancer Experiment)
#
# Áé∞Âú®ÔºåÊàë‰ª¨ÂÆö‰πâÈíàÂØπ‰π≥ËÖ∫ÁôåÁöÑ‰∫åÂàÜÁ±ªÊü•ËØ¢ÔºåÂπ∂‰ΩøÁî®Êñ∞Ê†∑Êú¨ `TENX99` Ë∞ÉÁî®‰∏ªÂáΩÊï∞„ÄÇ

# %%
# --- 1. ÂÆö‰πâ‰π≥ËÖ∫Áôå‰∫åÂàÜÁ±ªÊü•ËØ¢ ---
# Âü∫‰∫éÊÇ®ÁöÑÊèèËø∞Ôºå"non-tumor" Êü•ËØ¢Â∞ÜÂåÖÂê´ËÇøÁò§‰ΩéË°®ËææÁöÑÂü∫Âõ†„ÄÇ
# ËøôÈáåÁöÑÂü∫Âõ†ÂàóË°®ÊòØÁ§∫‰æãÔºåÊÇ®ÂèØ‰ª•ÊõøÊç¢‰∏∫Êõ¥Á≤æÁ°ÆÁöÑÂàóË°®„ÄÇ
breast_cancer_binary_queries = {
    "Tumor (IDC)": (
        "KRT8 KRT18 KRT19 EPCAM MUC1 CLDN3 CLDN4 CLDN7 CDH1 KRT7 "
        "ERBB2 GRB7 ERBB3 ESR1 PGR FOXA1 GATA3 AGR2 TFF1 TFF3 "
        "TRPS1 SCGB2A2 BCL2 AR MKI67 TOP2A MYC CCND1 EGFR BIRC5 "
        "UBE2C CCNA2 CCNB1 CDC20 PLK1 AURKA AURKB CENPF NUSAP1 KIF11 "
        "KIF2C PTTG1 PRC1 ANLN MCM2 MCM4 MCM6 MCM7 TK1 PCNA"
    ),
    "Normal & Stroma": (
        "COL1A1 COL1A2 COL3A1 COL5A1 COL5A2 COL6A1 COL6A2 COL6A3 FN1 VIM "
        "LUM DCN BGN THY1 FAP PDGFRB RGS5 CSPG4 ACTA2 TAGLN "
        "MYH11 CNN1 DES VWF PECAM1 CLDN5 KDR FLT1 ENG CD34 "
        "MCAM PDPN PROX1 LYVE1 PTPRC LST1 LYZ CSF1R CD68 C1QB "
        "ITGAM ITGAX HLA-DRA HLA-DPB1 CD3D CD4 CD8A TRAC MS4A1 CD79A"
    ),
}


# --- 2. ËøêË°åÂÆûÈ™å ---
if not DRYRUN:
    # Âè™ÊúâÂú®ÈùûÈ¢ÑÊºîÊ®°Âºè‰∏ãÊâçËøêË°åÔºåÂõ†‰∏∫ÂÆÉÊ∂âÂèäÂ§ßÈáèËÆ°ÁÆó
    sample_with_results = run_and_visualize_experiment(
        sample_id="TENX99",
        text_queries=breast_cancer_binary_queries,
        experiment_name="Breast_Cancer_Binary"
    )
else:
    logging.warning("Â§Ñ‰∫éÈ¢ÑÊºîÊ®°ÂºèÔºåË∑≥ËøáÂÆûÈ™åÊâßË°å„ÄÇ")

# ===== Cell 13: Code =====
sc.pl.spatial(
    sample_with_results.adata,
    color=f"anno_Breast_Cancer_Binary_OmiCLIP_Baseline",
    title=f"OmiCLIP (Baseline) - Breast Cancer Binary Annotation",
    s = 0,
    show = True,
    legend_loc = None,
    frameon = False,
    library_id = sample_with_results.sample_id, 
    img_key = 'downscaled_fullres',
)

# ===== Cell 14: Code (execution_count: 32) =====
# %% [markdown]
# ### 3. ‰∏ªÂ∑•‰ΩúÊµÅÂáΩÊï∞ (Main Workflow Function) - V2 with Quantitative Metrics
#
# Ëøô‰∏™Êõ¥Êñ∞ÁâàÊú¨Â¢ûÂä†‰∫ÜËá™Âä®ÁîüÊàê‰º™Ê†áÁ≠æÂíåËÆ°ÁÆóÂÆöÈáèÊåáÊ†áÔºàÂáÜÁ°ÆÁéá„ÄÅF1ÂàÜÊï∞„ÄÅÊ∑∑Ê∑ÜÁü©ÈòµÔºâÁöÑÂäüËÉΩ„ÄÇ

# %%
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay

def create_pseudo_labels(
    adata: sc.AnnData, 
    marker_map: Dict[str, str], 
    top_n_spots: int = 50
) -> pd.Series:
    """
    Âü∫‰∫éÂçï‰∏™Ê†áÂøóÊÄßÂü∫Âõ†ÁöÑÈ´òË°®ËææÂå∫ÂüüÂàõÂª∫‰º™Ê†áÁ≠æ„ÄÇ

    Args:
        adata (sc.AnnData): ÂåÖÂê´Âü∫Âõ†Ë°®ËææÊï∞ÊçÆÁöÑ AnnData ÂØπË±°„ÄÇ
        marker_map (Dict[str, str]): Â∞ÜÁ±ªÂà´ÂêçÊò†Â∞ÑÂà∞ÂÖ∂‰ª£Ë°®ÊÄßÊ†áÂøóÂü∫Âõ†ÁöÑÂ≠óÂÖ∏„ÄÇ
        top_n_spots (int): ‰∏∫ÊØè‰∏™Ê†áÂøóÂü∫Âõ†ÈÄâÊã©ÁöÑÊúÄÈ´òË°®Ëææ spot ÁöÑÊï∞Èáè„ÄÇ

    Returns:
        pd.Series: Á¥¢Âºï‰∏∫ spot IDÔºåÂÄº‰∏∫‰º™Ê†áÁ≠æÁöÑ Series„ÄÇ
    """
    pseudo_labels = pd.Series(index=adata.obs.index, dtype=object)
    available_genes = adata.var_names
    
    logging.info(f"Ê≠£Âú®ÂàõÂª∫‰º™Ê†áÁ≠æ (top {top_n_spots} spots per marker)...")
    
    # ‰∏∫‰∫ÜÈÅøÂÖçÈáçÂè†ÔºåÊàë‰ª¨ËÆ∞ÂΩïÂ∑≤Ë¢´ÂàÜÈÖçÁöÑ spots
    assigned_indices = set()

    for label, marker in marker_map.items():
        if marker not in available_genes:
            logging.warning(f"Ê†áÂøóÊÄßÂü∫Âõ† '{marker}' Âú®Êï∞ÊçÆ‰∏≠Êú™ÊâæÂà∞ÔºåË∑≥Ëøá '{label}' Á±ªÂà´„ÄÇ")
            continue
            
        gene_expression = adata[:, marker].X.toarray().flatten()
        
        # ÊéíÂ∫èÊâÄÊúâ spotsÔºåÊâæÂà∞Ë°®ËææÈáèÊúÄÈ´òÁöÑ
        sorted_indices = np.argsort(gene_expression)[::-1]
        
        # ‰ªéÊú™Ë¢´ÂàÜÈÖçÁöÑ spots ‰∏≠ÈÄâÊã© top_n
        top_unassigned_indices = [
            idx for idx in sorted_indices if idx not in assigned_indices
        ][:top_n_spots]
        
        if top_unassigned_indices:
            pseudo_labels.iloc[top_unassigned_indices] = label
            assigned_indices.update(top_unassigned_indices)
            logging.info(f"‰∏∫ '{label}' ÂàÜÈÖç‰∫Ü {len(top_unassigned_indices)} ‰∏™‰º™Ê†áÁ≠æ (Âü∫‰∫éÂü∫Âõ† {marker})„ÄÇ")

    labeled_spots = pseudo_labels.dropna()
    logging.info(f"‰º™Ê†áÁ≠æÂàõÂª∫ÂÆåÊàêÔºåÂÖ±Ê†áËÆ∞‰∫Ü {len(labeled_spots)} ‰∏™ spotsÔºåÊ∂µÁõñ {labeled_spots.nunique()} ‰∏™Á±ªÂà´„ÄÇ")
    return labeled_spots


def run_and_visualize_experiment(
    sample_id: str,
    text_queries: Dict[str, str],
    experiment_name: str,
    marker_map: Dict[str, str], # Êñ∞Â¢ûÔºöÊ†áÂøóÊÄßÂü∫Âõ†Êò†Â∞Ñ
    models_to_test: Dict[str, Dict[str, Any]] = MODELS_TO_TEST,
    output_dir: Path = OUTPUT_DIR,
    device: str = DEVICE,
    batch_size: int = BATCH_SIZE,
    num_workers: int = NUM_WORKERS,
) -> HESTSample:
    """
    ÂØπÊåáÂÆöÁöÑÊ†∑Êú¨ÂíåÊü•ËØ¢ÔºåÊâßË°åÂ§öÊ®°ÂûãÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÔºåËøõË°åÂÆöÊÄßÂèØËßÜÂåñÂíåÂÆöÈáèËØÑ‰º∞„ÄÇ

    Args:
        sample_id (str): Ë¶ÅÂàÜÊûêÁöÑ HEST Ê†∑Êú¨ID„ÄÇ
        text_queries (Dict): Áî®‰∫éÂàÜÁ±ªÁöÑÊñáÊú¨Êü•ËØ¢Â≠óÂÖ∏„ÄÇ
        experiment_name (str): ÂÆûÈ™åÁöÑÂîØ‰∏ÄÂêçÁß∞„ÄÇ
        marker_map (Dict[str, str]): Â∞ÜÁ±ªÂà´ÂêçÊò†Â∞ÑÂà∞Ê†áÂøóÂü∫Âõ†ÁöÑÂ≠óÂÖ∏ÔºåÁî®‰∫éÁîüÊàê‰º™Ê†áÁ≠æ„ÄÇ
        (ÂÖ∂‰ªñÂèÇÊï∞ÂêåÂâç)
        
    Returns:
        HESTSample: ÂåÖÂê´Ê†áÊ≥®ÂíåËØÑ‰º∞ÁªìÊûúÁöÑ HEST Ê†∑Êú¨ÂØπË±°„ÄÇ
    """
    logging.info(f"--- üöÄ ÂºÄÂßãÂÆûÈ™å: '{experiment_name}' on sample '{sample_id}' ---")
    
    # 1. Âä†ËΩΩÊï∞ÊçÆ
    sample = get_hest_sample(sample_id)
    tokenizer = open_clip.get_tokenizer(MODEL_NAME)
    query_labels = list(text_queries.keys())
    
    # --- 2. ËøêË°åÊâÄÊúâÊ®°ÂûãÁöÑÊé®ÁêÜ (Ê≠§ÈÉ®ÂàÜÈÄªËæë‰∏çÂèò) ---
    for model_name, config in models_to_test.items():
        logging.info(f"Ê≠£Âú®Â§ÑÁêÜÊ®°Âûã: {model_name}...")
        
        model = load_clip_model(config['path'], config['model_name'], device)
        _, _, image_preprocessor = open_clip.create_model_and_transforms(config['model_name'])

        dataloader = DataLoader(
            WSISpotDataset(wsi=sample.wsi, coords=sample.adata.obsm['spatial'], transform=image_preprocessor),
            batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True
        )

        with torch.inference_mode():
            tokenized_queries = tokenizer(list(text_queries.values())).to(device)
            text_features = model.encode_text(tokenized_queries, normalize=True)

            all_spot_embeddings = []
            for image_batch in tqdm(dataloader, desc=f"Êé®ÁêÜ: {model_name}", leave=False):
                image_features = model.encode_image(image_batch.to(device), normalize=True)
                all_spot_embeddings.append(image_features.cpu())
        
        all_spot_embeddings_tensor = torch.cat(all_spot_embeddings)
        similarity = all_spot_embeddings_tensor.to(device) @ text_features.T
        confidence, predictions = torch.max(torch.softmax(similarity, dim=1), dim=1)

        model_name_safe = model_name.replace(' ', '_').replace('(', '').replace(')', '')
        anno_col = f"anno_{experiment_name}_{model_name_safe}"
        conf_col = f"conf_{experiment_name}_{model_name_safe}"
        
        sample.adata.obs[anno_col] = pd.Categorical(
            [query_labels[i] for i in predictions.cpu().numpy()], 
            categories=query_labels
        )
        sample.adata.obs[conf_col] = confidence.cpu().numpy()
        logging.info(f"ÁªìÊûúÂ∑≤Ê∑ªÂä†Âà∞ AnnData: '{anno_col}'")

        del model, text_features, all_spot_embeddings_tensor
        torch.cuda.empty_cache()

    # --- 3. ÂÆöÊÄßÂèØËßÜÂåñ (Ê≠§ÈÉ®ÂàÜÈÄªËæë‰∏çÂèò) ---
    logging.info("--- üìä Èò∂ÊÆµ‰∏Ä: ÂÆöÊÄßËØÑ‰º∞ (ÂèØËßÜÂåñ) ---")
    # ... (ÂèØËßÜÂåñ‰ª£Á†Å‰øùÊåÅ‰∏çÂèòÔºåÊ≠§Â§Ñ‰∏∫Á≤æÁÆÄÁâà) ...
    fig, axes = plt.subplots(1, len(models_to_test), figsize=(7 * len(models_to_test), 12), squeeze=False)
    # ... (ÂÆåÊï¥ÁªòÂõæ‰ª£Á†Å) ...
    plt.show()


    # --- 4. ÂÆöÈáèËØÑ‰º∞ (Êñ∞Â¢û) ---
    logging.info("--- üìà Èò∂ÊÆµ‰∫å: ÂÆöÈáèËØÑ‰º∞ (Âü∫‰∫é‰º™Ê†áÁ≠æ) ---")
    pseudo_ground_truth = create_pseudo_labels(sample.adata, marker_map)
    
    if pseudo_ground_truth.empty:
        logging.warning("Êú™ËÉΩÁîüÊàê‰ªª‰Ωï‰º™Ê†áÁ≠æÔºåË∑≥ËøáÂÆöÈáèËØÑ‰º∞„ÄÇËØ∑Ê£ÄÊü• marker_map ‰∏≠ÁöÑÂü∫Âõ†ÊòØÂê¶Â≠òÂú®‰∫éÊï∞ÊçÆ‰∏≠„ÄÇ")
        return sample

    classification_results = []
    
    # ÂáÜÂ§áÊ∑∑Ê∑ÜÁü©ÈòµÁöÑÂ≠êÂõæ
    fig_cm, axes_cm = plt.subplots(1, len(models_to_test), figsize=(8 * len(models_to_test), 7), squeeze=False)
    fig_cm.suptitle(f"Confusion Matrices for '{experiment_name}' on Sample '{sample_id}'", fontsize=20, y=1.02)
    axes_cm = axes_cm.flatten()

    for i, (model_name, _) in enumerate(models_to_test.items()):
        model_name_safe = model_name.replace(' ', '_').replace('(', '').replace(')', '')
        anno_col = f"anno_{experiment_name}_{model_name_safe}"
        
        predictions = sample.adata.obs[anno_col].loc[pseudo_ground_truth.index]
        
        # Á°Æ‰øùÊ†áÁ≠æÈõÜ‰∏ÄËá¥
        labels = sorted(list(set(pseudo_ground_truth) | set(predictions)))
        
        acc = accuracy_score(pseudo_ground_truth, predictions)
        f1 = f1_score(pseudo_ground_truth, predictions, average='weighted', labels=labels, zero_division=0)
        
        classification_results.append({"model": model_name, "accuracy": acc, "f1_weighted": f1})
        
        # ËÆ°ÁÆóÂπ∂ÁªòÂà∂Ê∑∑Ê∑ÜÁü©Èòµ
        cm = confusion_matrix(pseudo_ground_truth, predictions, labels=labels)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
        disp.plot(ax=axes_cm[i], xticks_rotation='vertical', cmap='Blues')
        axes_cm[i].set_title(f"{model_name}")

    plt.tight_layout()
    if not DRYRUN:
        save_path = output_dir / f"conf_matrix_{experiment_name}_comparison_{sample_id}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"Ê∑∑Ê∑ÜÁü©ÈòµÂõæÂ∑≤‰øùÂ≠òÂà∞: {save_path}")
    plt.show()
    
    classification_df = pd.DataFrame(classification_results).set_index('model')
    print("\n--- Zero-Shot Classification Performance (on Pseudo-Labels) ---")
    print(classification_df)
    if not DRYRUN:
        df_save_path = output_dir / f"metrics_{experiment_name}_{sample_id}.csv"
        classification_df.to_csv(df_save_path)
        logging.info(f"ÂàÜÁ±ªÊåáÊ†áÂ∑≤‰øùÂ≠òÂà∞: {df_save_path}")

    logging.info(f"--- ‚úÖ ÂÆûÈ™å '{experiment_name}' ÂÆåÊàê ---")
    return sample


# %% [markdown]
# ### 4. ÊâßË°å‰∫åÂàÜÁ±ª‰π≥ËÖ∫ÁôåÂÆûÈ™å (Run Binary Breast Cancer Experiment)
#
# Áé∞Âú®ÔºåÊàë‰ª¨ÂÆö‰πâÈíàÂØπ‰π≥ËÖ∫ÁôåÁöÑ‰∫åÂàÜÁ±ªÊü•ËØ¢**‰ª•ÂèäÂØπÂ∫îÁöÑÊ†áÂøóÊÄßÂü∫Âõ†Êò†Â∞Ñ**ÔºåÁÑ∂ÂêéË∞ÉÁî®‰∏ªÂáΩÊï∞„ÄÇ

# %%
# --- 1. ÂÆö‰πâÊü•ËØ¢Â≠óÂÖ∏ ---
breast_cancer_binary_queries = {
    "Tumor (IDC)": (
        "KRT8 KRT18 KRT19 EPCAM MUC1 CLDN3 CLDN4 CLDN7 CDH1 KRT7 "
        "ERBB2 GRB7 ERBB3 ESR1 PGR FOXA1 GATA3 AGR2 TFF1 TFF3 "
        "TRPS1 SCGB2A2 BCL2 AR MKI67 TOP2A MYC CCND1 EGFR BIRC5 "
        "UBE2C CCNA2 CCNB1 CDC20 PLK1 AURKA AURKB CENPF NUSAP1 KIF11 "
        "KIF2C PTTG1 PRC1 ANLN MCM2 MCM4 MCM6 MCM7 TK1 PCNA"
    ),
    "Normal & Stroma": (
        "COL1A1 COL1A2 COL3A1 COL5A1 COL5A2 COL6A1 COL6A2 COL6A3 FN1 VIM "
        "LUM DCN BGN THY1 FAP PDGFRB RGS5 CSPG4 ACTA2 TAGLN "
        "MYH11 CNN1 DES VWF PECAM1 CLDN5 KDR FLT1 ENG CD34 "
        "MCAM PDPN PROX1 LYVE1 PTPRC LST1 LYZ CSF1R CD68 C1QB "
        "ITGAM ITGAX HLA-DRA HLA-DPB1 CD3D CD4 CD8A TRAC MS4A1 CD79A"
    ),
}


# --- 2. ÂÆö‰πâÁî®‰∫éÁîüÊàê‰º™Ê†áÁ≠æÁöÑÊ†áÂøóÊÄßÂü∫Âõ†Êò†Â∞Ñ ---
#    - key ÂøÖÈ°ª‰∏é‰∏äÈù¢Êü•ËØ¢Â≠óÂÖ∏ÁöÑ key ÂÆåÂÖ®ÂåπÈÖç„ÄÇ
#    - value ÊòØ‰∏Ä‰∏™È´òÁΩÆ‰ø°Â∫¶ÁöÑÂçï‰∏ÄÊ†áÂøóÊÄßÂü∫Âõ†„ÄÇ
breast_cancer_marker_map = {
    "Tumor (IDC)": "EPCAM",      # EPCAM ÊòØÁªèÂÖ∏ÁöÑÁôåÁªÜËÉûÊ†áÂøóÁâ©
    "Normal & Stroma": "COL1A1"  # COL1A1 ÊòØÊàêÁ∫§Áª¥ÁªÜËÉû/Âü∫Ë¥®ÁöÑÂº∫Ê†áÂøóÁâ©
}

# --- 3. ËøêË°åÂÆûÈ™å ---
if not DRYRUN:
    # Âè™ÊúâÂú®ÈùûÈ¢ÑÊºîÊ®°Âºè‰∏ãÊâçËøêË°å
    sample_with_results = run_and_visualize_experiment(
        sample_id="TENX99",
        text_queries=breast_cancer_binary_queries,
        experiment_name="Breast_Cancer_Binary",
        marker_map=breast_cancer_marker_map  # <-- ‰º†ÂÖ•Êñ∞ÁöÑÂèÇÊï∞
    )
else:
    logging.warning("Â§Ñ‰∫éÈ¢ÑÊºîÊ®°ÂºèÔºåË∑≥ËøáÂÆûÈ™åÊâßË°å„ÄÇ")

# ===== Cell 15: Markdown =====
# ChatgptÊõ¥Êñ∞ÁâàÊú¨

# ===== Cell 16: Code (execution_count: 38) =====
# %% [markdown]
# # ÂèØÂ§çÁî®ÁöÑÁ©∫Èó¥CLIPÊ®°ÂûãËØÑ‰º∞Â∑•‰ΩúÊµÅÔºàÁÆÄÊ¥ÅÁâàÔºåÂê´ÊîπËøõ‰º™Ê†áÁ≠æÔºâ
# - ‰º™Ê†áÁ≠æÔºöscanpy.tl.score_genes Â§öÂü∫Âõ†ÊâìÂàÜ + ÂàÜ‰ΩçÈòàÂÄº + ÁΩÆ‰ø°ËæπÈôÖ + Unknown
# - ÈÅøÂÖç‰∏éËÆ≠ÁªÉÊó∂‚Äúspot Top-50 Âü∫Âõ†Âè•Â≠ê‚ÄùÈó≠ÁéØÔºå‰ΩøÁî®Âõ∫ÂÆöÁöÑÁªèÂÖ∏Ê†áËÆ∞ÈõÜÂêàÔºàPanglaoDB/CellMarker/HPAÔºâ

# %% [markdown]
# ## 1. ÂØºÂÖ•‰∏éÁéØÂ¢ÉÈÖçÁΩÆ

# %%
# --- Ê†∏ÂøÉÂéüÂàô ---
DRYRUN = False

# --- Ê†áÂáÜÂ∫ì ---
import os
import sys
import logging
from pathlib import Path
from typing import Dict, Any, Tuple, List

# --- Á¨¨‰∏âÊñπÂ∫ì ---
import torch
import numpy as np
import pandas as pd
import scanpy as sc
from PIL import Image, ImageFile
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay

# --- È°πÁõÆË∑ØÂæÑÈÖçÁΩÆ ---
PROJECT_ROOT = Path("/home1/jijh/diffusion_project/git_repo/yuanspace")
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))
    print(f"Â∑≤Â∞Ü '{PROJECT_ROOT}' Ê∑ªÂä†Âà∞ sys.path")

# --- Êú¨Âú∞È°πÁõÆÂ∫ì ---
try:
    import open_clip
    from src.spaglam_preproc.utils.hest_loading import HESTDataset, HESTSample
except ImportError as e:
    print(f"ÂØºÂÖ•ÈîôËØØ: {e}\nËØ∑Á°Æ‰øùÊÇ®ÁöÑÈ°πÁõÆÁªìÊûÑÂíåË∑ØÂæÑÈÖçÁΩÆÊ≠£Á°Æ„ÄÇ")
    raise

# --- ÈÖçÁΩÆÊó•Âøó ---
ImageFile.LOAD_TRUNCATED_IMAGES = True
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout)


# %% [markdown]
# ## 2. Ë∑ØÂæÑ‰∏éÊ®°ÂûãÈÖçÁΩÆ

# %%
# --- Êï∞ÊçÆË∑ØÂæÑ ---
HEST_DATA_DIR = Path("/cwStorage/nodecw_group/jijh/hest_1k")

# --- ËæìÂá∫ÁõÆÂΩï ---
OUTPUT_DIR = Path("/cwStorage/nodecw_group/jijh/trained_models/all_comparisons/multipositive_vs_basline")

# --- Ê®°ÂûãÊ£ÄÊü•ÁÇπË∑ØÂæÑ ---
BASELINE_MODEL_CKPT = Path("/cwStorage/nodecw_group/jijh/trained_models/omiclip_base_model/omiclip_epoch_50.pt")
SPATIAL_MODEL_CKPT = Path("/cwStorage/nodecw_group/jijh/trained_models/spatial_clip_base_model/multi_positice_loss50.pt")

# --- Ê®°ÂûãÊû∂ÊûÑ ---
MODEL_NAME = "ViT-B-32"

# --- Êé®ÁêÜÈÖçÁΩÆ ---
BATCH_SIZE = 512
NUM_WORKERS = 16
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

# --- ÂæÖÊµãËØïÊ®°ÂûãÊ∏ÖÂçï ---
MODELS_TO_TEST = {
    "OmiCLIP (Baseline)": {"path": BASELINE_MODEL_CKPT, "model_name": MODEL_NAME},
    "Spatial CLIP (Ours)": {"path": SPATIAL_MODEL_CKPT, "model_name": MODEL_NAME},
}

# --- Ê£ÄÊü•‰∏éÂáÜÂ§á ---
if not DRYRUN:
    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
    logging.info(f"ËæìÂá∫Â∞Ü‰øùÂ≠òÂà∞: {OUTPUT_DIR}")

for name, config in MODELS_TO_TEST.items():
    assert config['path'].exists(), f"Ê®°Âûã '{name}' ÁöÑÊ£ÄÊü•ÁÇπÊñá‰ª∂‰∏çÂ≠òÂú®: {config['path']}"
assert HEST_DATA_DIR.exists(), f"HEST Êï∞ÊçÆÁõÆÂΩï‰∏çÂ≠òÂú®: {HEST_DATA_DIR}"

logging.info(f"ÈÖçÁΩÆÂÆåÊàê„ÄÇ‰ΩøÁî®ËÆæÂ§á: {DEVICE}")
logging.info(f"È¢ÑÊºîÊ®°Âºè (DRYRUN): {DRYRUN}")


# %% [markdown]
# ## 3. Êï∞ÊçÆÂä†ËΩΩ‰∏éWSIÂ∞ÅË£Ö

# %%
def load_clip_model(checkpoint_path: Path, model_name: str, device: str) -> torch.nn.Module:
    """‰ªéÁªôÂÆöÁöÑÊ£ÄÊü•ÁÇπË∑ØÂæÑÂä†ËΩΩ CLIP Ê®°Âûã„ÄÇ"""
    logging.info(f"Ê≠£Âú®Âä†ËΩΩÊ®°Âûã '{model_name}' ‰ªé: {checkpoint_path}")
    model, _, _ = open_clip.create_model_and_transforms(model_name, pretrained=None)
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint.get('state_dict', checkpoint)
    if all(key.startswith('module.') for key in state_dict.keys()):
        state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}
    try:
        model.load_state_dict(state_dict)
    except RuntimeError as e:
        logging.warning(f"Âä†ËΩΩ state_dict ‰∏•Ê†ºÊ®°ÂºèÂ§±Ë¥•: {e}„ÄÇÂ∞ùËØïÈùû‰∏•Ê†ºÊ®°Âºè...")
        model.load_state_dict(state_dict, strict=False)
    model.to(device)
    model.eval()
    logging.info(f"Ê®°Âûã '{model_name}' Âä†ËΩΩÊàêÂäü„ÄÇ")
    return model

def get_hest_sample(sample_id: str) -> HESTSample:
    """Âä†ËΩΩÂπ∂ÂáÜÂ§á‰∏Ä‰∏™ HEST Ê†∑Êú¨Áî®‰∫éÂàÜÊûê„ÄÇ"""
    logging.info(f"Âä†ËΩΩ HEST Ê†∑Êú¨: {sample_id}...")
    hest_dataset = HESTDataset(data_dir=HEST_DATA_DIR)
    sample = hest_dataset.get_samples(sample_ids=[sample_id])[0]
    sample.load_st_data(lazy=False)
    sample.load_wsi()

    # Áªü‰∏Ä spatial ÈîÆ
    spatial_data = sample.adata.uns['spatial']['ST']
    library_id = sample.sample_id
    sample.adata.uns['spatial'] = {library_id: spatial_data}
    logging.info(f"Ê†∑Êú¨ '{sample.sample_id}' Âä†ËΩΩÂπ∂ÂáÜÂ§áÂÆåÊØï„ÄÇ")
    return sample

class WSISpotDataset(Dataset):
    """‰ªéWSI‰∏≠ÊåâÈúÄÂä†ËΩΩspotÂØπÂ∫îÂõæÂùó„ÄÇ"""
    def __init__(self, wsi, coords, transform):
        self.wsi = wsi
        self.coords = coords
        self.transform = transform
        self.patch_size = (224, 224)
        self.patch_radius = self.patch_size[0] // 2
    def __len__(self): return len(self.coords)
    def __getitem__(self, idx):
        c = self.coords[idx]
        top_left = (int(c[0] - self.patch_radius), int(c[1] - self.patch_radius))
        tile = self.wsi.read_region(top_left, 0, self.patch_size).convert("RGB")
        if self.transform: tile = self.transform(tile)
        return tile


# %% [markdown]
# ## 4. ÊîπËøõÁâà‰º™Ê†áÁ≠æÔºöScanpy `score_genes` + ÂàÜ‰ΩçÈòàÂÄº + ÁΩÆ‰ø°ËæπÈôÖ + Unknown

# %%
def ensure_log1p_layer(adata: sc.AnnData, layer_name: str = "log1p") -> None:
    """Á°Æ‰øùÂ≠òÂú® log1p Â±ÇÔºõËã•Êó†Âàô‰ª• Scanpy Êé®ËçêÊµÅÁ®ãËøõË°å normalize_total + log1p„ÄÇ"""
    if layer_name in adata.layers:
        return
    sc.pp.normalize_total(adata, target_sum=1e4)  # count-depth scaling
    sc.pp.log1p(adata)
    adata.layers[layer_name] = adata.X.copy()

def create_silver_labels_scanpy(
    adata: sc.AnnData,
    marker_panels: Dict[str, List[str]],
    layer: str = "log1p",
    ctrl_size: int = 50,     # score_genes ÂØπÁÖßÂü∫Âõ†Êï∞Ôºõ‰ºöËá™Âä®ÊåâË°®ËææÈáèÂàÜÁÆ±ÂåπÈÖç
    q: float = 0.90,         # ÊØèÁ±ªÂàÜ‰ΩçÈòàÂÄºÔºàÈ´òÁΩÆ‰ø°Ôºâ
    margin_min: float = 0.30 # ÁΩÆ‰ø°ËæπÈôÖÔºàwinner - second bestÔºâ
) -> Tuple[pd.Series, pd.DataFrame]:
    """
    Áî® Scanpy ÁöÑ score_genes ‰∏∫ÊØè‰∏™Á±ªÂà´ÊâìÂàÜÔºåÁÑ∂ÂêéÊåâÂàÜ‰ΩçÈòàÂÄº‰∏éËæπÈôÖÁîüÊàê‚ÄúÈì∂Ê†áÂáÜ‚Äù‰º™Ê†áÁ≠æ„ÄÇ
    ËøîÂõû: (labels, score_df)
    """
    # ‰ªÖ‰øùÁïôÂ≠òÂú®ÁöÑÂü∫Âõ†
    panels = {lbl: [g for g in genes if g in adata.var_names]
              for lbl, genes in marker_panels.items()}
    score_cols = []

    # ÂØπÊØè‰∏™Ê†áÁ≠æÊâìÂàÜÔºàÂèÇËÄÉÈõÜÂêàÁî± score_genes Ëá™Âä®ÁîüÊàêÂπ∂Ë°®ËææÈáèÂåπÈÖçÔºâ
    for lbl, genes in panels.items():
        if len(genes) == 0:
            logging.warning(f"[{lbl}] Êó†ÂèØÁî®Ê†áËÆ∞Âü∫Âõ†ÔºåË∑≥Ëøá„ÄÇ")
            continue
        col = f"score__{lbl}"
        sc.tl.score_genes(
            adata, gene_list=genes, score_name=col,
            ctrl_size=ctrl_size, use_raw=False, layer=layer
        )
        score_cols.append(col)

    if not score_cols:
        raise ValueError("marker_panels ‰∏≠ÁöÑÂü∫Âõ†ÂùáÊú™Âú® adata.var_names ‰∏≠ÊâæÂà∞„ÄÇ")

    S = adata.obs[score_cols].copy()
    S.columns = [c.replace("score__", "") for c in S.columns]

    # Ëµ¢ÂÆ∂Ê†áÁ≠æ‰∏éËæπÈôÖ
    winner = S.idxmax(axis=1)
    top1 = S.max(axis=1)
    top2 = S.apply(lambda r: r.nlargest(2).iloc[-1] if (r.notna().sum() >= 2) else np.nan, axis=1)
    margin = top1 - top2

    # ÊåâÁ±ªÂàÜ‰ΩçÈòàÂÄº
    per_label_thresh = {lbl: S[lbl].quantile(q) for lbl in S.columns}

    # Êù°‰ª∂ÔºöÂàÜÊï∞ ‚â• Êú¨Á±ªÈòàÂÄº ‰∏î margin ‚â• ËÆæÂÆöËæπÈôÖ
    keep = pd.Series(False, index=S.index)
    for idx, lbl in winner.items():
        thr = per_label_thresh.get(lbl, np.inf)
        keep.at[idx] = (S.at[idx, lbl] >= thr) and (margin.at[idx] >= margin_min)

    labels = winner.copy()
    labels.loc[~keep] = "Unknown"
    labels = labels.astype("category")

    coverage = (labels != "Unknown").mean()
    logging.info(f"[Pseudo-Labels] Ë¶ÜÁõñÁéá: {coverage:.2%}  (q={q}, margin={margin_min})")
    return labels, S


# %% [markdown]
# ## 5. ‰∏ªÂ∑•‰ΩúÊµÅÔºàÊé®ÁêÜ + ÂèØËßÜÂåñ + Âü∫‰∫éÈì∂Ê†áÂáÜÁöÑÂÆöÈáèËØÑ‰º∞Ôºâ

# %%
def run_and_visualize_experiment(
    sample_id: str,
    text_queries: Dict[str, str],
    experiment_name: str,
    marker_panels: Dict[str, List[str]],    # Êñ∞ÔºöÊØèÁ±ªÂõ∫ÂÆö‚ÄúÁªèÂÖ∏Ê†áËÆ∞‚ÄùÂàóË°®
    q: float = 0.90, margin_min: float = 0.30,
    models_to_test: Dict[str, Dict[str, Any]] = MODELS_TO_TEST,
    output_dir: Path = OUTPUT_DIR,
    device: str = DEVICE,
    batch_size: int = BATCH_SIZE,
    num_workers: int = NUM_WORKERS,
    palette_name: str = "Set1"
) -> HESTSample:
    """
    ÂØπÊåáÂÆöÊ†∑Êú¨ÂíåÊü•ËØ¢ÔºåÊâßË°åÂ§öÊ®°ÂûãÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÔºõÂπ∂‰ª•ÊîπËøõ‰º™Ê†áÁ≠æËøõË°åÂÆöÈáèËØÑ‰º∞„ÄÇ
    """
    logging.info(f"--- üöÄ ÂºÄÂßãÂÆûÈ™å: '{experiment_name}' on sample '{sample_id}' ---")
    # 1) Âä†ËΩΩÊï∞ÊçÆÂπ∂‰øùËØÅÈ¢ÑÂ§ÑÁêÜ
    sample = get_hest_sample(sample_id)
    ensure_log1p_layer(sample.adata, layer_name="log1p")

    tokenizer = open_clip.get_tokenizer(MODEL_NAME)
    query_labels = list(text_queries.keys())

    # 2) ËøêË°åÊâÄÊúâÊ®°ÂûãÊé®ÁêÜ
    for model_name, config in models_to_test.items():
        logging.info(f"Ê≠£Âú®Â§ÑÁêÜÊ®°Âûã: {model_name}...")
        model = load_clip_model(config['path'], config['model_name'], device)
        _, _, image_preprocessor = open_clip.create_model_and_transforms(config['model_name'])

        dataloader = DataLoader(
            WSISpotDataset(wsi=sample.wsi, coords=sample.adata.obsm['spatial'], transform=image_preprocessor),
            batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=False
        )

        with torch.inference_mode():
            tokenized = tokenizer(list(text_queries.values())).to(device)
            text_features = model.encode_text(tokenized, normalize=True)

            all_spot_embeddings = []
            for image_batch in tqdm(dataloader, desc=f"Êé®ÁêÜ: {model_name}", leave=False):
                image_features = model.encode_image(image_batch.to(device), normalize=True)
                all_spot_embeddings.append(image_features.cpu())

        spot_emb = torch.cat(all_spot_embeddings)
        similarity = spot_emb.to(device) @ text_features.T
        conf, pred_idx = torch.max(torch.softmax(similarity, dim=1), dim=1)

        # ÂÜôÂÖ• AnnData
        model_name_safe = model_name.replace(' ', '_').replace('(', '').replace(')', '')
        anno_col = f"anno_{experiment_name}_{model_name_safe}"
        conf_col = f"conf_{experiment_name}_{model_name_safe}"
        sample.adata.obs[anno_col] = pd.Categorical([query_labels[i] for i in pred_idx.cpu().numpy()],
                                                    categories=query_labels)
        sample.adata.obs[conf_col] = conf.cpu().numpy()
        logging.info(f"ÁªìÊûúÂ∑≤Ê∑ªÂä†Âà∞ AnnData: '{anno_col}'")

        # ÈáäÊîæ
        del model, text_features, spot_emb
        torch.cuda.empty_cache()

    # 3) ÂèØËßÜÂåñÔºà‰∏éÂéüÈÄªËæë‰∏ÄËá¥ÔºåÁªü‰∏ÄË∞ÉËâ≤ÊùøÔºâ
    logging.info("--- üìä Èò∂ÊÆµ‰∏Ä: ÂÆöÊÄßËØÑ‰º∞ (ÂèØËßÜÂåñ) ---")
    categories = list(text_queries.keys())
    palette = dict(zip(categories, sns.color_palette(palette_name, len(categories))))
    fig, axes = plt.subplots(1, len(models_to_test), figsize=(7 * len(models_to_test), 12), squeeze=False)
    fig.suptitle(f"'{experiment_name}' on Sample '{sample_id}'", fontsize=20, y=0.9)
    plot_params = {"show": False, "legend_loc": None, "frameon": False,
                   "library_id": sample.sample_id, "img_key": 'downscaled_fullres', "size": 1.2, "s": 1.5}
    for i, (model_name, _) in enumerate(models_to_test.items()):
        ax = axes[0, i]
        model_name_safe = model_name.replace(' ', '_').replace('(', '').replace(')', '')
        anno_col = f"anno_{experiment_name}_{model_name_safe}"
        sc.pl.spatial(sample.adata, color=anno_col, title=f"{model_name}", ax=ax, palette=palette, **plot_params)
    handles = [mpatches.Patch(color=palette[cat], label=cat) for cat in categories]
    fig.legend(handles=handles, title="Categories", loc='center right', bbox_to_anchor=(1.05, 0.5))
    plt.tight_layout(rect=[0, 0, 0.90, 0.96])
    if not DRYRUN:
        save_path = output_dir / f"vis_{experiment_name}_comparison_{sample_id}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight'); logging.info(f"ÂèØËßÜÂåñÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞: {save_path}")
    plt.show()

    # 4) ‰º™Ê†áÁ≠æÔºàÈì∂Ê†áÂáÜÔºâ‰∏éÂÆöÈáèËØÑ‰º∞
    logging.info("--- üìà Èò∂ÊÆµ‰∫å: ÂÆöÈáèËØÑ‰º∞ (Âü∫‰∫éÊîπËøõ‰º™Ê†áÁ≠æ) ---")
    pseudo_labels, score_df = create_silver_labels_scanpy(
        sample.adata, marker_panels=marker_panels, layer="log1p", q=q, margin_min=margin_min
    )
    kept_idx = pseudo_labels.index[pseudo_labels.values != "Unknown"]
    if len(kept_idx) == 0:
        logging.warning("‰º™Ê†áÁ≠æË¶ÜÁõñÁéá‰∏∫ 0ÔºåË∑≥ËøáÂÆöÈáèËØÑ‰º∞ÔºàËØ∑ÈÄÇÂΩìÈôç‰ΩéÂàÜ‰ΩçÈòàÂÄºÊàñËæπÈôÖÈòàÂÄºÔºâ„ÄÇ")
        return sample

    classification_results = []
    fig_cm, axes_cm = plt.subplots(1, len(models_to_test), figsize=(8 * len(models_to_test), 7), squeeze=False)
    fig_cm.suptitle(f"Confusion Matrices for '{experiment_name}' on Sample '{sample_id}' (silver labels)", fontsize=18, y=1.02)
    axes_cm = axes_cm.flatten()

    eval_labels = sorted(list(set(pseudo_labels[kept_idx].unique())))  # ‰∏çÂê´ Unknown

    for i, (model_name, _) in enumerate(models_to_test.items()):
        model_name_safe = model_name.replace(' ', '_').replace('(', '').replace(')', '')
        anno_col = f"anno_{experiment_name}_{model_name_safe}"
        preds = sample.adata.obs[anno_col].loc[kept_idx]

        # Áªü‰∏ÄÊ†áÁ≠æÂüü
        y_true = pd.Categorical(pseudo_labels.loc[kept_idx], categories=eval_labels)
        y_pred = pd.Categorical(preds, categories=eval_labels)

        acc = accuracy_score(y_true, y_pred)
        f1w = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        classification_results.append({"model": model_name, "coverage": len(kept_idx)/sample.adata.n_obs, "accuracy": acc, "f1_weighted": f1w})

        cm = confusion_matrix(y_true, y_pred, labels=eval_labels)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=eval_labels)
        disp.plot(ax=axes_cm[i], xticks_rotation='vertical', cmap='Blues')
        axes_cm[i].set_title(f"{model_name}")

    plt.tight_layout()
    if not DRYRUN:
        cm_path = output_dir / f"conf_matrix_{experiment_name}_silver_{sample_id}.png"
        plt.savefig(cm_path, dpi=300, bbox_inches='tight'); logging.info(f"Ê∑∑Ê∑ÜÁü©ÈòµÂõæÂ∑≤‰øùÂ≠òÂà∞: {cm_path}")
    plt.show()

    classification_df = pd.DataFrame(classification_results).set_index('model')
    print("\n--- Zero-Shot Classification vs. Silver Labels ---")
    display(classification_df)
    if not DRYRUN:
        df_save_path = output_dir / f"metrics_{experiment_name}_{sample_id}.csv"
        classification_df.to_csv(df_save_path); logging.info(f"ÂàÜÁ±ªÊåáÊ†áÂ∑≤‰øùÂ≠òÂà∞: {df_save_path}")

    logging.info(f"--- ‚úÖ ÂÆûÈ™å '{experiment_name}' ÂÆåÊàê ---")
    return sample


# %% [markdown]
# ## 6. ËøêË°å‰∫åÂàÜÁ±ª‰π≥ËÖ∫ÁôåÂÆûÈ™åÔºàÁ§∫‰æãÔºâ

# %%
# --- ÊñáÊú¨Êü•ËØ¢Ôºà‰æõ CLIP ÊñáÊú¨ÁºñÁ†Å‰ΩøÁî®ÔºõÂèØÁªßÁª≠Ê≤øÁî®‰Ω†ÁöÑÂéüÂßãÂÆö‰πâÔºâ ---
breast_cancer_binary_queries = {
    "Tumor (IDC)": (
        "KRT8 KRT18 KRT19 EPCAM MUC1 CLDN3 CLDN4 CLDN7 CDH1 KRT7 "
        "ERBB2 GRB7 ERBB3 ESR1 PGR FOXA1 GATA3 AGR2 TFF1 TFF3 "
        "TRPS1 SCGB2A2 BCL2 AR MKI67 TOP2A MYC CCND1 EGFR BIRC5 "
        "UBE2C CCNA2 CCNB1 CDC20 PLK1 AURKA AURKB CENPF NUSAP1 KIF11 "
        "KIF2C PTTG1 PRC1 ANLN MCM2 MCM4 MCM6 MCM7 TK1 PCNA"
    ),
    "Normal & Stroma": (
        "COL1A1 COL1A2 COL3A1 COL5A1 COL5A2 COL6A1 COL6A2 COL6A3 FN1 VIM "
        "LUM DCN BGN THY1 FAP PDGFRB RGS5 CSPG4 ACTA2 TAGLN "
        "MYH11 CNN1 DES VWF PECAM1 CLDN5 KDR FLT1 ENG CD34 "
        "MCAM PDPN PROX1 LYVE1 PTPRC LST1 LYZ CSF1R CD68 C1QB "
        "ITGAM ITGAX HLA-DRA HLA-DPB1 CD3D CD4 CD8A TRAC MS4A1 CD79A"
    ),
}

# --- ÁªèÂÖ∏Ê†áËÆ∞Èù¢ÊùøÔºàÂõ∫ÂÆö„ÄÅÂ∞èËÄåÁ≤æÔºâÔºå‰∏é‰∏äÈù¢ÊñáÊú¨Êü•ËØ¢Áõ∏‰∫íÁã¨Á´ã‰ª•ÈÅøÂÖç‰ø°ÊÅØÊ≥ÑÊºè ---
marker_panels = {
    "Tumor (IDC)": ["EPCAM","KRT8","KRT18","KRT19","KRT7","MUC1","CLDN3","CLDN4","CLDN7","ERBB2"],
    "Normal & Stroma": ["COL1A1","COL1A2","COL3A1","DCN","LUM","BGN","THY1","PDGFRB","TAGLN","ACTA2","VWF","PECAM1"]
}

# --- ËøêË°å ---
if not DRYRUN:
    sample_with_results = run_and_visualize_experiment(
        sample_id="TENX99",
        text_queries=breast_cancer_binary_queries,
        experiment_name="Breast_Cancer_Binary",
        marker_panels=marker_panels,
        q=0.90,               # ÂèØÊåâË¶ÜÁõñÁéáÈúÄË¶ÅË∞ÉÊï¥‰∏∫ 0.85
        margin_min=0.30       # ÂèØÊåâÈúÄË¶ÅË∞ÉÊï¥‰∏∫ 0.20~0.50
    )
else:
    logging.warning("Â§Ñ‰∫éÈ¢ÑÊºîÊ®°ÂºèÔºåË∑≥ËøáÂÆûÈ™åÊâßË°å„ÄÇ")

# ÔºàÂèØÈÄâÔºâÂçïÊ®°ÂûãÁ©∫Èó¥ÂõæÂø´ÈÄüÊü•Áúã
# sc.pl.spatial(
#     sample_with_results.adata,
#     color=f"anno_Breast_Cancer_Binary_OmiCLIP_Baseline",
#     title=f"OmiCLIP (Baseline) - Breast Cancer Binary Annotation",
#     s=1.5, show=True, legend_loc=None, frameon=False,
#     library_id=sample_with_results.sample_id, img_key='downscaled_fullres',
# )


# ===== Cell 17: Markdown =====
# Newer Version

# ===== Cell 18: Code (execution_count: 39) =====
# %% [markdown]
# # Á©∫Èó¥CLIPËØÑ‰º∞Ôºö‰∫∫Á±ª‰π≥ËÖ∫ÁôåÊï¥ÈòüÂàóÔºàÂê´ AUROC/AUPRC/ECE/Brier & McNemarÔºâ
# - ‰º™Ê†áÁ≠æÔºöscanpy.tl.score_genes Â§öÂü∫Âõ†ÊâìÂàÜ + ÂàÜ‰ΩçÈòàÂÄº + ÁΩÆ‰ø°ËæπÈôÖ + Unknown
# - ÊåáÊ†áÔºöAccuracy/F1ÔºàÂ∑≤ÊúâÔºâ+ AUROCÔºàmacro-OVRÔºâ/AUPRCÔºàmacro-OVRÔºâ/ECEÔºàmax prob, 15 binsÔºâ/BrierÔºàmulticlassÔºâ
# - ÈÖçÂØπÊ£ÄÈ™åÔºöMcNemarÔºàÂØπÂêÑÊ®°ÂûãÂØπÂú®‚ÄúÊòØÂê¶È¢ÑÊµãÊ≠£Á°Æ‚Äù‰∏äÁöÑÂ∑ÆÂºÇËøõË°åÈÖçÂØπÁªüËÆ°ÔºâÔºåÊèê‰æõÊØèÊ†∑Êú¨‰∏éÂÖ®ÈòüÂàóÔºàpooledÔºâpÂÄº

# =========================
# 1) ÂØºÂÖ•‰∏éÂü∫Á°ÄÈÖçÁΩÆ
# =========================
DRYRUN = False

import os, sys, logging, glob
from pathlib import Path
from typing import Dict, Any, Tuple, List, Optional
import numpy as np
import pandas as pd
import scanpy as sc
import torch
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from sklearn.metrics import (
    accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay,
    roc_auc_score, average_precision_score
)
from torch.utils.data import DataLoader, Dataset
from IPython.display import display

# ÁªüËÆ°ÂåÖÔºàMcNemarÔºâ
try:
    from statsmodels.stats.contingency_tables import mcnemar as sm_mcnemar
    _HAS_STATSMODELS = True
except Exception:
    _HAS_STATSMODELS = False
from scipy.stats import binomtest  # ‰Ωú‰∏∫Êó† statsmodels Êó∂ÁöÑÁ≤æÁ°Æ‰∫åÈ°πÊ£ÄÈ™åÊõø‰ª£

# È°πÁõÆË∑ØÂæÑ‰∏éÊú¨Âú∞Â∫ì
PROJECT_ROOT = Path("/home1/jijh/diffusion_project/git_repo/yuanspace")
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))
    print(f"Â∑≤Â∞Ü '{PROJECT_ROOT}' Ê∑ªÂä†Âà∞ sys.path")

import open_clip
from src.spaglam_preproc.utils.hest_loading import HESTDataset, HESTSample  # Â§çÁî®‰Ω†ÁöÑÂÆûÁé∞

# Êó•Âøó
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# =========================
# 2) Ë∑ØÂæÑ‰∏éÊ®°ÂûãÈÖçÁΩÆ
# =========================
HEST_DATA_DIR = Path("/cwStorage/nodecw_group/jijh/hest_1k")
OUTPUT_DIR    = Path("/cwStorage/nodecw_group/jijh/trained_models/all_comparisons/multipositive_vs_basline")

BASELINE_MODEL_CKPT = Path("/cwStorage/nodecw_group/jijh/trained_models/omiclip_base_model/omiclip_epoch_50.pt")
SPATIAL_MODEL_CKPT  = Path("/cwStorage/nodecw_group/jijh/trained_models/spatial_clip_base_model/multi_positice_loss50.pt")
MODEL_NAME = "ViT-B-32"

BATCH_SIZE  = 512
NUM_WORKERS = 16
DEVICE      = "cuda:0" if torch.cuda.is_available() else "cpu"

MODELS_TO_TEST = {
    "OmiCLIP (Baseline)": {"path": BASELINE_MODEL_CKPT, "model_name": MODEL_NAME},
    "Spatial CLIP (Ours)": {"path": SPATIAL_MODEL_CKPT, "model_name": MODEL_NAME},
}

if not DRYRUN:
    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
for name, cfg in MODELS_TO_TEST.items():
    assert cfg["path"].exists(), f"[{name}] ckpt ‰∏çÂ≠òÂú®: {cfg['path']}"
assert HEST_DATA_DIR.exists(), f"HEST Êï∞ÊçÆÁõÆÂΩï‰∏çÂ≠òÂú®: {HEST_DATA_DIR}"

# =========================
# 3) WSI Patch Dataset
# =========================
class WSISpotDataset(Dataset):
    """Êåâ spot ÂùêÊ†á‰ªé WSI ËØªÂèñ 224x224 patch"""
    def __init__(self, wsi, coords, transform):
        self.wsi = wsi
        self.coords = coords
        self.transform = transform
        self.patch_size = (224, 224)
        self.patch_radius = self.patch_size[0] // 2
    def __len__(self): return len(self.coords)
    def __getitem__(self, idx):
        c = self.coords[idx]
        top_left = (int(c[0] - self.patch_radius), int(c[1] - self.patch_radius))
        tile = self.wsi.read_region(top_left, 0, self.patch_size).convert("RGB")
        if self.transform: tile = self.transform(tile)
        return tile

# =========================
# 4) ‰º™Ê†áÁ≠æÔºàÈì∂Ê†áÂáÜÔºâÔºöscore_genes + ÂàÜ‰ΩçÈòàÂÄº + ÁΩÆ‰ø°ËæπÈôÖ + Unknown
# =========================
def ensure_log1p_layer(adata: sc.AnnData, layer_name: str = "log1p") -> None:
    if layer_name in adata.layers: 
        return
    sc.pp.normalize_total(adata, target_sum=1e4)  # CP10k
    sc.pp.log1p(adata)
    adata.layers[layer_name] = adata.X.copy()

def create_silver_labels_scanpy(
    adata: sc.AnnData,
    marker_panels: Dict[str, List[str]],
    layer: str = "log1p",
    ctrl_size: int = 50,
    q: float = 0.90,
    margin_min: float = 0.30
) -> Tuple[pd.Series, pd.DataFrame]:
    """ËøîÂõû (labels, score_df)Ôºõlabels Âê´ Unknown„ÄÇ"""
    panels = {lbl: [g for g in genes if g in adata.var_names]
              for lbl, genes in marker_panels.items()}
    score_cols = []
    for lbl, genes in panels.items():
        if not genes:
            logging.warning(f"[{lbl}] Êó†ÂèØÁî®Ê†áËÆ∞Âü∫Âõ†ÔºåË∑≥Ëøá„ÄÇ")
            continue
        col = f"score__{lbl}"
        sc.tl.score_genes(adata, gene_list=genes, score_name=col,
                          ctrl_size=ctrl_size, use_raw=False, layer=layer)
        score_cols.append(col)
    if not score_cols:
        raise ValueError("marker_panels ‰∏≠Âü∫Âõ†ÂùáÊú™ÂëΩ‰∏≠ adata.var_names„ÄÇ")

    S = adata.obs[score_cols].copy()
    S.columns = [c.replace("score__", "") for c in S.columns]

    winner = S.idxmax(axis=1)
    top1   = S.max(axis=1)
    top2   = S.apply(lambda r: r.nlargest(2).iloc[-1] if (r.notna().sum() >= 2) else np.nan, axis=1)
    margin = top1 - top2

    per_label_thresh = {lbl: S[lbl].quantile(q) for lbl in S.columns}
    keep = pd.Series(False, index=S.index)
    for idx, lbl in winner.items():
        thr = per_label_thresh.get(lbl, np.inf)
        keep.at[idx] = (S.at[idx, lbl] >= thr) and (margin.at[idx] >= margin_min)

    labels = winner.copy()
    labels.loc[~keep] = "Unknown"
    labels = labels.astype("category")

    coverage = (labels != "Unknown").mean()
    logging.info(f"[Silver labels] Ë¶ÜÁõñÁéá: {coverage:.2%} (q={q}, margin={margin_min})")
    return labels, S

# =========================
# 5) ËØÑ‰º∞Â∑•ÂÖ∑ÂáΩÊï∞ÔºöAUROC/AUPRC/ECE/Brier & McNemar
# =========================
def one_hot(y_idx: np.ndarray, n_class: int) -> np.ndarray:
    Y = np.zeros((len(y_idx), n_class), dtype=float)
    Y[np.arange(len(y_idx)), y_idx] = 1.0
    return Y

def macro_ovr_auroc(y_idx: np.ndarray, P: np.ndarray) -> float:
    """Â§öÁ±ª macro-OVR AUROCÔºõ‰∫åÂàÜÁ±ª‰πüÈÄÇÁî®„ÄÇ"""
    C = P.shape[1]
    Y = one_hot(y_idx, C)
    try:
        return roc_auc_score(Y, P, average='macro', multi_class='ovr')
    except Exception:
        return np.nan

def macro_ovr_auprc(y_idx: np.ndarray, P: np.ndarray) -> float:
    """Â§öÁ±ª macro-OVR AUPRCÔºõÈÄêÁ±ª AP ÂÜçÂπ≥Âùá„ÄÇ"""
    C = P.shape[1]
    ap = []
    for k in range(C):
        yk = (y_idx == k).astype(int)
        try:
            ap.append(average_precision_score(yk, P[:, k]))
        except Exception:
            ap.append(np.nan)
    return float(np.nanmean(ap)) if len(ap) else np.nan

def multiclass_brier(y_idx: np.ndarray, P: np.ndarray) -> float:
    Y = one_hot(y_idx, P.shape[1])
    return float(np.mean(np.sum((P - Y) ** 2, axis=1)))

def ece_maxprob(y_idx: np.ndarray, P: np.ndarray, n_bins: int = 15) -> float:
    """ECEÔºöÁî® max prob & ÊòØÂê¶È¢ÑÊµãÊ≠£Á°ÆÔºàÂ§öÁ±ªÁâàÊú¨ÁöÑÂ∏∏Áî®ÂÆûÁé∞Ôºâ"""
    conf = P.max(axis=1)
    pred = P.argmax(axis=1)
    corr = (pred == y_idx).astype(float)
    bins = np.linspace(0.0, 1.0, n_bins + 1)
    ece = 0.0
    N = len(conf)
    for i in range(n_bins):
        lo, hi = bins[i], bins[i+1]
        m = (conf >= lo) & (conf < hi) if i < n_bins - 1 else (conf >= lo) & (conf <= hi)
        if m.sum() == 0: 
            continue
        acc_bin = corr[m].mean()
        conf_bin = conf[m].mean()
        ece += np.abs(acc_bin - conf_bin) * (m.sum() / N)
    return float(ece)

def mcnemar_pvalue_from_correct(correctA: np.ndarray, correctB: np.ndarray) -> Tuple[int, int, int, int, float]:
    """Âü∫‰∫é‚ÄúÊòØÂê¶È¢ÑÊµãÊ≠£Á°Æ‚ÄùÁöÑ2x2Ë°®ÂÅöMcNemarÔºöËøîÂõû a,b,c,d,p"""
    a = int(np.sum( (correctA == 1) & (correctB == 1) ))
    b = int(np.sum( (correctA == 0) & (correctB == 1) ))  # AÈîôBÂØπ
    c = int(np.sum( (correctA == 1) & (correctB == 0) ))  # AÂØπBÈîô
    d = int(np.sum( (correctA == 0) & (correctB == 0) ))
    if b + c == 0:
        pval = 1.0  # ÂÆåÂÖ®‰∏ÄËá¥ÔºåÊó†Â∑ÆÂºÇ
    else:
        if _HAS_STATSMODELS:
            exact = (b + c) <= 100
            res = sm_mcnemar([[a, b], [c, d]], exact=exact, correction=not exact)
            pval = float(res.pvalue)
        else:
            # Á≤æÁ°Æ‰∫åÈ°πÊõø‰ª£ÔºöH0 ‰∏ã b~Binom(b+c,0.5)ÔºåÂèå‰æß
            pval = float(binomtest(k=min(b, c), n=b+c, p=0.5, alternative='two-sided').pvalue)
    return a, b, c, d, pval

# =========================
# 6) ÂçïÊ†∑Êú¨ÔºöÊé®ÁêÜ + ËØÑ‰º∞ÔºàÊñ∞Â¢ûÔºöAUROC/AUPRC/ECE/BrierÔºõËøîÂõûMcNemarÊâÄÈúÄÊ≠£Á°ÆÊÄßÂêëÈáèÔºâ
# =========================
def load_clip_model(checkpoint_path: Path, model_name: str, device: str) -> torch.nn.Module:
    model, _, _ = open_clip.create_model_and_transforms(model_name, pretrained=None)
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    if all(k.startswith('module.') for k in state_dict.keys()):
        state_dict = {k[7:]: v for k, v in state_dict.items()}
    try:
        model.load_state_dict(state_dict)
    except RuntimeError as e:
        logging.warning(f"‰∏•Ê†ºÂä†ËΩΩÂ§±Ë¥•: {e}ÔºåÈááÁî®Èùû‰∏•Ê†º„ÄÇ")
        model.load_state_dict(state_dict, strict=False)
    model.to(device); model.eval()
    return model

def run_inference_and_eval_single(
    sample_id: str,
    text_queries: Dict[str, str],
    marker_panels: Dict[str, List[str]],
    experiment_name: str,
    q: float = 0.90,
    margin_min: float = 0.30,
    models_to_test: Dict[str, Dict[str, Any]] = MODELS_TO_TEST,
    device: str = DEVICE,
    batch_size: int = BATCH_SIZE,
    num_workers: int = NUM_WORKERS,
) -> Tuple[HESTSample, pd.DataFrame, Dict[str, np.ndarray], List[str]]:
    """
    ËøîÂõûÔºö
      - sample: HESTSample
      - metrics_df_per_model: ÊØèÊ®°ÂûãËØÑ‰º∞ÔºàÂê´ Accuracy/F1/AUROC/AUPRC/ECE/BrierÔºâ
      - correct_flags: {model_name: bool array of correctness on evaluated spots}
      - eval_labels: ÂèÇ‰∏éËØÑ‰º∞ÁöÑÊ†áÁ≠æÈ°∫Â∫èÔºà‰∏éÊ¶ÇÁéáÂàó‰∏ÄËá¥Ôºâ
    """
    # Âä†ËΩΩÊ†∑Êú¨
    ds = HESTDataset(str(HEST_DATA_DIR))
    samples = ds.get_samples(sample_ids=[sample_id])
    if len(samples) == 0:
        raise FileNotFoundError(f"{sample_id} Êú™Âú®ÂÖÉÊï∞ÊçÆ‰∏≠ÊâæÂà∞„ÄÇ")
    sample = samples[0]
    sample.load_st_data(lazy=False)
    sample.load_wsi()

    if sample.wsi is None:
        logging.warning(f"{sample_id}: Áº∫Â∞ë WSIÔºåË∑≥Ëøá„ÄÇ")
        return sample, pd.DataFrame(), {}, []

    ensure_log1p_layer(sample.adata, layer_name="log1p")

    tokenizer = open_clip.get_tokenizer(MODEL_NAME)
    query_labels = list(text_queries.keys())
    label_to_col = {lbl: i for i, lbl in enumerate(query_labels)}

    # Êé®ÁêÜÔºöÈÄêÊ®°ÂûãÔºå‰øùÂ≠òÂÖ®‰ΩìspotÁöÑÁ±ªÂà´Ê¶ÇÁéá
    model_probs = {}  # {model_name: np.ndarray (n_spots x n_classes)}
    for model_name, cfg in models_to_test.items():
        model = load_clip_model(cfg['path'], cfg['model_name'], device)
        _, _, image_pre = open_clip.create_model_and_transforms(cfg['model_name'])
        loader = DataLoader(
            WSISpotDataset(wsi=sample.wsi, coords=sample.adata.obsm['spatial'], transform=image_pre),
            batch_size=batch_size, shuffle=False, num_workers=num_workers,
            pin_memory=True, persistent_workers=False
        )
        with torch.inference_mode():
            tok = tokenizer(list(text_queries.values())).to(device)
            txt_feat = model.encode_text(tok, normalize=True)
            all_emb = []
            for batch in tqdm(loader, desc=f"{sample_id} Êé®ÁêÜ: {model_name}", leave=False):
                img_feat = model.encode_image(batch.to(device), normalize=True)
                all_emb.append(img_feat.cpu())
        spot_emb = torch.cat(all_emb, dim=0)
        sim = spot_emb.to(device) @ txt_feat.T
        prob = torch.softmax(sim, dim=1).cpu().numpy()
        model_probs[model_name] = prob

        # ‰øùÂ≠ò‰∏ªÁªìÊûúÂàóÔºà‰∏é‰πãÂâç‰øùÊåÅÂÖºÂÆπÔºâ
        conf = prob.max(axis=1)
        pred_idx = prob.argmax(axis=1)
        safe = model_name.replace(' ', '_').replace('(', '').replace(')', '')
        anno_col = f"anno_{experiment_name}_{safe}"
        conf_col = f"conf_{experiment_name}_{safe}"
        sample.adata.obs[anno_col] = pd.Categorical([query_labels[i] for i in pred_idx],
                                                    categories=query_labels)
        sample.adata.obs[conf_col] = conf

        del model, txt_feat, spot_emb
        torch.cuda.empty_cache()

    # ‰º™Ê†áÁ≠æÔºàÈì∂Ê†áÂáÜÔºâ
    pseudo, score_df = create_silver_labels_scanpy(
        sample.adata, marker_panels=marker_panels, layer="log1p", q=q, margin_min=margin_min
    )
    kept_idx = pseudo.index[pseudo.values != "Unknown"]
    n_eval   = len(kept_idx); n_total = sample.adata.n_obs
    if n_eval == 0:
        logging.warning(f"{sample_id}: ‰º™Ê†áÁ≠æË¶ÜÁõñÁéá‰∏∫ 0ÔºåË∑≥ËøáÂÆöÈáèËØÑ‰º∞„ÄÇ")
        return sample, pd.DataFrame(), {}, []

    # ÂØπÂ∫îÂà∞Êï∞ÁªÑ‰∏ãÊ†áÔºàDataLoader È°∫Â∫èÂç≥ obs È°∫Â∫èÔºâ
    pos = sample.adata.obs_names.get_indexer(kept_idx)
    assert np.all(pos >= 0)

    # ‰ªÖÂØπ pseudo Âá∫Áé∞ÁöÑÁ±ªÂÅöËØÑ‰º∞Ôºå‰øùÊåÅÂàóÈ°∫Â∫èÁ®≥ÂÆö
    eval_labels = sorted(list(set(pseudo[kept_idx].unique())))  # ‰∏çÂê´ Unknown
    eval_cols = [label_to_col[lbl] for lbl in eval_labels]

    # ÈÄêÊ®°ÂûãËØÑ‰º∞
    rows = []
    correct_flags = {}
    y_true_idx = np.array([eval_labels.index(lbl) for lbl in pseudo.loc[kept_idx].tolist()], dtype=int)

    for model_name, prob_all in model_probs.items():
        # ÈÄâÂèñËØÑ‰º∞ÂàóÂπ∂Âú®ËØÑ‰º∞Âàó‰∏äÂΩí‰∏ÄÂåñÔºàÈò≤Ê≠¢Êúâ‚ÄúÂÖ∂‰ªñÁ±ª‚ÄùÊó∂Ê¶ÇÁéá‰∏çÊª°1Ôºâ
        P = prob_all[pos][:, eval_cols]
        P = P / np.maximum(P.sum(axis=1, keepdims=True), 1e-12)

        # È¢ÑÊµã/Ê≠£Á°ÆÊÄß
        pred_idx = P.argmax(axis=1)
        correct = (pred_idx == y_true_idx).astype(int)
        correct_flags[model_name] = correct.copy()

        # ÊåáÊ†á
        acc = float(correct.mean())
        f1w = float(f1_score(y_true_idx, pred_idx, average='weighted', zero_division=0))
        auroc = macro_ovr_auroc(y_true_idx, P)
        auprc = macro_ovr_auprc(y_true_idx, P)
        ece   = ece_maxprob(y_true_idx, P, n_bins=15)
        brier = multiclass_brier(y_true_idx, P)

        rows.append({
            "sample_id": sample.sample_id,
            "model": model_name,
            "accuracy": acc,
            "f1_weighted": f1w,
            "auroc_macro_ovr": auroc,
            "auprc_macro_ovr": auprc,
            "ece_15bin": ece,
            "brier": brier,
            "coverage": n_eval / n_total,
            "n_eval_spots": n_eval,
            "n_total_spots": n_total,
            "n_correct": acc * n_eval
        })

    metrics_df = pd.DataFrame(rows)
    return sample, metrics_df, correct_flags, eval_labels

# =========================
# 7) ÈÄâÊã©ÈòüÂàóÔºö‰∫∫Á±ª + ‰π≥ËÖ∫ÔºàBreastÔºâ
# =========================
def get_breast_cancer_sample_ids(hest_data_dir: Path) -> List[str]:
    ds = HESTDataset(str(hest_data_dir))
    df = ds.meta_df.copy()
    organ_mask   = (df['organ'] == 'Breast')
    species_mask = (df['species'] == 'Homo sapiens')
    cancer_mask  = (df['disease_state'].fillna('') == 'Cancer')
    brca_mask    = (df['oncotree_code'].fillna('') == 'BRCA')
    mask = species_mask & (organ_mask | brca_mask) & (cancer_mask | brca_mask)
    cohort = df.loc[mask, 'id'].dropna().unique().tolist()
    logging.info(f"[Cohort] ‰∫∫Á±ª‰π≥ËÖ∫ÁôåÊ†∑Êú¨Êï∞Èáè: {len(cohort)} -> {cohort}")
    return cohort

# =========================
# 8) ÈòüÂàóËØÑ‰º∞ÔºàÂê´ McNemarÔºöÊØèÊ†∑Êú¨ & ÂÖ®ÈòüÂàó pooledÔºâ
# =========================
from itertools import combinations

def evaluate_breast_cancer_cohort(
    text_queries: Dict[str, str],
    marker_panels: Dict[str, List[str]],
    experiment_name: str = "Breast_Cancer_Binary",
    q: float = 0.90, margin_min: float = 0.30,
    models_to_test: Dict[str, Dict[str, Any]] = MODELS_TO_TEST,
    device: str = DEVICE
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    ËøîÂõûÔºö
      - per_sample_df: ÊØèÊ†∑Êú¨ x ÊØèÊ®°Âûã ÊåáÊ†á + ÂÖÉÊï∞ÊçÆ
      - per_group_df : ÊåâÊ®°Âûã & (st_technology/oncotree_code/preservation_method) ÂàÜÁªÑÂä†ÊùÉÊ±áÊÄª
      - global_df    : ÂÖ®Â±ÄÔºàÊåâ spot Âä†ÊùÉÔºâ‰∏éÊ†∑Êú¨ÂÆèÂπ≥Âùá
      - mcnemar_per_sample_df: ÊØèÊ†∑Êú¨ÁöÑÊ®°ÂûãÂØπ McNemar Ê£ÄÈ™åÁªìÊûú
      - mcnemar_pooled_df    : ÂÖ®ÈòüÂàó pooled ÁöÑÊ®°ÂûãÂØπ McNemar Ê£ÄÈ™åÁªìÊûú
    """
    sample_ids = get_breast_cancer_sample_ids(HEST_DATA_DIR)
    if len(sample_ids) == 0:
        logging.warning("Êú™ÊâæÂà∞‰∫∫Á±ª‰π≥ËÖ∫ÁôåÊ†∑Êú¨„ÄÇ")
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    ds = HESTDataset(str(HEST_DATA_DIR))
    all_rows = []
    mcnemar_rows = []
    pooled_bc = {}  # {(A,B): {'b':sum, 'c':sum, 'a':sum, 'd':sum}}
    failed   = []

    model_names = list(models_to_test.keys())
    model_pairs = list(combinations(model_names, 2))

    for sid in sample_ids:
        try:
            sample, df_metrics, correct_flags, eval_labels = run_inference_and_eval_single(
                sid, text_queries, marker_panels, experiment_name,
                q=q, margin_min=margin_min, models_to_test=models_to_test, device=device
            )
            if df_metrics.empty or not correct_flags:
                failed.append(sid)
                continue

            # ËøΩÂä†ÂÖÉÊï∞ÊçÆ
            meta = sample.metadata_dict if isinstance(sample.metadata_dict, dict) else {}
            md = {
                'sample_id'            : sample.sample_id,
                'organ'                : meta.get('organ'),
                'species'              : meta.get('species'),
                'disease_state'        : meta.get('disease_state'),
                'oncotree_code'        : meta.get('oncotree_code'),
                'st_technology'        : meta.get('st_technology'),
                'preservation_method'  : meta.get('preservation_method'),
                'nb_genes'             : meta.get('nb_genes'),
                'data_publication_date': meta.get('data_publication_date'),
                'license'              : meta.get('license'),
                'tissue'               : meta.get('tissue'),
                'subseries'            : meta.get('subseries'),
                'inter_spot_dist'      : meta.get('inter_spot_dist'),
                'spot_diameter'        : meta.get('spot_diameter'),
                'pixel_size_um_embedded': meta.get('pixel_size_um_embedded'),
                'pixel_size_um_estimated': meta.get('pixel_size_um_estimated'),
                'fullres_px_width'     : meta.get('fullres_px_width'),
                'fullres_px_height'    : meta.get('fullres_px_height'),
            }
            df_metrics = df_metrics.merge(pd.DataFrame([md]), on='sample_id', how='left')
            all_rows.append(df_metrics)

            # ---- McNemarÔºöÊØèÊ†∑Êú¨ + Á¥ØÁßØÂà∞ pooled ----
            for A, B in model_pairs:
                if A not in correct_flags or B not in correct_flags:
                    continue
                a, b, c, d, pval = mcnemar_pvalue_from_correct(
                    correct_flags[A].astype(int), correct_flags[B].astype(int)
                )
                mcnemar_rows.append({
                    "sample_id": sample.sample_id,
                    "model_A": A, "model_B": B,
                    "a_both_correct": a, "b_A_wrong_B_right": b, "c_A_right_B_wrong": c, "d_both_wrong": d,
                    "n_discordant": b + c,
                    "p_value": pval,
                    "st_technology": meta.get('st_technology'),
                    "oncotree_code": meta.get('oncotree_code'),
                    "preservation_method": meta.get('preservation_method')
                })
                key = (A, B)
                if key not in pooled_bc:
                    pooled_bc[key] = {'a':0,'b':0,'c':0,'d':0}
                pooled_bc[key]['a'] += a
                pooled_bc[key]['b'] += b
                pooled_bc[key]['c'] += c
                pooled_bc[key]['d'] += d

        except Exception as e:
            logging.exception(f"{sid} ËØÑ‰º∞Â§±Ë¥•: {e}")
            failed.append(sid)

    if len(all_rows) == 0:
        logging.warning("ÈòüÂàóËØÑ‰º∞Êú™ÂæóÂà∞‰ªª‰ΩïÊåáÊ†á„ÄÇ")
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    per_sample_df = pd.concat(all_rows, ignore_index=True)
    mcnemar_per_sample_df = pd.DataFrame(mcnemar_rows)

    # -------- ÂÖ®Â±ÄÊ±áÊÄªÔºàÊåâ spot Âä†ÊùÉÔºâ‰∏éÊ†∑Êú¨ÂÆèÂπ≥Âùá --------
    def _global_summary(df: pd.DataFrame) -> pd.DataFrame:
        out = []
        for model, g in df.groupby('model'):
            n_eval = g['n_eval_spots'].sum()
            n_corr = g['n_correct'].sum()
            row = {
                "model": model,
                "global_n_eval_spots": int(n_eval),
                "global_accuracy_weighted": (n_corr / n_eval) if n_eval > 0 else np.nan,
                "macro_accuracy": g['accuracy'].mean(),
                "macro_f1_weighted": g['f1_weighted'].mean(),
                "macro_auroc_macro_ovr": g['auroc_macro_ovr'].mean(),
                "macro_auprc_macro_ovr": g['auprc_macro_ovr'].mean(),
                "macro_ece_15bin": g['ece_15bin'].mean(),
                "macro_brier": g['brier'].mean(),
                "n_samples": g['sample_id'].nunique()
            }
            out.append(row)
        return pd.DataFrame(out)

    global_df = _global_summary(per_sample_df)

    # -------- ÂàÜÁªÑÊ±áÊÄªÔºàÂπ≥Âè∞/ËÇøÁò§Á±ªÂûã/‰øùÂ≠òÊñπÂºèÔºâÊåâ spot Âä†ÊùÉ --------
    def _grouped_summary(df: pd.DataFrame, by: str) -> pd.DataFrame:
        rows = []
        for (model, grp), gdf in df.groupby(['model', by]):
            n_eval = gdf['n_eval_spots'].sum()
            n_corr = gdf['n_correct'].sum()
            rows.append({
                "group_by": by,
                by: grp,
                "model": model,
                "n_samples": gdf['sample_id'].nunique(),
                "global_n_eval_spots": int(n_eval),
                "global_accuracy_weighted": (n_corr / n_eval) if n_eval > 0 else np.nan,
                "macro_accuracy": gdf['accuracy'].mean(),
                "macro_f1_weighted": gdf['f1_weighted'].mean(),
                "macro_auroc_macro_ovr": gdf['auroc_macro_ovr'].mean(),
                "macro_auprc_macro_ovr": gdf['auprc_macro_ovr'].mean(),
                "macro_ece_15bin": gdf['ece_15bin'].mean(),
                "macro_brier": gdf['brier'].mean(),
            })
        return pd.DataFrame(rows)

    group_keys = ['st_technology', 'oncotree_code', 'preservation_method']
    grouped_frames = [_grouped_summary(per_sample_df, k) for k in group_keys]
    per_group_df = pd.concat(grouped_frames, ignore_index=True)

    # -------- pooled McNemarÔºàÂÖ®ÈòüÂàóÔºâ--------
    pooled_rows = []
    for (A, B), abcd in pooled_bc.items():
        a, b, c, d = abcd['a'], abcd['b'], abcd['c'], abcd['d']
        if b + c == 0:
            pval = 1.0
        else:
            if _HAS_STATSMODELS:
                exact = (b + c) <= 1000
                res = sm_mcnemar([[a,b],[c,d]], exact=exact, correction=not exact)
                pval = float(res.pvalue)
            else:
                pval = float(binomtest(k=min(b,c), n=b+c, p=0.5, alternative='two-sided').pvalue)
        pooled_rows.append({
            "model_A": A, "model_B": B,
            "a_both_correct": a, "b_A_wrong_B_right": b, "c_A_right_B_wrong": c, "d_both_wrong": d,
            "n_discordant": b + c,
            "p_value": pval
        })
    mcnemar_pooled_df = pd.DataFrame(pooled_rows)

    # -------- ‰øùÂ≠òÁªìÊûú --------
    if not DRYRUN:
        per_sample_path = OUTPUT_DIR / f"cohort_breast_per_sample_{experiment_name}.csv"
        per_group_path  = OUTPUT_DIR / f"cohort_breast_grouped_{experiment_name}.csv"
        global_path     = OUTPUT_DIR / f"cohort_breast_global_{experiment_name}.csv"
        mcnemar_s_path  = OUTPUT_DIR / f"cohort_breast_mcnemar_per_sample_{experiment_name}.csv"
        mcnemar_p_path  = OUTPUT_DIR / f"cohort_breast_mcnemar_pooled_{experiment_name}.csv"
        per_sample_df.to_csv(per_sample_path, index=False)
        per_group_df.to_csv(per_group_path, index=False)
        global_df.to_csv(global_path, index=False)
        mcnemar_per_sample_df.to_csv(mcnemar_s_path, index=False)
        mcnemar_pooled_df.to_csv(mcnemar_p_path, index=False)
        logging.info(f"Â∑≤‰øùÂ≠ò:\n- {per_sample_path}\n- {per_group_path}\n- {global_path}\n- {mcnemar_s_path}\n- {mcnemar_p_path}")

    if failed:
        logging.warning(f"‰ª•‰∏ãÊ†∑Êú¨Êú™ËÉΩËØÑ‰º∞ÔºàÁº∫ WSI ÊàñÊä•ÈîôÔºâÔºö{failed}")

    return per_sample_df, per_group_df, global_df, mcnemar_per_sample_df, mcnemar_pooled_df

# =========================
# 9) ‰π≥ËÖ∫‰∫åÂàÜÁ±ªÔºöÊñáÊú¨Êü•ËØ¢ & ÁªèÂÖ∏Ê†áËÆ∞Ôºà‰∏éËÆ≠ÁªÉ Top-50 ÊñáÊú¨ÊèêÁ§∫Ëß£ËÄ¶Ôºâ
# =========================
breast_cancer_binary_queries = {
    "Tumor (IDC)": (
        "KRT8 KRT18 KRT19 EPCAM MUC1 CLDN3 CLDN4 CLDN7 CDH1 KRT7 "
        "ERBB2 GRB7 ERBB3 ESR1 PGR FOXA1 GATA3 AGR2 TFF1 TFF3 "
        "TRPS1 SCGB2A2 BCL2 AR MKI67 TOP2A MYC CCND1 EGFR BIRC5 "
        "UBE2C CCNA2 CCNB1 CDC20 PLK1 AURKA AURKB CENPF NUSAP1 KIF11 "
        "KIF2C PTTG1 PRC1 ANLN MCM2 MCM4 MCM6 MCM7 TK1 PCNA"
    ),
    "Normal & Stroma": (
        "COL1A1 COL1A2 COL3A1 COL5A1 COL5A2 COL6A1 COL6A2 COL6A3 FN1 VIM "
        "LUM DCN BGN THY1 FAP PDGFRB RGS5 CSPG4 ACTA2 TAGLN "
        "MYH11 CNN1 DES VWF PECAM1 CLDN5 KDR FLT1 ENG CD34 "
        "MCAM PDPN PROX1 LYVE1 PTPRC LST1 LYZ CSF1R CD68 C1QB "
        "ITGAM ITGAX HLA-DRA HLA-DPB1 CD3D CD4 CD8A TRAC MS4A1 CD79A"
    ),
}

marker_panels = {
    "Tumor (IDC)": ["EPCAM","KRT8","KRT18","KRT19","KRT7","MUC1","CLDN3","CLDN4","CLDN7","ERBB2"],
    "Normal & Stroma": ["COL1A1","COL1A2","COL3A1","DCN","LUM","BGN","THY1","PDGFRB","TAGLN","ACTA2","VWF","PECAM1"],
}

# =========================
# 10) ËøêË°åÊï¥ÈòüÂàóËØÑ‰º∞
# =========================
if not DRYRUN:
    per_sample_df, per_group_df, global_df, mcnemar_per_sample_df, mcnemar_pooled_df = evaluate_breast_cancer_cohort(
        text_queries=breast_cancer_binary_queries,
        marker_panels=marker_panels,
        experiment_name="Breast_Cancer_Binary",
        q=0.90, margin_min=0.30,
        models_to_test=MODELS_TO_TEST,
        device=DEVICE
    )
    print("=== ÊØèÊ†∑Êú¨ÊòéÁªÜÔºàÂâç10Ë°åÔºâ===")
    display(per_sample_df.head(10))
    print("=== ÂàÜÁªÑÊ±áÊÄªÔºàÂπ≥Âè∞/ËÇøÁò§Á±ªÂûã/‰øùÂ≠òÊñπÂºèÔºâ===")
    display(per_group_df)
    print("=== ÂÖ®Â±ÄÊ±áÊÄª ===")
    display(global_df)
    print("=== McNemarÔºàÊØèÊ†∑Êú¨Ôºâ===")
    display(mcnemar_per_sample_df.head(10))
    print("=== McNemarÔºàÂÖ®ÈòüÂàó pooledÔºâ===")
    display(mcnemar_pooled_df)
else:
    logging.warning("DRYRUN=TrueÔºåË∑≥ËøáÈòüÂàóËØÑ‰º∞„ÄÇ")


# ===== Cell 19: Code (execution_count: 40) =====
# %% [markdown]
# # ÂèØËßÜÂåñÔºöÊï¥ÈòüÂàóÁ©∫Èó¥CLIPËØÑ‰º∞ÁªìÊûú

# ========= Áî®Êà∑ÂèØË∞ÉÂèÇÊï∞ =========
EXPERIMENT_NAME = "Breast_Cancer_Binary"
TOP_K_GROUPS = 12      # ÂàÜÁªÑÂèØËßÜÂåñÂè™ÂèñÂâçK‰∏™Ê†∑Êú¨Êï∞ÊúÄÂ§öÁöÑÁªÑ
SHOW_VIOLIN = False    # True: Â∞èÊèêÁê¥ÂõæÔºõFalse: ÁÆ±Á∫øÂõæ
DOT_ALPHA = 0.25       # ÁÆ±/ÊèêÁê¥Âè†Âä†Êï£ÁÇπÈÄèÊòéÂ∫¶
SEED = 7               # ÈöèÊú∫ÁßçÂ≠ê(Áî®‰∫éÊäñÂä®)
# =================================

import os
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

# ===== Ë∑ØÂæÑÔºå‰∏éËÆ≠ÁªÉ/ËØÑ‰º∞‰ª£Á†Å‰øùÊåÅ‰∏ÄËá¥ =====
OUTPUT_DIR = Path("/cwStorage/nodecw_group/jijh/trained_models/all_comparisons/multipositive_vs_basline")
FIG_DIR = OUTPUT_DIR / "figs" / EXPERIMENT_NAME
FIG_DIR.mkdir(parents=True, exist_ok=True)

# ====== ËØªÂÖ•ÁªìÊûú ======
def load_results(output_dir: Path, experiment_name: str):
    def _try_read(name):
        p = output_dir / f"cohort_breast_{name}_{experiment_name}.csv"
        if p.exists():
            return pd.read_csv(p)
        return None
    per_sample = _try_read("per_sample")
    per_group  = _try_read("grouped")
    global_df  = _try_read("global")
    mcnemar_s  = _try_read("mcnemar_per_sample")
    mcnemar_p  = _try_read("mcnemar_pooled")
    return per_sample, per_group, global_df, mcnemar_s, mcnemar_p

per_sample_df, per_group_df, global_df, mcnemar_per_sample_df, mcnemar_pooled_df = load_results(OUTPUT_DIR, EXPERIMENT_NAME)

# ÁÆÄÂçïÊ†°È™å
print("Loaded:")
print(" - per_sample_df:", None if per_sample_df is None else per_sample_df.shape)
print(" - per_group_df :", None if per_group_df  is None else per_group_df.shape)
print(" - global_df    :", None if global_df     is None else global_df.shape)
print(" - mcnemar_sample:", None if mcnemar_per_sample_df is None else mcnemar_per_sample_df.shape)
print(" - mcnemar_pooled:", None if mcnemar_pooled_df     is None else mcnemar_pooled_df.shape)

# ====== ÈÄöÁî®Â∑•ÂÖ∑ ======
def _savefig(fig, name: str, dpi=180, tight=True):
    out = FIG_DIR / f"{name}.png"
    if tight:
        fig.savefig(out, dpi=dpi, bbox_inches="tight")
    else:
        fig.savefig(out, dpi=dpi)
    print(f"Saved: {out}")

def _maybe_cols(df: pd.DataFrame, cols: list):
    return [c for c in cols if (df is not None and c in df.columns)]

def _jitter(n, scale=0.08, seed=SEED):
    rng = np.random.default_rng(seed)
    return rng.normal(0, scale, n)

# ====== 1) ÂÖ®Â±ÄÊ±áÊÄªÊü±Áä∂Âõæ ======
def plot_global_bars(global_df: pd.DataFrame):
    if global_df is None or global_df.empty:
        print("[Global] Êó†Êï∞ÊçÆ„ÄÇ")
        return
    metrics = _maybe_cols(global_df, [
        "global_accuracy_weighted",
        "macro_accuracy",
        "macro_f1_weighted",
        "macro_auroc_macro_ovr",
        "macro_auprc_macro_ovr",
        "macro_ece_15bin",
        "macro_brier",
    ])
    if not metrics:
        print("[Global] Êú™ÊâæÂà∞ÂèØÁªòÂà∂ÁöÑÊåáÊ†áÂàó„ÄÇ")
        return

    for m in metrics:
        fig, ax = plt.subplots(figsize=(7,4.5))
        x = np.arange(len(global_df))
        ax.bar(x, global_df[m].values)
        ax.set_xticks(x); ax.set_xticklabels(global_df["model"].astype(str), rotation=20, ha='right')
        ax.set_ylabel(m)
        ax.set_title(f"Global Summary ‚Äî {m}")
        for i, v in enumerate(global_df[m].values):
            ax.text(i, v, f"{v:.3f}", ha='center', va='bottom', fontsize=9)
        plt.tight_layout()
        _savefig(fig, f"global_bar_{m}")

plot_global_bars(global_df)

# ====== 2) ÊØèÊ†∑Êú¨ÂàÜÂ∏ÉÔºàÁÆ±/ÊèêÁê¥Ôºâ ======
def plot_per_sample_distribution(per_sample_df: pd.DataFrame, metric: str, show_violin: bool = SHOW_VIOLIN):
    if per_sample_df is None or per_sample_df.empty or metric not in per_sample_df.columns:
        print(f"[Per-sample] Âàó {metric} ‰∏çÂ≠òÂú®ÊàñÊó†Êï∞ÊçÆ„ÄÇ")
        return
    order = per_sample_df.groupby("model")[metric].median().sort_values(ascending=False).index.tolist()
    models = order
    fig, ax = plt.subplots(figsize=(8,5))
    data = [per_sample_df.loc[per_sample_df["model"]==m, metric].dropna().values for m in models]

    # ÁÆ±/ÊèêÁê¥
    if show_violin:
        parts = ax.violinplot(data, showmeans=False, showmedians=True, showextrema=False)
    else:
        bp = ax.boxplot(data, showfliers=False, notch=False)

    # Âè†Âä†Êï£ÁÇπ
    xs = []
    ys = []
    for i, m in enumerate(models, start=1):
        y = per_sample_df.loc[per_sample_df["model"]==m, metric].dropna().values
        x = np.full_like(y, i, dtype=float) + _jitter(len(y))
        xs.append(x); ys.append(y)
    ax.plot(np.concatenate(xs), np.concatenate(ys), 'o', alpha=DOT_ALPHA, markersize=3)

    ax.set_xticks(range(1, len(models)+1)); ax.set_xticklabels(models, rotation=20, ha='right')
    ax.set_ylabel(metric); ax.set_title(f"Per-Sample Distribution ‚Äî {metric}")
    plt.tight_layout()
    _savefig(fig, f"per_sample_{metric}_{'violin' if show_violin else 'box'}")

for m in ["accuracy","f1_weighted","auroc_macro_ovr","auprc_macro_ovr","ece_15bin","brier"]:
    plot_per_sample_distribution(per_sample_df, m, show_violin=SHOW_VIOLIN)

# ====== 3) Ë¶ÜÁõñÁéá vs ÂáÜÁ°ÆÁéá Êï£ÁÇπ ======
def plot_coverage_scatter(per_sample_df: pd.DataFrame):
    if per_sample_df is None or per_sample_df.empty or ("coverage" not in per_sample_df.columns):
        print("[Coverage] Êó† coverage Âàó„ÄÇ")
        return
    fig, ax = plt.subplots(figsize=(6.5,5))
    models = per_sample_df["model"].unique().tolist()
    for i, m in enumerate(models):
        d = per_sample_df.loc[per_sample_df["model"]==m]
        ax.plot(d["coverage"], d["accuracy"], 'o', alpha=0.6, label=m)
    ax.set_xlabel("Coverage (fraction of spots with silver labels)")
    ax.set_ylabel("Accuracy")
    ax.set_title("Coverage vs. Accuracy (per sample)")
    ax.legend()
    plt.tight_layout()
    _savefig(fig, "coverage_vs_accuracy")

plot_coverage_scatter(per_sample_df)

# ====== 4) ÂàÜÁªÑÊØîËæÉÔºàÂπ≥Âè∞/ËÇøÁò§Á±ªÂûã/‰øùÂ≠òÊñπÂºèÔºâ ======
def pick_top_groups(per_group_df: pd.DataFrame, by: str, top_k: int = TOP_K_GROUPS):
    if per_group_df is None or per_group_df.empty: return []
    sub = per_group_df[per_group_df["group_by"]==by].copy()
    if sub.empty: return []
    # ÊåâËØ•ÁªÑ‰∏ãÁöÑÊ†∑Êú¨Êï∞ÊéíÂ∫èÔºà‰∏çÂêåÊ®°Âûã‰ºöÈáçÂ§çÔºåÂèñÊÄªËÆ°Ôºâ
    size = sub.groupby(by)["n_samples"].sum().sort_values(ascending=False)
    return size.head(top_k).index.tolist()

def plot_group_bars(per_group_df: pd.DataFrame, by: str, metric: str, top_k: int = TOP_K_GROUPS):
    if per_group_df is None or per_group_df.empty: 
        print(f"[Group] Êó†ÂàÜÁªÑÊï∞ÊçÆ„ÄÇ")
        return
    if metric not in per_group_df.columns:
        print(f"[Group] ÊåáÊ†á {metric} Áº∫Â§±„ÄÇ")
        return
    sub = per_group_df[per_group_df["group_by"]==by].copy()
    if sub.empty:
        print(f"[Group] Êó† {by} Áª¥Â∫¶ÁöÑÊï∞ÊçÆ„ÄÇ")
        return
    groups = pick_top_groups(per_group_df, by, top_k)
    if not groups:
        print(f"[Group] Êú™ÊâæÂà∞ {by} ÁöÑÂâç {top_k} ÁªÑ„ÄÇ")
        return
    sub = sub[sub[by].isin(groups)]
    # ÊåâÁªÑÂÜÖÊ®°ÂûãÁîªÂàÜÁªÑÊü±Âõæ
    models = sub["model"].unique().tolist()
    g_order = groups
    width = 0.8 / max(len(models),1)
    fig, ax = plt.subplots(figsize=(min(12, 1.2*len(g_order)+3), 5))
    x = np.arange(len(g_order))
    for j, m in enumerate(models):
        y = [sub.loc[(sub[by]==g) & (sub["model"]==m), metric].mean() for g in g_order]
        ax.bar(x + j*width, y, width, label=m)
    ax.set_xticks(x + width*(len(models)-1)/2)
    ax.set_xticklabels([str(g) for g in g_order], rotation=20, ha='right')
    ax.set_ylabel(metric); ax.set_title(f"{by}: top-{len(g_order)} groups ‚Äî {metric}")
    ax.legend()
    plt.tight_layout()
    _savefig(fig, f"group_{by}_{metric}_bar_top{len(g_order)}")

for by in ["st_technology","oncotree_code","preservation_method"]:
    for metric in ["global_accuracy_weighted","macro_accuracy","macro_f1_weighted","macro_auroc_macro_ovr","macro_auprc_macro_ovr"]:
        if per_group_df is not None and metric in per_group_df.columns:
            plot_group_bars(per_group_df, by, metric, TOP_K_GROUPS)

# ====== 5) ÂàÜÁªÑÁÉ≠ÂõæÔºàÁªÑ √ó Ê®°ÂûãÔºâ ======
def plot_group_heatmap(per_group_df: pd.DataFrame, by: str, metric: str, top_k: int = TOP_K_GROUPS):
    if per_group_df is None or per_group_df.empty or metric not in per_group_df.columns:
        print(f"[Heatmap] Êó†Ê≥ïÁªòÂà∂ {by}-{metric}")
        return
    sub = per_group_df[per_group_df["group_by"]==by].copy()
    if sub.empty:
        print(f"[Heatmap] Êó† {by} Êï∞ÊçÆ„ÄÇ"); return
    groups = pick_top_groups(per_group_df, by, top_k)
    if not groups:
        print(f"[Heatmap] Êú™ÊâæÂà∞ {by} ÁöÑÂâç {top_k} ÁªÑ„ÄÇ"); return
    sub = sub[sub[by].isin(groups)]
    pivot = sub.pivot_table(index=by, columns="model", values=metric, aggfunc='mean')
    # Á∫Ø matplotlib ÁÉ≠Âõæ
    fig, ax = plt.subplots(figsize=(min(12, 0.6*pivot.shape[1]+4), min(10, 0.4*pivot.shape[0]+3)))
    im = ax.imshow(pivot.values, aspect='auto')
    ax.set_xticks(np.arange(pivot.shape[1])); ax.set_xticklabels(list(pivot.columns), rotation=20, ha='right')
    ax.set_yticks(np.arange(pivot.shape[0])); ax.set_yticklabels(list(pivot.index))
    ax.set_title(f"Heatmap ‚Äî {by} √ó model ({metric})")
    cbar = fig.colorbar(im, ax=ax); cbar.set_label(metric)
    plt.tight_layout()
    _savefig(fig, f"heatmap_{by}_{metric}_top{len(groups)}")

for by in ["st_technology","oncotree_code","preservation_method"]:
    for metric in ["global_accuracy_weighted","macro_accuracy","macro_f1_weighted","macro_auroc_macro_ovr","macro_auprc_macro_ovr"]:
        plot_group_heatmap(per_group_df, by, metric, TOP_K_GROUPS)

# ====== 6) McNemar ÂèØËßÜÂåñ ======
def plot_mcnemar_pooled(mcnemar_pooled_df: pd.DataFrame):
    if mcnemar_pooled_df is None or mcnemar_pooled_df.empty:
        print("[McNemar pooled] Êó†Êï∞ÊçÆ„ÄÇ"); return
    df = mcnemar_pooled_df.copy()
    df["pair"] = df["model_A"].astype(str) + " vs " + df["model_B"].astype(str)
    df["neglog10_p"] = -np.log10(df["p_value"].clip(lower=1e-300))
    # ‚àílog10(p) Êü±Áä∂Âõæ + ÊòæËëóÊÄßÈòàÂÄºÁ∫ø
    fig, ax = plt.subplots(figsize=(max(6, 1.2*len(df)), 4.8))
    x = np.arange(len(df))
    ax.bar(x, df["neglog10_p"].values)
    ax.axhline(-np.log10(0.05), linestyle='--')
    ax.set_xticks(x); ax.set_xticklabels(df["pair"], rotation=20, ha='right')
    ax.set_ylabel("-log10(p)"); ax.set_title("Pooled McNemar: significance")
    plt.tight_layout()
    _savefig(fig, "mcnemar_pooled_neglog10p")

    # ‰∏ç‰∏ÄËá¥ËÆ°Êï∞ (b vs c)
    fig, ax = plt.subplots(figsize=(max(6, 1.2*len(df)), 4.8))
    w = 0.4
    ax.bar(x - w/2, df["b_A_wrong_B_right"].values, width=w, label="A wrong, B right (b)")
    ax.bar(x + w/2, df["c_A_right_B_wrong"].values, width=w, label="A right, B wrong (c)")
    ax.set_xticks(x); ax.set_xticklabels(df["pair"], rotation=20, ha='right')
    ax.set_ylabel("count"); ax.set_title("Pooled McNemar: discordant counts")
    ax.legend()
    plt.tight_layout()
    _savefig(fig, "mcnemar_pooled_discordant_counts")

plot_mcnemar_pooled(mcnemar_pooled_df)

def plot_mcnemar_top_per_sample(mcnemar_per_sample_df: pd.DataFrame, topN: int = 20):
    if mcnemar_per_sample_df is None or mcnemar_per_sample_df.empty:
        print("[McNemar per-sample] Êó†Êï∞ÊçÆ„ÄÇ"); return
    df = mcnemar_per_sample_df.copy()
    df = df.sort_values("p_value", ascending=True).head(topN).copy()
    df["pair"] = df["model_A"].astype(str) + " vs " + df["model_B"].astype(str)
    df["label"] = df["sample_id"].astype(str) + " | " + df["pair"]
    df["neglog10_p"] = -np.log10(df["p_value"].clip(lower=1e-300))
    fig, ax = plt.subplots(figsize=(8, max(5, 0.35*len(df))))
    y = np.arange(len(df))
    ax.barh(y, df["neglog10_p"].values)
    ax.set_yticks(y); ax.set_yticklabels(df["label"])
    ax.axvline(-np.log10(0.05), linestyle='--')
    ax.set_xlabel("-log10(p)"); ax.set_title(f"Top-{topN} most significant McNemar pairs (per-sample)")
    plt.tight_layout()
    _savefig(fig, f"mcnemar_per_sample_top{topN}")

plot_mcnemar_top_per_sample(mcnemar_per_sample_df, topN=20)

# ====== 7) ‚ÄúÊúÄ‰Ω≥/ÊúÄÂ∑Æ‚ÄùÊ†∑Êú¨ÊéíË°åÊ¶úÔºàÊåâÂáÜÁ°ÆÁéáÔºâ ======
def topk_samples(per_sample_df: pd.DataFrame, metric="accuracy", k=10, largest=True):
    if per_sample_df is None or per_sample_df.empty or metric not in per_sample_df.columns:
        print("[TopK] Êó†Êï∞ÊçÆÊàñÂàóÁº∫Â§±„ÄÇ"); return
    df = per_sample_df[["sample_id","model",metric,"coverage","st_technology","oncotree_code","preservation_method"]].copy()
    df = df.sort_values(metric, ascending=not largest).groupby("model").head(k)
    print(f"Top-{k} samples by {metric} ({'largest' if largest else 'smallest'})")
    display(df)

topk_samples(per_sample_df, "accuracy", k=10, largest=True)   # ÊúÄ‰Ω≥
topk_samples(per_sample_df, "accuracy", k=10, largest=False)  # ÊúÄÂ∑Æ

print("\n‚úÖ ÂèØËßÜÂåñÂ∑≤ÂÆåÊàê„ÄÇÊâÄÊúâÂõæÁâá‰øùÂ≠òÂú®Ôºö", FIG_DIR)


# ===== Cell 20: Code =====



===== notebooks/d2_true_train_val_split.ipynb =====
# ===== Cell 1: Code (execution_count: 1) =====
import logging
import sys
from pathlib import Path
from typing import List, Set

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# --- Ê†∏ÂøÉÈÖçÁΩÆ ---
# üõ°Ô∏è ÂÆâÂÖ®Á¨¨‰∏Ä: ËÆæ‰∏∫ False ‰ª•ÂÆûÈôÖÊâßË°åÊñá‰ª∂ÂÜôÂÖ•
DRYRUN = False

# --- Ë∑ØÂæÑÈÖçÁΩÆ ---
# ËæìÂÖ•: ÂåÖÂê´È¢ÑÂ§ÑÁêÜÂêéÊï∞ÊçÆÁöÑÁõÆÂΩï
INPUT_ARTIFACTS_DIR = Path("/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts")
# ËæìÂá∫: Â∞ÜÂ≠òÊîæ train/ Âíå val/ Â≠êÁõÆÂΩïÁöÑÂü∫ÂáÜÁõÆÂΩï
OUTPUT_SPLIT_DIR = Path("/cwStorage/nodecw_group/jijh/yuanspace_data/dataset_split")

# ÂÆö‰πâÂéüÂßãÊ∏ÖÂçïÊñá‰ª∂Ë∑ØÂæÑÔºåÁî®‰∫éËé∑Âèñ sample_id
ORIGINAL_MANIFEST_FILE = Path("/cwStorage/nodecw_group/jijh/hest_1k/HEST_v1_1_0.csv") 

# Ê£ÄÊü•ËæìÂÖ•Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
assert INPUT_ARTIFACTS_DIR.exists(), f"ËæìÂÖ•ÁõÆÂΩï‰∏çÂ≠òÂú®: {INPUT_ARTIFACTS_DIR}"
assert ORIGINAL_MANIFEST_FILE.exists(), f"ÂéüÂßãÊ∏ÖÂçïÊñá‰ª∂‰∏çÂ≠òÂú®: {ORIGINAL_MANIFEST_FILE}"
for filename in ["nodes.parquet", "edges.parquet", "image_embeds.npy", "text_embeds.npy"]:
    assert (INPUT_ARTIFACTS_DIR / filename).exists(), f"ËæìÂÖ•Êñá‰ª∂Áº∫Â§±: {filename}"

# --- Êó•ÂøóÈÖçÁΩÆ ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

# ===== Cell 2: Code (execution_count: 2) =====
def get_validation_sample_ids(manifest_path: Path) -> Set[str]:
    """
    Ê†πÊçÆÈ¢ÑÂÆö‰πâÁöÑÁ†îÁ©∂È°πÁõÆÊ†áÈ¢òÔºå‰ªéÂéüÂßãÊ∏ÖÂçï‰∏≠Ëé∑ÂèñÈ™åËØÅÈõÜÁöÑ sample_id ÂàóË°®„ÄÇ
    """
    df_manifest = pd.read_csv(manifest_path)
    
    VALIDATION_TITLES = [
        "Spatial deconvolution of HER2-positive breast cancer delineates tumor-associated cell type interactions",
        "Characterization of immune cell populations in the tumor microenvironment of colorectal cancer using high definition spatial profiling"
    ]
    
    validation_ids = df_manifest[df_manifest['dataset_title'].isin(VALIDATION_TITLES)]['id'].unique()
    
    logging.info(f"‰ªéÊ∏ÖÂçï‰∏≠Á°ÆÂÆö‰∫Ü {len(validation_ids)} ‰∏™Áî®‰∫éÈ™åËØÅÁöÑÁã¨Á´ãÊ†∑Êú¨ ID„ÄÇ")
    return set(validation_ids)

def split_and_save_dataset(
    input_dir: Path, 
    output_dir: Path, 
    validation_sample_ids: Set[str]
):
    """
    Âä†ËΩΩÈ¢ÑÂ§ÑÁêÜÁöÑÊï∞ÊçÆÔºåÊåâ sample_id ÊãÜÂàÜ‰∏∫ËÆ≠ÁªÉ/È™åËØÅÈõÜÔºåÂπ∂‰øùÂ≠òÂà∞Êñ∞ÁõÆÂΩï„ÄÇ
    """
    logging.info(f"ÂºÄÂßã‰ªé '{input_dir}' Âä†ËΩΩÊï∞ÊçÆ...")
    nodes_df = pd.read_parquet(input_dir / "nodes.parquet")
    edges_df = pd.read_parquet(input_dir / "edges.parquet")
    all_img_embeds = np.load(input_dir / "image_embeds.npy")
    all_txt_embeds = np.load(input_dir / "text_embeds.npy")
    logging.info("ÊâÄÊúâËæìÂÖ•Êñá‰ª∂Âä†ËΩΩÂÆåÊØï„ÄÇ")

    # 1. ÊãÜÂàÜ Nodes Âíå Embeddings
    val_nodes_mask = nodes_df['sample_id'].isin(validation_sample_ids)
    
    train_nodes_df = nodes_df[~val_nodes_mask].copy()
    val_nodes_df = nodes_df[val_nodes_mask].copy()
    
    # ‰ΩøÁî®ÂéüÂßãÁ¥¢ÂºïÊù•ÂàáÁâá numpy Êï∞ÁªÑ
    train_indices = train_nodes_df.index.to_numpy()
    val_indices = val_nodes_df.index.to_numpy()
    
    train_img_embeds = all_img_embeds[train_indices]
    val_img_embeds = all_img_embeds[val_indices]
    
    train_txt_embeds = all_txt_embeds[train_indices]
    val_txt_embeds = all_txt_embeds[val_indices]
    
    # 2. ÊãÜÂàÜ Edges
    train_tile_ids = set(train_nodes_df['tile_id'])
    val_tile_ids = set(val_nodes_df['tile_id'])
    
    train_edges_mask = edges_df['src_tile_id'].isin(train_tile_ids) & edges_df['nbr_tile_id'].isin(train_tile_ids)
    val_edges_mask = edges_df['src_tile_id'].isin(val_tile_ids) & edges_df['nbr_tile_id'].isin(val_tile_ids)
    
    train_edges_df = edges_df[train_edges_mask].copy()
    val_edges_df = edges_df[val_edges_mask].copy()

    # 3. Êä•ÂëäÁªüËÆ°Êï∞ÊçÆ
    logging.info("--- ÊãÜÂàÜÁªüËÆ° ---")
    logging.info(f"ËÆ≠ÁªÉÈõÜ: {len(train_nodes_df)} tiles, {len(train_edges_df)} edges")
    logging.info(f"È™åËØÅÈõÜ: {len(val_nodes_df)} tiles, {len(val_edges_df)} edges")
    logging.info("--------------------")

    # 4. ÂÆâÂÖ®ÂÜôÂÖ•Êñá‰ª∂
    if not DRYRUN:
        train_dir = output_dir / "train"
        val_dir = output_dir / "val"
        train_dir.mkdir(parents=True, exist_ok=True)
        val_dir.mkdir(parents=True, exist_ok=True)
        
        logging.info(f"Ê≠£Âú®Âêë '{train_dir}' ÂÜôÂÖ•ËÆ≠ÁªÉÈõÜÊñá‰ª∂...")
        pq.write_table(pa.Table.from_pandas(train_nodes_df, preserve_index=False), train_dir / "nodes.parquet")
        pq.write_table(pa.Table.from_pandas(train_edges_df, preserve_index=False), train_dir / "edges.parquet")
        np.save(train_dir / "image_embeds.npy", train_img_embeds)
        np.save(train_dir / "text_embeds.npy", train_txt_embeds)
        
        logging.info(f"Ê≠£Âú®Âêë '{val_dir}' ÂÜôÂÖ•È™åËØÅÈõÜÊñá‰ª∂...")
        pq.write_table(pa.Table.from_pandas(val_nodes_df, preserve_index=False), val_dir / "nodes.parquet")
        pq.write_table(pa.Table.from_pandas(val_edges_df, preserve_index=False), val_dir / "edges.parquet")
        np.save(val_dir / "image_embeds.npy", val_img_embeds)
        np.save(val_dir / "text_embeds.npy", val_txt_embeds)
        
        logging.info("ÊâÄÊúâÊñá‰ª∂Â∑≤ÊàêÂäüÂÜôÂÖ•Á£ÅÁõò„ÄÇ")
    else:
        logging.warning("ÂΩìÂâç‰∏∫È¢ÑÊºîÊ®°Âºè (DRYRUN=True)ÔºåÊú™ÊâßË°å‰ªª‰ΩïÂÜôÁõòÊìç‰Ωú„ÄÇ")

# ===== Cell 3: Code (execution_count: 3) =====
def validate_split(output_dir: Path):
    """
    Âä†ËΩΩÊãÜÂàÜÂêéÁöÑÊï∞ÊçÆÂπ∂ÊâßË°å‰∏ÄÁ≥ªÂàóÊ£ÄÊü•‰ª•Á°Æ‰øùÂÖ∂ÂÆåÊï¥ÊÄßÂíåÊ≠£Á°ÆÊÄß„ÄÇ
    """
    logging.info("--- ÂºÄÂßãÈ™åËØÅÊãÜÂàÜÂêéÁöÑÊï∞ÊçÆÈõÜ ---")
    train_dir = output_dir / "train"
    val_dir = output_dir / "val"
    
    if not train_dir.exists() or not val_dir.exists():
        logging.error("È™åËØÅÂ§±Ë¥•: ËæìÂá∫ÁõÆÂΩï train/ Êàñ val/ ‰∏çÂ≠òÂú®„ÄÇ")
        return

    # Âä†ËΩΩÊï∞ÊçÆ
    train_nodes = pd.read_parquet(train_dir / "nodes.parquet")
    val_nodes = pd.read_parquet(val_dir / "nodes.parquet")
    train_edges = pd.read_parquet(train_dir / "edges.parquet")
    val_edges = pd.read_parquet(val_dir / "edges.parquet")
    train_img_embeds = np.load(train_dir / "image_embeds.npy")
    val_img_embeds = np.load(val_dir / "image_embeds.npy")

    # Ê£ÄÊü• 1: Êï∞ÊçÆÊ≥ÑÈú≤ (ÊúÄÈáçË¶ÅÔºÅ)
    train_samples = set(train_nodes['sample_id'].unique())
    val_samples = set(val_nodes['sample_id'].unique())
    common_samples = train_samples.intersection(val_samples)
    
    assert not common_samples, f"È™åËØÅÂ§±Ë¥•ÔºöÂèëÁé∞Êï∞ÊçÆÊ≥ÑÈú≤ÔºÅ‰ª•‰∏ã sample_id ÂêåÊó∂Â≠òÂú®‰∫éËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜ: {common_samples}"
    logging.info("‚úÖ PASSED: ËÆ≠ÁªÉÈõÜ‰∏éÈ™åËØÅÈõÜ‰πãÈó¥Êó†Ê†∑Êú¨ID‰∫§ÈõÜ (Êó†Êï∞ÊçÆÊ≥ÑÈú≤)„ÄÇ")
    
    # Ê£ÄÊü• 2: ÂÜÖÈÉ®‰∏ÄËá¥ÊÄß
    assert len(train_nodes) == train_img_embeds.shape[0], "ËÆ≠ÁªÉÈõÜËäÇÁÇπÊï∞‰∏éÂµåÂÖ•Êï∞‰∏çÂåπÈÖç„ÄÇ"
    assert len(val_nodes) == val_img_embeds.shape[0], "È™åËØÅÈõÜËäÇÁÇπÊï∞‰∏éÂµåÂÖ•Êï∞‰∏çÂåπÈÖç„ÄÇ"
    logging.info("‚úÖ PASSED: ËäÇÁÇπÊï∞‰∏éÂµåÂÖ•Êï∞Âú®ÂêÑÂ≠êÈõÜ‰∏≠‰øùÊåÅ‰∏ÄËá¥„ÄÇ")

    # Ê£ÄÊü• 3: ËæπÂºïÁî®ÂÆåÊï¥ÊÄß
    train_tile_ids = set(train_nodes['tile_id'])
    val_tile_ids = set(val_nodes['tile_id'])
    assert train_edges['src_tile_id'].isin(train_tile_ids).all(), "ËÆ≠ÁªÉÈõÜËæπË°®ÂåÖÂê´Êó†ÊïàÁöÑÊ∫êËäÇÁÇπID„ÄÇ"
    assert train_edges['nbr_tile_id'].isin(train_tile_ids).all(), "ËÆ≠ÁªÉÈõÜËæπË°®ÂåÖÂê´Êó†ÊïàÁöÑÈÇªÂ±ÖËäÇÁÇπID„ÄÇ"
    assert val_edges['src_tile_id'].isin(val_tile_ids).all(), "È™åËØÅÈõÜËæπË°®ÂåÖÂê´Êó†ÊïàÁöÑÊ∫êËäÇÁÇπID„ÄÇ"
    assert val_edges['nbr_tile_id'].isin(val_tile_ids).all(), "È™åËØÅÈõÜËæπË°®ÂåÖÂê´Êó†ÊïàÁöÑÈÇªÂ±ÖËäÇÁÇπID„ÄÇ"
    logging.info("‚úÖ PASSED: ËæπË°®ÁöÑÂºïÁî®ÂÆåÊï¥ÊÄßÂú®ÂêÑÂ≠êÈõÜ‰∏≠ÂæóÂà∞‰øùÊåÅ„ÄÇ")
    
    logging.info("üéâ ÂÖ®ÈÉ®È™åËØÅÊàêÂäüÔºÅÊï∞ÊçÆÈõÜÂ∑≤ÂáÜÂ§áÂ∞±Áª™„ÄÇ")

# ===== Cell 4: Code (execution_count: 4) =====
# --- ‰∏ªÊµÅÁ®ã ---
# 1. ‰ªéÂéüÂßãÊ∏ÖÂçï‰∏≠Ëé∑ÂèñÊùÉÂ®ÅÁöÑÈ™åËØÅÈõÜ sample_id ÂàóË°®
validation_sample_ids = get_validation_sample_ids(ORIGINAL_MANIFEST_FILE)



# ===== Cell 5: Code (execution_count: 5) =====
validation_sample_ids

# ===== Cell 6: Code (execution_count: 6) =====
# 2. ÊâßË°åÊãÜÂàÜÂíå‰øùÂ≠ò
split_and_save_dataset(
    input_dir=INPUT_ARTIFACTS_DIR,
    output_dir=OUTPUT_SPLIT_DIR,
    validation_sample_ids=validation_sample_ids
)

# 3. ËøêË°åÈ™åËØÅËÑöÊú¨
if not DRYRUN:
    validate_split(OUTPUT_SPLIT_DIR)

# ===== Cell 7: Code =====



===== notebooks/d4_dataset_debug.ipynb =====
# ===== Cell 1: Markdown =====
# Debug

# ===== Cell 2: Code =====
# ===== Cell: Deep Dive Diagnostic for Sample NCBI525 =====
# CodeGuardian: This cell is a high-precision diagnostic tool to uncover why
# sample NCBI525 has missing data. We will trace the data from its source
# to its final state, pinpointing the exact point of failure.

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import anndata
import logging
import os
import rootutils

# --- 1. ÈÖçÁΩÆ (‰∏éÊÇ®ÁöÑÁéØÂ¢É‰øùÊåÅ‰∏ÄËá¥) ---
# Use rootutils to find the project root based on a marker file (e.g., .git)
try:
    PROJECT_ROOT = rootutils.find_root(search_from=__file__, indicator=".git")
except NameError:
    # Fallback for interactive environments like Jupyter
    PROJECT_ROOT = rootutils.find_root(search_from=".", indicator=".git")

if not PROJECT_ROOT.exists():
    raise FileNotFoundError(f"È°πÁõÆÊ†πÁõÆÂΩï‰∏çÂ≠òÂú®: {PROJECT_ROOT}")

if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

# a. ÂéüÂßã HEST Êï∞ÊçÆÈõÜÁõÆÂΩï (Áî®‰∫éÂä†ËΩΩ .h5ad)
RAW_DATA_DIR = PROJECT_ROOT / "data" / "hest_1k_original"
# b. d1 notebook ÁîüÊàêÁöÑÂ∑≤Â§ÑÁêÜÊï∞ÊçÆÁõÆÂΩï
# Âä®ÊÄÅÊü•ÊâæÂåÖÂê´ nodes.parquet ÁöÑÁõÆÂΩïÔºåÈÅøÂÖçÁ°¨ÁºñÁ†ÅË∑ØÂæÑ
def find_processed_data_dir(base_path: Path) -> Path | None:
    for p in base_path.iterdir():
        if p.is_dir() and (p / "nodes.parquet").exists():
            return p
    return None

PROCESSED_DATA_DIR = find_processed_data_dir(PROJECT_ROOT / "data")
if PROCESSED_DATA_DIR is None:
    raise FileNotFoundError(f"Âú® '{PROJECT_ROOT / 'data'}' ÁõÆÂΩï‰∏ãÊú™ÊâæÂà∞‰ªª‰ΩïÂåÖÂê´ 'nodes.parquet' ÁöÑÂ∑≤Â§ÑÁêÜÊï∞ÊçÆÊñá‰ª∂Â§π„ÄÇ")

# c. ÁõÆÊ†áÊ†∑Êú¨
TARGET_SAMPLE_ID = "NCBI525"

logger = logging.getLogger("ncbi525_diagnostic")
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(handler)
logger.setLevel(logging.INFO)
logger.propagate = False

# --- 2. ÈáèÂåñÈóÆÈ¢òÔºöÂØπÊØî AnnData ‰∏é Parquet ‰∏≠ÁöÑ Spot Êï∞Èáè ---
logger.info(f"--- 1. ÈáèÂåñÈóÆÈ¢ò: ÂØπÊØî '{TARGET_SAMPLE_ID}' ÁöÑ Spot Êï∞Èáè ---")
adata_path = RAW_DATA_DIR / "st" / f"{TARGET_SAMPLE_ID}.h5ad"
if not adata_path.exists():
    raise FileNotFoundError(f"Êú™ÊâæÂà∞ AnnData Êñá‰ª∂: {adata_path}")

nodes_path = PROCESSED_DATA_DIR / "nodes.parquet"
if not nodes_path.exists():
    raise FileNotFoundError(f"Êú™ÊâæÂà∞Â§ÑÁêÜÂêéÁöÑËäÇÁÇπÊñá‰ª∂: {nodes_path}")

try:
    adata = anndata.read_h5ad(adata_path)
except Exception as exc:
    logger.error("‚ùå Âú®ËØªÂèñ AnnData Êñá‰ª∂Êó∂ÂèëÁîüÈîôËØØ", exc_info=True)
    raise

adata_spot_ids = set(adata.obs.index)
num_adata_spots = len(adata_spot_ids)
logger.info(f"ÂéüÂßã AnnData (.h5ad) ‰∏≠ '{TARGET_SAMPLE_ID}' ÁöÑ Spot ÊÄªÊï∞: {num_adata_spots}")

try:
    nodes_df = pd.read_parquet(nodes_path)
except Exception:
    logger.error("‚ùå Âú®ËØªÂèñ nodes.parquet Êñá‰ª∂Êó∂ÂèëÁîüÈîôËØØ", exc_info=True)
    raise

processed_nodes_for_sample = nodes_df[nodes_df["sample_id"] == TARGET_SAMPLE_ID].copy()
if processed_nodes_for_sample.empty:
    logger.warning("‚ö†Ô∏è Âú®Â§ÑÁêÜÊï∞ÊçÆÈõÜ‰∏≠Êú™ÊâæÂà∞ÁõÆÊ†áÊ†∑Êú¨„ÄÇÂêéÁª≠Ê£ÄÊü•Â∞ÜË¢´Ë∑≥Ëøá„ÄÇ")
else:
    num_processed_spots = len(processed_nodes_for_sample)
    logger.info(f"Â∑≤Â§ÑÁêÜ Parquet ‰∏≠ '{TARGET_SAMPLE_ID}' ÁöÑ Spot Êï∞Èáè: {num_processed_spots}")

    discrepancy = num_adata_spots - num_processed_spots
    loss_percentage = (discrepancy / num_adata_spots) * 100 if num_adata_spots > 0 else 0.0
    logger.info(f"Êï∞ÈáèÂ∑ÆÂºÇ: {discrepancy} ‰∏™ spots Ë¢´‰∏¢ÂºÉ (ÊçüÂ§±Áéá: {loss_percentage:.2f}%)")

    if discrepancy == 0:
        logger.info("‚úÖ Êï∞Èáè‰∏ÄËá¥ÔºåÈóÆÈ¢òÂèØËÉΩ‰∏çÂú®‰∫éÊï∞ÊçÆ‰∏¢Â§±„ÄÇ")
    else:
        logger.warning("üî• Êï∞Èáè‰∏•Èáç‰∏çÂåπÈÖçÔºÅËøôÊòØÈóÆÈ¢òÁöÑÊ†∏ÂøÉ„ÄÇ")

# --- 3. ÂÆö‰Ωç‚ÄúÂ§±Ë∏™‚Äù‰∏é‚ÄúÂπ∏Â≠ò‚ÄùÁöÑ Spot ---
if not processed_nodes_for_sample.empty:
    logger.info("\n--- 2. ÂÆö‰ΩçÂ§±Ë∏™‰∏éÂπ∏Â≠òÁöÑ Spot ---")

    def parse_spot_id_from_path_v2(path_str: str | Path) -> str | None:
        try:
            return Path(path_str).stem
        except Exception:
            return None

    valid_image_paths = processed_nodes_for_sample["image_path"].dropna().tolist()
    processed_spot_ids_from_path = {
        spot_id
        for spot_id in (parse_spot_id_from_path_v2(path) for path in valid_image_paths)
        if spot_id
    }

    missing_spot_ids = adata_spot_ids - processed_spot_ids_from_path
    logger.info(f"ÊâæÂà∞‰∫Ü {len(missing_spot_ids)} ‰∏™ 'Â§±Ë∏™' ÁöÑ Spot„ÄÇÁ§∫‰æã: {list(missing_spot_ids)[:5]}")

    # --- 4. ÂÆ°ÈóÆ‚ÄúÂ§±Ë∏™ËÄÖ‚ÄùÔºöÈ™åËØÅÂÆÉ‰ª¨ÂØπÂ∫îÁöÑÁì¶ÁâáÊñá‰ª∂ÊòØÂê¶ÊúâÊïà ---
    logger.info("\n--- 3. Ê†πÊú¨ÂéüÂõ†ÂàÜÊûê: Ê£ÄÊü• 'Â§±Ë∏™' Spot ÂØπÂ∫îÁöÑÁì¶ÁâáÊñá‰ª∂ ---")
    
    # ‰ªéÂπ∏Â≠òÁöÑ spot Êé®Êñ≠Áì¶ÁâáÁõÆÂΩïÂíåÊñá‰ª∂Êâ©Â±ïÂêç
    tile_dir = None
    tile_ext = None
    if valid_image_paths:
        first_path = Path(valid_image_paths[0])
        tile_dir = first_path.parent
        tile_ext = first_path.suffix
        logger.info(f"‰ªéÂπ∏Â≠òÁöÑ spot Êé®Êñ≠Âá∫Áì¶ÁâáÁõÆÂΩï: {tile_dir}")
        logger.info(f"‰ªéÂπ∏Â≠òÁöÑ spot Êé®Êñ≠Âá∫Êñá‰ª∂Êâ©Â±ïÂêç: {tile_ext}")

    if not tile_dir or not tile_ext:
        logger.warning("‚ö†Ô∏è Êó†Ê≥ï‰ªé 'nodes.parquet' Êé®Êñ≠Áì¶ÁâáÁõÆÂΩïÊàñÊâ©Â±ïÂêçÔºåË∑≥ËøáÊñá‰ª∂Ê£ÄÊü•„ÄÇ")
    else:
        logger.info(f"ÂØπ {min(5, len(missing_spot_ids))} ‰∏™ 'Â§±Ë∏™' Spot ÁöÑÊñá‰ª∂ËøõË°åÊ£ÄÊü•...")
        num_missing_files = 0
        num_empty_files = 0

        for spot_id in list(missing_spot_ids)[:5]:
            # Âü∫‰∫éÊé®Êñ≠ÁöÑÁõÆÂΩïÂíåÊâ©Â±ïÂêçÊûÑÂª∫Áì¶ÁâáÊñá‰ª∂ÁöÑÂÆåÊï¥Ë∑ØÂæÑ
            path_to_check = tile_dir / f"{spot_id}{tile_ext}"
            
            if not path_to_check.is_absolute():
                path_to_check = (PROJECT_ROOT / path_to_check).resolve()
            
            log_msg = f"  - Spot '{spot_id}', Ë∑ØÂæÑ: {path_to_check}"
            if not path_to_check.exists():
                log_msg += " -> ‚ùå Êñá‰ª∂‰∏çÂ≠òÂú®"
                num_missing_files += 1
            elif path_to_check.stat().st_size == 0:
                log_msg += " -> ‚ùå Êñá‰ª∂Â§ßÂ∞è‰∏∫ 0"
                num_empty_files += 1
            else:
                log_msg += f" -> ‚úÖ Êñá‰ª∂Â≠òÂú®‰∏îÂ§ßÂ∞è‰∏∫ {path_to_check.stat().st_size} bytes"
            logger.info(log_msg)

        logger.info("\n--- ËØäÊñ≠ÁªìËÆ∫ ---")
        if num_missing_files > 0:
            logger.error("üî• ‰∏ªË¶ÅÈóÆÈ¢ò: ËÆ∏Â§öÁì¶ÁâáÊñá‰ª∂‰ªéÊú™Ë¢´ÁîüÊàêÊàñË∑ØÂæÑÈîôËØØÔºåÂØºËá¥ÂÆÉ‰ª¨Âú®È¢ÑÂ§ÑÁêÜÁöÑÁ¨¨‰∏ÄÊ≠•Â∞±Ë¢´ËøáÊª§Êéâ‰∫Ü„ÄÇ")
        if num_empty_files > 0:
            logger.error("üî• ‰∏ªË¶ÅÈóÆÈ¢ò: ËÆ∏Â§öÁì¶ÁâáÊñá‰ª∂Ë¢´ÁîüÊàê‰∏∫Á©∫Êñá‰ª∂ (0KB)ÔºåÂêåÊ†∑ÂØºËá¥ÂÆÉ‰ª¨Ë¢´ËøáÊª§„ÄÇ")
        if num_missing_files == 0 and num_empty_files == 0:
            logger.info("‚úÖ 'Â§±Ë∏™' spot ÂØπÂ∫îÁöÑÁì¶ÁâáÊñá‰ª∂‰ºº‰πéÂ≠òÂú®‰∏îÊúâÊïà„ÄÇÈóÆÈ¢òÂèØËÉΩÂú®È¢ÑÂ§ÑÁêÜÁöÑÁ≠õÈÄâÈÄªËæë‰∏≠ÔºåËÄå‰∏çÊòØÊñá‰ª∂‰∏¢Â§±„ÄÇ")

        if num_missing_files > 0 or num_empty_files > 0:
            print("\n" + "=" * 80)
            print("üí° Ë°åÂä®Âª∫ËÆÆ:")
            print("1. ËØ∑ÂõûÂà∞ÊÇ®ÊúÄÂàùÁöÑÂàáÁâáËÑöÊú¨ (ÂèØËÉΩÂú® `d1_...ipynb` ‰πãÂâçÊàñ‰πã‰∏≠)„ÄÇ")
            print(f"2. Ê£ÄÊü•Â§ÑÁêÜÊ†∑Êú¨ '{TARGET_SAMPLE_ID}' Êó∂ÊòØÂê¶Êúâ‰ªª‰ΩïÈîôËØØÊó•Âøó„ÄÇ")
            print("3. ÂèØËÉΩÁöÑÂéüÂõ†ÂåÖÊã¨Ôºö")
            print("   - WSI Êñá‰ª∂Êú¨Ë∫´Â≠òÂú®ÈóÆÈ¢ò (‰æãÂ¶ÇÔºåÈÉ®ÂàÜÂå∫ÂüüÊçüÂùè)„ÄÇ")
            print("   - ÂÜôÂÖ•Áì¶ÁâáÊñá‰ª∂Êó∂ÈÅáÂà∞‰∫ÜÁ£ÅÁõòÁ©∫Èó¥‰∏çË∂≥ÊàñÊùÉÈôêÈóÆÈ¢ò„ÄÇ")
            print("   - ËÑöÊú¨‰∏≠ÁöÑÂùêÊ†áËΩ¨Êç¢ÈÄªËæëÂØπ‰∫éËøô‰∏™ÁâπÂÆöÊ†∑Êú¨ÂèØËÉΩÂ≠òÂú® bug„ÄÇ")
            print(f"4. Ëß£ÂÜ≥ÊñπÊ°à: ÈúÄË¶ÅÈáçÊñ∞ËøêË°åÂØπ `{TARGET_SAMPLE_ID}` Ê†∑Êú¨ÁöÑÁì¶ÁâáÂàáÂâ≤ËøáÁ®ãÔºåÂπ∂ÂØÜÂàáÁõëÊéßÊó•Âøó‰ª•Á°Æ‰øùÊâÄÊúâÁì¶ÁâáÈÉΩË¢´Ê≠£Á°ÆÁîüÊàê„ÄÇ")
            print("=" * 80)


# ===== Cell 3: Markdown =====
## ÈöèÊú∫ÊäΩÂèñÂçÅ‰∏™Ê†∑Êú¨ËøõË°åÁõëÊµã

# ===== Cell 4: Code =====
# ===== Cell: Widespread Data Loss Diagnostic (10 Random Samples) =====
# CodeGuardian: This cell automates the diagnostic process from the previous cell
# for a random subset of samples to determine if the data loss issue is systemic.

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import anndata
import logging
import os
import random
import rootutils
from tqdm.auto import tqdm

# --- 1. ÈÖçÁΩÆ (‰∏éÊÇ®ÁöÑÁéØÂ¢É‰øùÊåÅ‰∏ÄËá¥) ---
# Use rootutils to find the project root based on a marker file (e.g., .project-root)
try:
    PROJECT_ROOT = rootutils.find_root(search_from=__file__, indicator=".project-root")
except NameError:
    # Fallback for interactive environments like Jupyter
    PROJECT_ROOT = rootutils.find_root(search_from=".", indicator=".project-root")

if not PROJECT_ROOT.exists():
    raise FileNotFoundError(f"È°πÁõÆÊ†πÁõÆÂΩï‰∏çÂ≠òÂú®: {PROJECT_ROOT}")

# a. ÂéüÂßã HEST Êï∞ÊçÆÈõÜÁõÆÂΩï (Áî®‰∫éÂä†ËΩΩ .h5ad)
RAW_DATA_DIR = PROJECT_ROOT / "data" / "hest_1k_original"
# b. d1 notebook ÁîüÊàêÁöÑÂ∑≤Â§ÑÁêÜÊï∞ÊçÆÁõÆÂΩï
# Âä®ÊÄÅÊü•ÊâæÂåÖÂê´ nodes.parquet ÁöÑÁõÆÂΩïÔºåÈÅøÂÖçÁ°¨ÁºñÁ†ÅË∑ØÂæÑ
def find_processed_data_dir(base_path: Path) -> Path | None:
    # ÂÅáËÆæÂ∑≤Â§ÑÁêÜÊï∞ÊçÆÂú® hest_hugo_6nei_correct_parquet_data/train ÁõÆÂΩï‰∏ã
    candidate_path = base_path / "hest_hugo_6nei_correct_parquet_data" / "train"
    if candidate_path.exists() and (candidate_path / "nodes.parquet").exists():
        return candidate_path
    
    # Â¶ÇÊûúÊâæ‰∏çÂà∞ÔºåÂÜçËøõË°åÈÄöÁî®ÊêúÁ¥¢
    for p in base_path.iterdir():
        if p.is_dir() and (p / "nodes.parquet").exists():
            return p
    return None

PROCESSED_DATA_DIR = find_processed_data_dir(PROJECT_ROOT / "data")
if PROCESSED_DATA_DIR is None:
    raise FileNotFoundError(f"Âú® '{PROJECT_ROOT / 'data'}' ÁõÆÂΩï‰∏ãÊú™ÊâæÂà∞‰ªª‰ΩïÂåÖÂê´ 'nodes.parquet' ÁöÑÂ∑≤Â§ÑÁêÜÊï∞ÊçÆÊñá‰ª∂Â§π„ÄÇ")

# --- 2. Â∞ÅË£ÖËØäÊñ≠ÈÄªËæë‰∏∫‰∏Ä‰∏™ÂáΩÊï∞ ---
logger = logging.getLogger("widespread_diagnostic")
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(handler)
logger.setLevel(logging.INFO)
logger.propagate = False

def diagnose_sample_data_loss(sample_id: str, raw_data_dir: Path, processed_nodes_df: pd.DataFrame) -> dict:
    """
    Performs a deep-dive diagnostic for a single sample to check for data loss.
    """
    result = {
        "sample_id": sample_id,
        "adata_spots": 0,
        "processed_spots": 0,
        "loss_percentage": 0.0,
        "diagnosis": "N/A",
        "details": ""
    }
    
    # 1. Âä†ËΩΩ AnnData
    adata_path = raw_data_dir / "st" / f"{sample_id}.h5ad"
    if not adata_path.exists():
        result["diagnosis"] = "Error"
        result["details"] = f"AnnData file not found at {adata_path}"
        logger.error(f"[{sample_id}] ‚ùå {result['details']}")
        return result
    
    try:
        adata = anndata.read_h5ad(adata_path)
        adata_spot_ids = set(adata.obs.index)
        result["adata_spots"] = len(adata_spot_ids)
    except Exception as e:
        result["diagnosis"] = "Error"
        result["details"] = f"Failed to read AnnData file: {e}"
        logger.error(f"[{sample_id}] ‚ùå {result['details']}")
        return result

    # 2. ‰ªéÂ∑≤Â§ÑÁêÜÊï∞ÊçÆ‰∏≠Á≠õÈÄâ
    sample_nodes_df = processed_nodes_df[processed_nodes_df["sample_id"] == sample_id]
    result["processed_spots"] = len(sample_nodes_df)

    if result["adata_spots"] == 0:
        result["diagnosis"] = "Warning"
        result["details"] = "Source AnnData has 0 spots."
        logger.warning(f"[{sample_id}] ‚ö†Ô∏è {result['details']}")
        return result
        
    # 3. ËÆ°ÁÆóÂ∑ÆÂºÇ
    discrepancy = result["adata_spots"] - result["processed_spots"]
    result["loss_percentage"] = (discrepancy / result["adata_spots"]) * 100
    
    if discrepancy == 0:
        result["diagnosis"] = "OK"
        result["details"] = "All spots accounted for."
        logger.info(f"[{sample_id}] ‚úÖ {result['details']} ({result['adata_spots']} spots)")
        return result

    logger.warning(
        f"[{sample_id}] üî• Mismatch found! "
        f"AnnData: {result['adata_spots']}, Parquet: {result['processed_spots']}. "
        f"Loss: {discrepancy} spots ({result['loss_percentage']:.2f}%)"
    )

    # 4. Ê†πÊú¨ÂéüÂõ†ÂàÜÊûê
    valid_image_paths = sample_nodes_df["image_path"].dropna().tolist()
    if not valid_image_paths:
        result["diagnosis"] = "Critical"
        result["details"] = "No valid image paths found in processed data, but spots exist in AnnData."
        logger.error(f"[{sample_id}] ‚ùå {result['details']}")
        return result
    
    # ‰ªéË∑ØÂæÑ‰∏≠Ëß£ÊûêÂá∫ spot_id
    processed_spot_ids_from_path = {Path(p).stem for p in valid_image_paths}
    missing_spot_ids = adata_spot_ids - processed_spot_ids_from_path

    # Êé®Êñ≠Áì¶ÁâáÁõÆÂΩïÂíåÊâ©Â±ïÂêç
    first_path = Path(valid_image_paths[0])
    tile_dir, tile_ext = first_path.parent, first_path.suffix

    # ÊäΩÊü•Âá†‰∏™‰∏¢Â§±ÁöÑ spot
    num_missing_files = 0
    for spot_id in list(missing_spot_ids)[:5]: # Check up to 5 missing spots
        path_to_check = tile_dir / f"{spot_id}{tile_ext}"
        if not path_to_check.exists():
            num_missing_files += 1
            
    if num_missing_files > 0:
        result["diagnosis"] = "File Not Found"
        result["details"] = f"At least {num_missing_files} of 5 checked 'missing' spots correspond to non-existent tile image files."
        logger.error(f"[{sample_id}] ‚ùå {result['details']}")
    else:
        result["diagnosis"] = "Logic/Filter Issue"
        result["details"] = "'Missing' spots seem to have valid image files. The issue is likely in the data filtering logic."
        logger.warning(f"[{sample_id}] ‚ö†Ô∏è {result['details']}")

    return result

# --- 3. ‰∏ªÊâßË°åÈÄªËæë ---
logger.info("--- Starting Widespread Data Loss Diagnostic ---")

# a. Âä†ËΩΩÂÆåÊï¥ÁöÑËäÇÁÇπÊï∞ÊçÆ
nodes_path = PROCESSED_DATA_DIR / "nodes.parquet"
if not nodes_path.exists():
    raise FileNotFoundError(f"Êú™ÊâæÂà∞Â§ÑÁêÜÂêéÁöÑËäÇÁÇπÊñá‰ª∂: {nodes_path}")
full_nodes_df = pd.read_parquet(nodes_path)

# b. Ëé∑ÂèñÊâÄÊúâÊ†∑Êú¨IDÂπ∂ÈöèÊú∫ÊäΩÊ†∑
all_sample_ids = full_nodes_df['sample_id'].unique().tolist()
num_samples_to_check = min(10, len(all_sample_ids)) # Check up to 10 samples
selected_samples = random.sample(all_sample_ids, num_samples_to_check)

logger.info(f"Randomly selected {len(selected_samples)} samples for diagnosis: {selected_samples}")

# c. Âæ™ÁéØËØäÊñ≠
diagnostic_results = []
for sample_id in tqdm(selected_samples, desc="Diagnosing Samples"):
    diagnostic_results.append(
        diagnose_sample_data_loss(sample_id, RAW_DATA_DIR, full_nodes_df)
    )

# d. Ê±áÊÄªÂπ∂Â±ïÁ§∫ÁªìÊûú
summary_df = pd.DataFrame(diagnostic_results)
summary_df['loss_percentage'] = summary_df['loss_percentage'].map('{:.2f}%'.format)

# Ë∞ÉÊï¥ÂàóÈ°∫Â∫è‰ª•‰æøÊü•Áúã
display_cols = ['sample_id', 'diagnosis', 'adata_spots', 'processed_spots', 'loss_percentage', 'details']
summary_df = summary_df[display_cols]

print("\n" + "="*80)
print("                       Widespread Data Loss Diagnostic Summary")
print("="*80)
pd.set_option('display.max_rows', 20)
pd.set_option('display.max_colwidth', 60)
print(summary_df)
print("="*80)

# e. ÊúÄÁªàÁªìËÆ∫
critical_issues = summary_df[summary_df['diagnosis'].isin(['File Not Found', 'Critical'])]
if not critical_issues.empty:
    logger.error(f"\nüî• CRITICAL FINDING: {len(critical_issues)} out of {num_samples_to_check} checked samples have significant data loss, likely due to missing tile files.")
    logger.error("This indicates a systemic problem in the initial tile generation/saving process. Please review the upstream scripts.")
else:
    logger.info("\n‚úÖ All randomly checked samples appear to be healthy or have minor issues. The problem might be isolated to specific samples like NCBI525.")

# ===== Cell 5: Code =====
from concurrent.futures import ThreadPoolExecutor
import os

# Check whether all the paths in the nodesdf exist
logger.info("--- ÂÖ®Èù¢Ê£ÄÊü• `nodes.parquet` ‰∏≠ÁöÑÊâÄÊúâÊñá‰ª∂Ë∑ØÂæÑ ---")

# Á°Æ‰øù full_nodes_df Â∑≤‰ªéÂâç‰∏Ä‰∏™ÂçïÂÖÉÊ†ºÂä†ËΩΩ
if 'full_nodes_df' not in locals():
    logger.error("‚ùå `full_nodes_df` Êú™ÂÆö‰πâ„ÄÇËØ∑ÂÖàËøêË°å‰∏äÈù¢ÁöÑÂçïÂÖÉÊ†º„ÄÇ")
else:
    # Ëé∑ÂèñÊâÄÊúâÂîØ‰∏ÄÁöÑ„ÄÅÈùûÁ©∫ÁöÑÂõæÂÉèË∑ØÂæÑ
    image_paths_to_check = full_nodes_df['image_path'].dropna().unique()
    total_paths = len(image_paths_to_check)
    logger.info(f"Â∞ÜË¶ÅÊ£ÄÊü• {total_paths} ‰∏™ÂîØ‰∏ÄÁöÑÊñá‰ª∂Ë∑ØÂæÑ...")

    def check_path_exists(path_str):
        """Ê£ÄÊü•Âçï‰∏™Êñá‰ª∂Ë∑ØÂæÑÊòØÂê¶Â≠òÂú®ÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôËøîÂõûË∑ØÂæÑÂ≠óÁ¨¶‰∏≤„ÄÇ"""
        absolute_path = PROJECT_ROOT / path_str
        if not absolute_path.exists():
            return path_str
        return None

    missing_paths = []
    # ‰ΩøÁî® ThreadPoolExecutor Âπ∂Ë°åÊ£ÄÊü•Êñá‰ª∂
    with ThreadPoolExecutor(max_workers=os.cpu_count() or 4) as executor:
        # ‰ΩøÁî® tqdm ÊòæÁ§∫ËøõÂ∫¶Êù°
        results = list(tqdm(executor.map(check_path_exists, image_paths_to_check), total=total_paths, desc="Ê≠£Âú®È™åËØÅÊñá‰ª∂Ë∑ØÂæÑ"))
    
    # ‰ªéÁªìÊûú‰∏≠ËøáÊª§Âá∫ÊâÄÊúâÈùû None ÁöÑÈ°πÔºàÂç≥Áº∫Â§±ÁöÑË∑ØÂæÑÔºâ
    missing_paths = [path for path in results if path is not None]

    # --- Êä•ÂëäÁªìÊûú ---
    print("\n" + "="*80)
    print("                       Êñá‰ª∂Ë∑ØÂæÑÂÆåÊï¥ÊÄßÊ£ÄÊü•Êä•Âëä")
    print("="*80)
    if not missing_paths:
        logger.info(f"‚úÖ ÊàêÂäüÔºÅÊâÄÊúâ {total_paths} ‰∏™Êñá‰ª∂Ë∑ØÂæÑÂùáÊúâÊïà‰∏îÊñá‰ª∂Â≠òÂú®„ÄÇ")
    else:
        num_missing = len(missing_paths)
        logger.error(f"üî• Â§±Ë¥•ÔºÅÂú® {total_paths} ‰∏™Ë∑ØÂæÑ‰∏≠ÂèëÁé∞ {num_missing} ‰∏™Áº∫Â§±Êñá‰ª∂„ÄÇ")
        logger.error("ËøôË°®ÊòéÂú®Êï∞ÊçÆÂ§ÑÁêÜÁöÑÊüê‰∏™Èò∂ÊÆµÔºåÁì¶ÁâáÊñá‰ª∂‰∏¢Â§±ÊàñË∑ØÂæÑËÆ∞ÂΩïÈîôËØØ„ÄÇ")
        
        # ÊâìÂç∞‰∏Ä‰∫õÁº∫Â§±Êñá‰ª∂ÁöÑ‰æãÂ≠ê
        print("\nÁº∫Â§±Êñá‰ª∂Á§∫‰æã (ÊúÄÂ§öÊòæÁ§∫10‰∏™):")
        for p in missing_paths[:10]:
            print(f"  - {p}")
        if num_missing > 10:
            print(f"  ... ‰ª•ÂèäÂÖ∂‰ªñ {num_missing - 10} ‰∏™Êñá‰ª∂„ÄÇ")
    print("="*80)


===== notebooks/test1_loss_test.ipynb =====
# ===== Cell 1: Code =====


# ===== Cell 2: Code (execution_count: 4) =====
# --- Check 1: Row-sum == 1 (main positive + neighbors, row-normalized) ---

from pathlib import Path
import numpy as np
import pandas as pd

# === ÈÖçÁΩÆ ===
ARTIFACTS_DIR = Path("/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts")
NODES_PATH = ARTIFACTS_DIR / "nodes.parquet"
EDGES_PATH = ARTIFACTS_DIR / "edges.parquet"

# ÊäΩÊ†∑ÁöÑ anchor Êï∞ÈáèÔºàÂª∫ËÆÆ 200~1000 ËßÜÂÜÖÂ≠òÔºâ
NUM_ANCHORS_TO_CHECK = 500

# ‚Äú‰∏ªÊ≠£Ê†∑Êú¨ÊùÉÈáç=1 + ÈÇªÂ±Ö alpha ÂêéÂΩí‰∏ÄÂåñ‚ÄùÁöÑÈªòËÆ§Á≠ñÁï•
# Â¶ÇÊûú‰Ω†Âú®ËÆ≠ÁªÉÈáå‰ΩøÁî®‰∫Ü‚ÄúÈîêÂåñ/Ê∏©Â∫¶‚ÄùÂ§ÑÁêÜÔºà‰æãÂ¶Ç alpha^gamma Êàñ‰∏ªÊ≠£Ê†∑Êú¨ÊùÉÈáç œÑÔºâÔºåÂèØ‰ª•Âú®ËøôÈáåÂêåÊ≠•ËÆæÁΩÆÔºö
USE_GAMMA = False
GAMMA = 1.0      # Ëã• USE_GAMMA=TrueÔºåÂàôÂØπ alpha ‰ΩøÁî® alpha ** GAMMA
USE_TAU = False
TAU = 1.0        # Ëã• USE_TAU=TrueÔºåÂàô‰∏ªÊ≠£Ê†∑Êú¨ÊùÉÈáçËÆæ‰∏∫ TAUÔºàÊõø‰ª£ 1Ôºâ

print(f"Loading nodes: {NODES_PATH}")
nodes = pd.read_parquet(NODES_PATH, columns=["tile_id"])
tile_ids = nodes["tile_id"].to_numpy()

print(f"Loading edges: {EDGES_PATH}")
edges = pd.read_parquet(EDGES_PATH, columns=["src_tile_id", "nbr_tile_id", "alpha"])

# ‰∏∫‰∫ÜÂä†ÈÄüÈöèÊú∫ÊäΩÊ†∑ÔºåËøôÈáåÂè™Áî®Â≠òÂú®Âá∫ËæπÁöÑ src ÈõÜÂêà
src_unique = edges["src_tile_id"].unique()
rng = np.random.default_rng(2025)
sampled_src = rng.choice(src_unique, size=min(NUM_ANCHORS_TO_CHECK, len(src_unique)), replace=False)

row_sums = []
bad_rows = []

# Â∞ÜËæπÊåâ src ÂàÜÁªÑÔºå‰æø‰∫éÂø´ÈÄüÊ£ÄÁ¥¢
grp = edges.groupby("src_tile_id", sort=False)

for src in sampled_src:
    if src not in grp.groups:
        # ËØ• src Ê≤°ÊúâÈÇªÂ±ÖËæπÔºàÊûÅÂ∞ëÊï∞ÊÉÖÂÜµÔºâÔºåÁõ¥Êé•Ë∑≥Ëøá
        continue
    sub = grp.get_group(src)

    # ÈÇªÂ±Ö alpha
    alpha = sub["alpha"].to_numpy(dtype=np.float64)

    # ÂèØÈÄâÔºöÂØπ alpha ÂÅöÂπÇÊ¨°ÈîêÂåñ
    if USE_GAMMA and GAMMA != 1.0:
        alpha = np.power(alpha, GAMMA)

    # ‰∏ªÊ≠£Ê†∑Êú¨ÊùÉÈáç
    main_w = TAU if USE_TAU else 1.0

    total = main_w + alpha.sum()
    if total <= 0:
        # ‰∏çÂ∫îÂá∫Áé∞ÔºõÈò≤Âæ°ÊÄßÂ§ÑÁêÜ
        rs = np.nan
    else:
        # ÂΩí‰∏ÄÂåñÂêéÁöÑË°åÂíåÂ∫î‰∏∫ 1
        rs = (main_w / total) + (alpha / total).sum()

    row_sums.append(rs)
    if not np.isfinite(rs) or abs(rs - 1.0) > 1e-6:
        bad_rows.append((src, rs))

row_sums = np.array(row_sums, dtype=np.float64)
print(f"[Check1] Checked {len(row_sums)} rows. mean={row_sums.mean():.8f}, "
      f"std={row_sums.std():.8f}, min={row_sums.min():.8f}, max={row_sums.max():.8f}")
if bad_rows:
    print(f"[Check1][WARN] Found {len(bad_rows)} rows with |sum-1|>1e-6. Show first 5:")
    for a,b in bad_rows[:5]:
        print(f"  src_tile_id={a}, row_sum={b}")
else:
    print("[Check1] ‚úÖ All sampled rows sum to 1 within tolerance.")


# ===== Cell 3: Code (execution_count: 5) =====
# --- Check 2: Mapping assertion for main positives (simulated) ---

from pathlib import Path
import numpy as np
import pandas as pd

ARTIFACTS_DIR = Path("/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts")
NODES_PATH = ARTIFACTS_DIR / "nodes.parquet"

# === Ê®°ÊãüÂàÜÂ∏ÉÂºèËÆæÁΩÆ ===
WORLD_SIZE = 3
N_LOCAL = 16      # ÊØè‰∏™ rank ÁöÑÊú¨Âú∞ batch Â§ßÂ∞èÔºàÁî®‰∫éÊ®°ÊãüÔºâÔºåÂèØÊîπÂ∞è‰∏ÄÁÇπ‰ª•‰æøÂèØËßÜÂåñ
RANKS_TO_TEST = [0, 1, 2]  # Ë¶ÅÊµãËØïÁöÑ rank ÈõÜÂêà

nodes = pd.read_parquet(NODES_PATH, columns=["tile_id"])
tile_ids = nodes["tile_id"].to_numpy()
assert len(tile_ids) >= WORLD_SIZE * N_LOCAL, "Êï∞ÊçÆÈáè‰∏çË∂≥‰ª•ÊûÑÈÄ†Ê®°ÊãüÊâπÊ¨°ÔºåËØ∑ÂáèÂ∞è N_LOCAL Êàñ WORLD_SIZE"

# ÁÆÄÂçïÁöÑ‚ÄúËΩÆËΩ¨ÂàÜÈÖç‚ÄùÊñπÂºèÊ®°Êãü DistributedSamplerÔºàÁúüÂÆûÂÆûÁé∞Êúâ shuffleÔºå‰ΩÜÁ¥¢ÂºïÂÖ¨Âºè‰∏ÄËá¥Ôºâ
# Êàë‰ª¨ÂèñÂâç WORLD_SIZE * N_LOCAL ‰∏™Ê†∑Êú¨ÔºåÂàáÊàê WORLD_SIZE ÁâáÔºåÊØèÁâá N_LOCAL Â§ßÂ∞è
local_batches = []
for r in range(WORLD_SIZE):
    start = r * N_LOCAL
    end = (r + 1) * N_LOCAL
    local_img_ids = tile_ids[start:end].copy()  # ‰Ωú‰∏∫ image_tile_ids
    local_txt_ids = tile_ids[start:end].copy()  # ‰Ωú‰∏∫ text_tile_idsÔºàÂØπÁß∞Ôºâ
    local_batches.append((local_img_ids, local_txt_ids))

# Ê®°Êãü all_gather ÂêéÁöÑ‚ÄúÂÖ®Â±ÄÂàóÂüü‚Äù
global_tile_ids = np.concatenate([b[1] for b in local_batches], axis=0)  # Áî® text Á´ØÂÜ≥ÂÆöÂàóÂüü
txt_id_to_idx = {int(tid): idx for idx, tid in enumerate(global_tile_ids)}

# ÂØπÊØè‰∏™ rank ÂÅöÊñ≠Ë®ÄÔºötxt_id_to_idx[image_tile_ids[i]] == rank*N_LOCAL + i
bad = []
for rank in RANKS_TO_TEST:
    image_tile_ids, _ = local_batches[rank]
    for i in range(N_LOCAL):
        tid = int(image_tile_ids[i])
        col = txt_id_to_idx.get(tid, None)
        expect = rank * N_LOCAL + i
        if col != expect:
            bad.append((rank, i, tid, col, expect))

if bad:
    print(f"[Check2][FAIL] Found {len(bad)} mismatches. Show first 10:")
    for r,i,tid,col,exp in bad[:10]:
        print(f"  rank={r}, i={i}, tile_id={tid}, mapped_col={col}, expected={exp}")
else:
    print("[Check2] ‚úÖ Mapping assertion passed for all tested ranks.")


# ===== Cell 4: Code (execution_count: 6) =====
# --- Check 3: Print pos_col and neighbor_cols (simulated batch domain) ---

from pathlib import Path
import numpy as np
import pandas as pd

ARTIFACTS_DIR = Path("/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts")
NODES_PATH = ARTIFACTS_DIR / "nodes.parquet"
EDGES_PATH = ARTIFACTS_DIR / "edges.parquet"

WORLD_SIZE = 3
N_LOCAL = 16      # ‰∏éÊ£ÄÊü• 2 Áõ∏ÂêåÔºå‰øùÊåÅ‰∏ÄËá¥
RANK = 0          # ÈÄâÊã©‰∏Ä‰∏™ rank Êù•Â±ïÁ§∫
ANCHOR_INDEX_IN_LOCAL = 3  # ÈÄâÊã©ËØ• rank ÁöÑÁ¨¨Âá†‰∏™Ê†∑Êú¨‰Ωú‰∏∫ anchor

# 1) ËØªÂèñÊï∞ÊçÆ
nodes = pd.read_parquet(NODES_PATH, columns=["tile_id"])
edges = pd.read_parquet(EDGES_PATH, columns=["src_tile_id", "nbr_tile_id", "distance", "alpha"])
tile_ids = nodes["tile_id"].to_numpy()
assert len(tile_ids) >= WORLD_SIZE * N_LOCAL

# 2) ÊûÑÈÄ†Ê®°ÊãüÊâπÊ¨°‰∏éÂàóÂüüÔºà‰∏éÊ£ÄÊü• 2 Áõ∏ÂêåÔºâ
local_batches = []
for r in range(WORLD_SIZE):
    start = r * N_LOCAL
    end = (r + 1) * N_LOCAL
    local_img_ids = tile_ids[start:end].copy()
    local_txt_ids = tile_ids[start:end].copy()
    local_batches.append((local_img_ids, local_txt_ids))

global_tile_ids = np.concatenate([b[1] for b in local_batches], axis=0)  # ÂàóÂüüÁî± text Á´ØÁ°ÆÂÆö
txt_id_to_idx = {int(tid): idx for idx, tid in enumerate(global_tile_ids)}

# 3) ÈÄâÊã© anchorÔºàÂèñ image Á´ØÁöÑÁ¨¨ ANCHOR_INDEX_IN_LOCAL ‰∏™Ôºâ
image_tile_ids, text_tile_ids = local_batches[RANK]
anchor_tile_id = int(image_tile_ids[ANCHOR_INDEX_IN_LOCAL])

# 4) ÂèñËØ• anchor ÁöÑÈÇªÂ±ÖÔºàÊåâË∑ùÁ¶ª‰ªéËøëÂà∞ËøúÔºâ
nbr_df = edges[edges["src_tile_id"] == anchor_tile_id].sort_values("distance").copy()
# Âè™‰øùÁïô‚ÄúÊú¨ÊâπÊ¨°ÂàóÂüüÈáåÂ≠òÂú®‚ÄùÁöÑÈÇªÂ±Ö
in_batch_mask = nbr_df["nbr_tile_id"].isin(global_tile_ids)
nbr_in = nbr_df[in_batch_mask]

pos_col = txt_id_to_idx.get(anchor_tile_id, None)
neighbor_cols = [txt_id_to_idx.get(int(t), None) for t in nbr_in["nbr_tile_id"].tolist()]

print(f"[Check3] Anchor tile_id={anchor_tile_id}")
print(f"  pos_col (should be rank*N_LOCAL + local_index): {pos_col}")
print(f"  neighbors in-batch = {len(nbr_in)} / total neighbors = {len(nbr_df)}")

# ÊâìÂç∞ÂâçËã•Âπ≤ÈÇªÂ±Ö
MAX_SHOW = 12
for idx, row in nbr_in.head(MAX_SHOW).iterrows():
    t = int(row["nbr_tile_id"])
    col = txt_id_to_idx.get(t, None)
    dist = float(row["distance"])
    alpha = float(row["alpha"])
    print(f"    nbr_tile_id={t:>10d} | col={col:>5} | dist={dist:8.3f} | alpha={alpha:6.3f}")

# 5) ÂèØÈÄâÔºöËÆ°ÁÆó‚Äú‰∏ªÊ≠£Ê†∑Êú¨+ÈÇªÂ±ÖÔºà‰ªÖÊâπÂÜÖÔºâ‚ÄùÁöÑÂΩí‰∏ÄÂåñË°åÂíåÔºåËæÖÂä©‰∫∫Â∑•Ê†∏È™å
main_w = 1.0
alpha_vec = nbr_in["alpha"].to_numpy(dtype=float)
row_sum = (main_w + alpha_vec.sum())
print(f"  (Optional) main+in-batch-neighbors sum before normalization = {row_sum:.6f} "
      f"(Â∫î < 1+Œ£ÊâÄÊúâÈÇªÂ±ÖÔºõ‰ªÖÁî®‰∫éËæÖÂä©ÁêÜËß£)")


# ===== Cell 5: Markdown =====
# ÊµãËØïÊîπÊ≠£

# ===== Cell 6: Code (execution_count: 7) =====
# ==== Âú® Notebook ÈáåËøêË°åÔºö‰∏∫‰Ω†ÁöÑ Dataset Â¢ûË°•Á¥¢ÂºïÂä†ÈÄüÁªìÊûÑ ====
import numpy as np
from collections import defaultdict

def dataset_build_fast_indices(dataset, k_neighbors: int = 6):
    """
    ‰∏∫ SpatiallyAwareDataset ÊûÑÂª∫Âø´ÈÄüÁ¥¢Âºï‰∏éÂêëÈáèÂåñÈÇªÂ±ÖÁü©Èòµ„ÄÇ
    ÈúÄË¶Å dataset Êã•Êúâ:
      - dataset.tile_ids: np.ndarray shape [N]
      - dataset.sample_ids: np.ndarray shape [N] ÊàñÁ≠â‰ª∑ÁöÑÂàóË°®
      - dataset.edges_map: dict[int, list[int]]  # src_tile_id -> nbr_tile_id(ÈïøÂ∫¶<=k)
    ÁîüÊàê:
      - dataset.id2idx: dict[tile_id] -> dataset index
      - dataset.sample_to_indices: dict[sample_id] -> np.ndarray of indices
      - dataset.nbr_index: np.ndarray [N, k_neighbors]ÔºåÂÖÉÁ¥†‰∏∫Á¥¢ÂºïÔºåÁº∫Â§±Â°´ -1
    """
    assert hasattr(dataset, "tile_ids") and hasattr(dataset, "sample_ids"), "Dataset Áº∫Â∞ë tile_ids Êàñ sample_ids"
    assert hasattr(dataset, "edges_map"), "Dataset ÈúÄË¶ÅÊúâ edges_map (src_tile_id -> [nbr_tile_id...])"
    tile_ids = np.asarray(dataset.tile_ids)
    sample_ids = np.asarray(dataset.sample_ids)

    N = tile_ids.shape[0]
    id2idx = {int(t): int(i) for i, t in enumerate(tile_ids)}
    dataset.id2idx = id2idx

    # ÊØè‰∏™Ê†∑Êú¨ÁöÑÁ¥¢ÂºïÂàóË°®ÔºàÂêëÈáèÂåñÔºâ
    sample_to_indices = defaultdict(list)
    for i, sid in enumerate(sample_ids):
        sample_to_indices[sid].append(i)
    dataset.sample_to_indices = {sid: np.asarray(idxs, dtype=np.int64) for sid, idxs in sample_to_indices.items()}

    # ÈÇªÂ±Ö‚ÄúÁ¥¢ÂºïÁü©Èòµ‚Äù [N, K]ÔºåÂêëÈáèÂåñÊò†Â∞Ñ tile_id->index
    K = int(k_neighbors)
    nbr_index = np.full((N, K), -1, dtype=np.int64)
    for i in range(N):
        src_tid = int(tile_ids[i])
        nbr_tids = dataset.edges_map.get(src_tid, [])
        # Âè™ÂèñÂâçK‰∏™
        if len(nbr_tids) > K:
            nbr_tids = nbr_tids[:K]
        # Êò†Â∞Ñ‰∏∫Á¥¢ÂºïÔºåÊò†Â∞Ñ‰∏çÂà∞ÁöÑÁΩÆ -1
        mapped = [id2idx.get(int(t), -1) for t in nbr_tids]
        if mapped:
            nbr_index[i, :len(mapped)] = np.asarray(mapped, dtype=np.int64)
    dataset.nbr_index = nbr_index
    print(f"[build_fast_indices] N={N}, K={K}; sample buckets={len(dataset.sample_to_indices)}; done.")


# ===== Cell 7: Code (execution_count: 8) =====
# ==== Âú® Notebook ÈáåËøêË°åÔºöÈÇªÂüüÊÑüÁü•ÊâπÈááÊ†∑Âô® ====
import math
import random
import torch
from torch.utils.data import Sampler

class SpatialBucketBatchSampler(Sampler):
    """
    ÈÇªÂüüÊÑüÁü• BatchSamplerÔºö
    - ÊØè‰∏™ batch Áî±Ëã•Âπ≤‚Äú‰∏≠ÂøÉÈîöÁÇπ‚Äù + Ëøô‰∫õÈîöÁÇπÁöÑÈÇªÂ±ÖÔºà‰πü‰Ωú‰∏∫ÈîöÁÇπÔºâÁªÑÊàêÔºõ
    - Âè™‰∫ßÂá∫Á¥¢ÂºïÂàóË°®Ôºà‰∏çÊîπ DataLoader ÁöÑ __getitem__ Ë°å‰∏∫Ôºâ‚Üí IO ‰∏çÂ¢ûÂä†Ôºõ
    - DDP ‰∏ãÊåâ sample_id ÂàÜÊ°∂Áªô rankÔºàhash ÂàÜÈÖçÔºâÔºåÂêÑ rank Êâ´Êèè‰∏çÂêåÊ†∑Êú¨ÈõÜÂêàÔºõ
    - ‰øùËØÅÊØè‰∏™ rank ÁöÑ batch Êï∞Á≠âÈïøÔºàÂæ™ÁéØË°•ÈΩêÊàñ‰∏¢ÂºÉÂ§ö‰ΩôÔºâ„ÄÇ

    ‰æùËµñ dataset ÊàêÂëòÔºö
      dataset.sample_to_indices: dict[sample_id] -> np.ndarray of indices
      dataset.nbr_index: np.ndarray[N, K] ÈÇªÂ±ÖÁ¥¢ÂºïÁü©ÈòµÔºà-1 Ë°®Á§∫Êó†ÊïàÔºâ
      dataset.sample_ids: np.ndarray[N]
    """
    def __init__(
        self,
        dataset,
        batch_size: int,
        world_size: int = 1,
        rank: int = 0,
        centers_per_batch: int = 16,
        max_neighbors_per_center: int = 4,
        same_sample_only: bool = True,
        drop_last: bool = True,
        seed: int = 2025
    ):
        self.dataset = dataset
        self.batch_size = int(batch_size)
        self.world_size = int(world_size)
        self.rank = int(rank)
        self.centers_per_batch = int(centers_per_batch)
        self.max_neighbors_per_center = int(max_neighbors_per_center)
        self.same_sample_only = bool(same_sample_only)
        self.drop_last = bool(drop_last)
        self.seed = int(seed)

        assert hasattr(dataset, "sample_to_indices") and hasattr(dataset, "nbr_index"), \
            "ËØ∑ÂÖàË∞ÉÁî® dataset_build_fast_indices(dataset) ÁîüÊàê sample_to_indices / nbr_index"
        self.N = len(dataset.sample_ids)
        self.sample_ids = np.asarray(dataset.sample_ids)

        # 1) Â∞ÜÊ†∑Êú¨Ê°∂ÊåâÂìàÂ∏åÂàÜÈÖçÁªôÂêÑ rankÔºåÈÅøÂÖç DDP ÈáçÂè†
        all_samples = list(dataset.sample_to_indices.keys())
        all_samples.sort(key=lambda x: str(x))  # Á®≥ÂÆöÈ°∫Â∫è
        self.assigned_samples = [s for s in all_samples if (hash(str(s)) % self.world_size) == self.rank]
        assert len(self.assigned_samples) > 0, "Êú¨ rank Ë¢´ÂàÜÈÖçÁöÑÊ†∑Êú¨Ê°∂‰∏∫Á©∫ÔºåËØ∑Ê£ÄÊü•Êï∞ÊçÆÊàñ world_size ËÆæÁΩÆ„ÄÇ"

        # 2) ËÆ°ÁÆóÊØè‰∏™ epoch ÁöÑÊâπÊ¨°Êï∞Ôºå‰øùËØÅÂêÑ rank Ê≠•Êï∞‰∏ÄËá¥
        #    Áî®ÂÖ®Â±Ä N ‰º∞ÁÆóÔºöÊØè‰∏™ rank ÁîüÊàê floor(N / (B * world_size)) ‰∏™ batch
        self.batches_per_epoch = self._compute_batches_per_epoch()

        # 3) ‰∏∫ÊØè‰∏™Ê†∑Êú¨Ê°∂ÂáÜÂ§á‰∏Ä‰∏™‚ÄúÊ∏∏Ê†áÊåáÈíà‚Äù‰∏é‰π±Â∫èÁ¥¢Âºï
        self._rng = random.Random(self.seed)
        self._epoch = 0
        self._reset_per_sample_state()

    def _compute_batches_per_epoch(self):
        # ÊåâÊÄªÊ†∑Êú¨Èáè‰∏é batch_size/world_size ‰º∞ÁÆó‰∏Ä‰∏™‚ÄúÂÖ®Â±ÄÁªü‰∏Ä‚ÄùÁöÑÊ≠•Êï∞
        est = self.N // (self.batch_size * self.world_size)
        return max(1, int(est))

    def _reset_per_sample_state(self):
        self._per_sample_order = {}
        self._per_sample_ptr = {}
        for s in self.assigned_samples:
            idxs = self.dataset.sample_to_indices[s]
            # Êâì‰π±
            order = idxs.copy()
            self._rng.shuffle(order.tolist())  # ÂéüÂú∞Ê¥óÁâåÔºàlist Êõ¥Âø´Ôºâ
            self._per_sample_order[s] = order
            self._per_sample_ptr[s] = 0

        # ÁªôÊ†∑Êú¨Ê°∂‰∏Ä‰∏™Á®≥ÂÆö‰ΩÜÊ¥óÁâåÂêéÁöÑÈÅçÂéÜÈ°∫Â∫è
        self._sample_cycle = self.assigned_samples.copy()
        self._rng.shuffle(self._sample_cycle)

    def set_epoch(self, epoch: int):
        self._epoch = int(epoch)
        # ‰ª• epoch ‰∏∫ÁßçÂ≠êÊâ∞Âä®ÔºåÁ°Æ‰øùÊØè‰∏™ epoch ÁöÑÈ°∫Â∫è‰∏çÂêå‰ΩÜÂêÑ rank ‰∏ÄËá¥ÂèØÂ§çÁé∞
        self._rng.seed(self.seed + self._epoch)
        self._reset_per_sample_state()

    def __len__(self):
        return self.batches_per_epoch

    def __iter__(self):
        rng = self._rng
        ds = self.dataset
        nbr_index = ds.nbr_index
        sample_ids = self.sample_ids

        # round-robin Êâ´ÊèèÊ†∑Êú¨Ê°∂Ôºå‰∫ßÁîüÂõ∫ÂÆöÊï∞ÈáèÁöÑ batch
        num_yield = 0
        sample_cursor = 0
        while num_yield < self.batches_per_epoch:
            s = self._sample_cycle[sample_cursor]
            sample_cursor = (sample_cursor + 1) % len(self._sample_cycle)

            order = self._per_sample_order[s]
            ptr = self._per_sample_ptr[s]
            if ptr >= len(order):
                # ËØ•Ê°∂ËÄóÂ∞ΩÔºåÈáçÁΩÆËØ•Ê°∂ÔºàÂæ™ÁéØ‰ΩøÁî®ÔºâÔºå‰ª•ÈÅøÂÖç‰∏çÂêå rank Ê≠•Êï∞‰∏ç‰∏ÄËá¥
                self._rng.shuffle(order.tolist())
                self._per_sample_ptr[s] = 0
                ptr = 0

            batch = []
            used = set()

            # 1) ÈÄâ‰∏≠ÂøÉÈîöÁÇπ
            centers = []
            centers_take = min(self.centers_per_batch, len(order) - ptr)
            if centers_take <= 0:
                continue
            centers = order[ptr:ptr + centers_take]
            self._per_sample_ptr[s] += centers_take

            # 2) Â∞Ü‰∏≠ÂøÉÂä†ÂÖ• batch
            for c in centers:
                if len(batch) >= self.batch_size:
                    break
                if c in used:
                    continue
                batch.append(int(c))
                used.add(int(c))

                # 3) ÊãâÂèñËØ•‰∏≠ÂøÉÁöÑÈÇªÂ±ÖÁ¥¢ÂºïÔºåÈôêÂà∂ÂêåÊ†∑Êú¨„ÄÅÂéªÈáçÔºåÊúÄÂ§öÂèñ max_neighbors_per_center
                nbrs = nbr_index[int(c)]
                # ËøáÊª§Êó†Êïà
                nbrs = nbrs[nbrs >= 0]
                if self.same_sample_only:
                    nbrs = nbrs[sample_ids[nbrs] == s]
                # ÂéªÈáçËá™Â∑±/Â∑≤ÈÄâ
                if len(nbrs) > 0:
                    # Êâì‰π±ÈÇªÂ±ÖÊ¨°Â∫èÔºåÈÅøÂÖçÊÄªÊòØÂèñÂâç K ‰∏™
                    nbrs = nbrs.copy()
                    rng.shuffle(nbrs.tolist())
                    for nb in nbrs:
                        if len(batch) >= self.batch_size:
                            break
                        nb = int(nb)
                        if nb in used:
                            continue
                        batch.append(nb)
                        used.add(nb)
                        if len(batch) >= self.batch_size or \
                           (len(batch) - len(centers)) >= self.max_neighbors_per_center * len(centers):
                            break  # ÊéßÂà∂ÈÇªÂ±ÖÈ¢ÑÁÆó

            # 4) Ëã• batch Êú™Êª°ÔºåÁªßÁª≠‰ªéÂêåÊ†∑Êú¨Ë°•ÈΩêÈöèÊú∫Ê†∑Êú¨Ôºà‰∏ç‰øùËØÅÊòØÈÇªÂ±ÖÔºâ
            if len(batch) < self.batch_size:
                remain = self.batch_size - len(batch)
                # ‰ªéËØ•Ê°∂Ââ©‰ΩôÁ¥¢Âºï‰∏≠Ë°•
                ptr2 = self._per_sample_ptr[s]
                if ptr2 + remain <= len(order):
                    fill = order[ptr2:ptr2 + remain]
                    self._per_sample_ptr[s] += remain
                    for f in fill:
                        fi = int(f)
                        if fi in used:
                            continue
                        batch.append(fi)
                        used.add(fi)
                else:
                    # ‰∏çÂ§üÂ∞±Âæ™ÁéØË°•
                    take = 0
                    while len(batch) < self.batch_size:
                        if self._per_sample_ptr[s] >= len(order):
                            self._rng.shuffle(order.tolist())
                            self._per_sample_ptr[s] = 0
                        fi = int(order[self._per_sample_ptr[s]])
                        self._per_sample_ptr[s] += 1
                        if fi in used:
                            continue
                        batch.append(fi)
                        used.add(fi)

            # 5) ‰∫ßÂá∫
            if self.drop_last and len(batch) < self.batch_size:
                continue
            num_yield += 1
            yield batch


# ===== Cell 8: Code (execution_count: 15) =====
# ==== Âú® Notebook ÈáåËøêË°åÁ§∫‰æãÔºàÊàñÁ≤òË¥¥Âà∞‰Ω†ÁöÑ train ËÑöÊú¨ÈáåÔºâ ====
from torch.utils.data import DataLoader
import torch.distributed as dist
from open_clip_train.spatial_data import SpatiallyAwareDataset

# 1) ÂáÜÂ§á datasetÔºà‰Ω†Â∑≤ÊúâÔºâ
dataset = SpatiallyAwareDataset(artifacts_dir="/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts", k_neighbors=6)
# ÂÅáÂÆö dataset Â∑≤ÁªèÊúâ tile_ids / sample_ids / edges_map
dataset_build_fast_indices(dataset, k_neighbors=6)

# 2) ÊûÑÈÄ† batch_samplerÔºàDDP ÁéØÂ¢É‰∏ãÔºâ
if dist.is_available() and dist.is_initialized():
    world_size = dist.get_world_size()
    rank = dist.get_rank()
else:
    world_size = 1
    rank = 0

batch_sampler = SpatialBucketBatchSampler(
    dataset=dataset,
    batch_size=1600,
    world_size=world_size,
    rank=rank,
    centers_per_batch=16,           # ÂèØË∞ÉÔºö‰∏≠ÂøÉÈîöÁÇπ‰∏™Êï∞
    max_neighbors_per_center=4,     # ÂèØË∞ÉÔºöÊØè‰∏™‰∏≠ÂøÉÂ∏¶ÁöÑÈÇªÂ±Ö‰∏äÈôê
    same_sample_only=True,
    drop_last=True,
    seed=2025,
)

# 3) DataLoaderÔºàÊ≥®ÊÑèÔºö‰∏çË¶ÅÂÜç‰º† batch_size / shuffle / samplerÔºâ
loader = DataLoader(
    dataset,
    batch_sampler=batch_sampler,
    num_workers=16,
    pin_memory=True,
    persistent_workers=True,
    prefetch_factor=4,
    collate_fn=getattr(dataset, "collate_fn", None)  # Ëã•‰Ω†ÊúâËá™ÂÆö‰πâ collate_fn Â∞±‰º†ÂÖ•
)

# 4) ËÆ≠ÁªÉÂæ™ÁéØ‰∏≠ÔºåÊØè‰∏™ epoch ÂºÄÂßãÂâçËÆæÁΩÆ epochÔºàÁ°Æ‰øùÂèØÂ§çÁé∞Ê¥óÁâåÔºâ
for epoch in range(num_epochs):
    if hasattr(batch_sampler, "set_epoch"):
        batch_sampler.set_epoch(epoch)
    # for step, batch in enumerate(loader):
    #     ... ‰Ω†ÁöÑËÆ≠ÁªÉÈÄªËæë ...


# ===== Cell 9: Code =====



===== notebooks/d3_smoke_test_dataset_construct.ipynb =====
# ‚ö†Ô∏è WARNING: File near/over threshold (size=1145056 bytes, chars=13191)
# ===== Cell 1: Code (execution_count: 20) =====
# Êñá‰ª∂Ë∑ØÂæÑ: notebooks/d3_create_smoke_test_dataset.ipynb
# CodeGuardian: SOTA ÁâàÊú¨ v4 - Áªü‰∏ÄÊï∞ÊçÆÂä†ËΩΩÔºå‰ºòÂåñÊµÅÁ®ãÔºåÂπ∂Êï¥ÂêàÂä®ÊÄÅÊ†∑Êú¨ÈÄâÊã©„ÄÇ

# ==============================================================================
# Ê≠•È™§ 1: ËÆæÁΩÆ„ÄÅÂØºÂÖ•‰∏éËá™Âä®ÂåñË∑ØÂæÑÈÖçÁΩÆ
# ==============================================================================
import os
from pathlib import Path
import pandas as pd
import numpy as np
import pyarrow.parquet as pq
import pyarrow as pa
import shutil
import logging
import rootutils
import matplotlib.pyplot as plt
from PIL import Image
from tqdm.notebook import tqdm

# --- 1. Ëá™Âä®ËÆæÁΩÆÈ°πÁõÆÊ†πÁõÆÂΩï ---
# CodeGuardian: Ëøô‰∏ÄÊ≠•ÊòØÊ†∏ÂøÉ„ÄÇÊó†ËÆ∫ÊÇ®Âú®Âì™ÈáåËøêË°åËøô‰∏™ notebookÔºå
# rootutils ÈÉΩ‰ºöÊâæÂà∞È°πÁõÆÊ†πÁõÆÂΩïÔºåÂä†ËΩΩ .env Êñá‰ª∂ÔºåÂπ∂Ê≠£Á°ÆËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè„ÄÇ
try:
    root = rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True, dotenv=True)
except NameError:
    root = rootutils.setup_root(search_from=".", indicator=".project-root", pythonpath=True, dotenv=True)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- 2. Ë∑ØÂæÑÈÖçÁΩÆ (ÂÆåÂÖ®Ëá™Âä®Âåñ) ---
PROJECT_ROOT_STR = os.getenv("PROJECT_ROOT")
if not PROJECT_ROOT_STR:
    raise RuntimeError("ÁéØÂ¢ÉÂèòÈáè 'PROJECT_ROOT' Êú™ËÆæÁΩÆ„ÄÇËØ∑Á°Æ‰øùÈ°πÁõÆÊ†πÁõÆÂΩïÊúâ .env Êñá‰ª∂‰∏îÂÜÖÂÆπ‰∏∫ 'PROJECT_ROOT=${rootutils.find_root}'")
PROJECT_ROOT = Path(PROJECT_ROOT_STR)

INPUT_DATA_BASE_DIR = PROJECT_ROOT / "data/hest_hugo_6nei_correct_parquet_data"
INPUT_ARTIFACTS_DIR = INPUT_DATA_BASE_DIR / "train"
SMOKE_TEST_DIR = PROJECT_ROOT / "data/smoke_test_dataset"



# ===== Cell 2: Code (execution_count: 21) =====
# ==============================================================================
# Ê≠•È™§ 2: ‰∏ÄÊ¨°ÊÄßÂä†ËΩΩÊ∫êÊï∞ÊçÆ (Load Once, Use Many)
# ==============================================================================
logging.info("\n--- Ê≠•È™§ 2: ÂºÄÂßã‰∏ÄÊ¨°ÊÄßÂä†ËΩΩÂÆåÊï¥ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ... ---")
try:
    nodes_df = pd.read_parquet(INPUT_ARTIFACTS_DIR / "nodes.parquet")
    edges_df = pd.read_parquet(INPUT_ARTIFACTS_DIR / "edges.parquet")
    img_embeds = np.load(INPUT_ARTIFACTS_DIR / "image_embeds.npy")
    txt_embeds = np.load(INPUT_ARTIFACTS_DIR / "text_embeds.npy")
    logging.info("‚úÖ ÂÆåÊï¥ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂä†ËΩΩÂÆåÊØï„ÄÇ")
except FileNotFoundError as e:
    logging.error(f"‚ùå ÈîôËØØ: Êó†Ê≥ïÂä†ËΩΩËæìÂÖ•Êñá‰ª∂„ÄÇËØ∑Á°Æ‰øùË∑ØÂæÑ '{INPUT_ARTIFACTS_DIR}' Ê≠£Á°Æ„ÄÇËØ¶ÁªÜ‰ø°ÊÅØ: {e}")
    raise



# ===== Cell 3: Code (execution_count: 22) =====
# ==============================================================================
# Ê≠•È™§ 3: Ê∫êÊï∞ÊçÆÂÆåÊï¥ÊÄßÈ¢ÑÊ£ÄÊü•
# ==============================================================================
logging.info("\n--- Ê≠•È™§ 3: Ê≠£Âú®ÊâßË°åÊ∫êÊï∞ÊçÆÂÆåÊï¥ÊÄßÊ£ÄÊü•... ---")
# CodeGuardian: Áé∞Âú®Êàë‰ª¨Áõ¥Êé•Êìç‰ΩúÂÜÖÂ≠ò‰∏≠ÁöÑÊï∞ÊçÆÔºåÊó†ÈúÄÈáçÂ§çËØªÂèñÊñá‰ª∂„ÄÇ
num_nodes = len(nodes_df)
max_tile_id = nodes_df['tile_id'].max()
num_embeddings = img_embeds.shape[0]

logging.info(f"Parquet ‰∏≠ÁöÑËäÇÁÇπÊï∞Èáè: {num_nodes}")
logging.info(f"Parquet ‰∏≠ÁöÑÊúÄÂ§ß tile_id: {max_tile_id}")
logging.info(f"NumPy Êï∞ÁªÑ‰∏≠ÁöÑÂµåÂÖ•Êï∞Èáè: {num_embeddings}")

if num_nodes != num_embeddings:
    raise RuntimeError(f"‚ùå ‰∏•Èáç‰∏çÂåπÈÖç: ParquetË°åÊï∞ ({num_nodes}) ‰∏é NumPyË°åÊï∞ ({num_embeddings}) ‰∏çÂêå! Êï∞ÊçÆÂ∑≤ÊçüÂùèÔºåËØ∑ÈáçÊñ∞ÁîüÊàê„ÄÇ")
elif max_tile_id >= num_embeddings:
    logging.warning(f"‚ö†Ô∏è Ê≥®ÊÑè: ÊúÄÂ§ß tile_id ({max_tile_id}) Ë∂ÖÂá∫ NumPy Êï∞ÁªÑËæπÁïå ({num_embeddings})„ÄÇËøôÁ°ÆËÆ§‰∫Ü tile_id ‰∏çËøûÁª≠ÔºåËÑöÊú¨Â∞Ü‰æùËµñË°åÂè∑ËøõË°åÂ§ÑÁêÜ„ÄÇ")
else:
    logging.info("‚úÖ Êñá‰ª∂Ë°åÊï∞ÂØπÈΩê„ÄÇ")



# ===== Cell 4: Code (execution_count: 23) =====
# ==============================================================================
# Ê≠•È™§ 4: Âä®ÊÄÅÈÄâÊã©ÊµãËØïÊ†∑Êú¨Âπ∂ÂáÜÂ§áËæìÂá∫ÁõÆÂΩï
# ==============================================================================
logging.info("\n--- Ê≠•È™§ 4: Âä®ÊÄÅÈÄâÊã©ÊµãËØïÊ†∑Êú¨ ---")

sample_counts = nodes_df['sample_id'].value_counts().to_frame(name='count')
logging.info("Ê†∑Êú¨IDÂèäÂÖ∂ÂØπÂ∫îÁöÑÁì¶ÁâáÊï∞Èáè:")
print(sample_counts)

# ÈÄâÊã©Áì¶ÁâáÊï∞ÊúÄÂ∞ëÁöÑÊ†∑Êú¨ÂíåÁì¶ÁâáÊï∞‰Ωç‰∫é‰∏≠‰ΩçÊï∞ÁöÑÊ†∑Êú¨
smallest_sample_id = sample_counts.index[-1]
median_index = len(sample_counts) // 2
median_sample_id = sample_counts.index[median_index]
SAMPLE_IDS_TO_USE = [smallest_sample_id, median_sample_id]

logging.info(f"Â∑≤ÈÄâÊã©Áì¶ÁâáÊï∞ÊúÄÂ∞ëÁöÑÊ†∑Êú¨: '{smallest_sample_id}' ({sample_counts.loc[smallest_sample_id, 'count']} tiles)")
logging.info(f"Â∑≤ÈÄâÊã©Áì¶ÁâáÊï∞‰∏≠‰ΩçÊï∞ÁöÑÊ†∑Êú¨: '{median_sample_id}' ({sample_counts.loc[median_sample_id, 'count']} tiles)")
logging.info(f"ÊúÄÁªàÁî®‰∫éÂÜíÁÉüÊµãËØïÁöÑÊ†∑Êú¨ID: {SAMPLE_IDS_TO_USE}")

# --- ÂáÜÂ§áËæìÂá∫ÁõÆÂΩï ---
if SMOKE_TEST_DIR.exists():
    logging.warning(f"ËæìÂá∫ÁõÆÂΩï {SMOKE_TEST_DIR} Â∑≤Â≠òÂú®ÔºåÂ∞ÜÂÆåÂÖ®Âà†Èô§Âπ∂ÈáçÂª∫„ÄÇ")
    shutil.rmtree(SMOKE_TEST_DIR)
SMOKE_TEST_DIR.mkdir(parents=True)
logging.info(f"Â∑≤ÂàõÂª∫Á©∫ÁöÑËæìÂá∫ÁõÆÂΩï: {SMOKE_TEST_DIR}")



# ===== Cell 5: Code (execution_count: 24) =====

# ==============================================================================
# Ê≠•È™§ 5: ÊâßË°åÊï∞ÊçÆÂ≠êÈõÜÂàáÁâá‰∏é ID ÈáçÊò†Â∞Ñ (Ê†∏ÂøÉÈÄªËæë)
# ==============================================================================
logging.info(f"\n--- Ê≠•È™§ 5: Ê≠£Âú®Á≠õÈÄâÊ†∑Êú¨ ID: {SAMPLE_IDS_TO_USE} ---")
smoke_nodes_df = nodes_df[nodes_df['sample_id'].isin(SAMPLE_IDS_TO_USE)].copy()

if smoke_nodes_df.empty:
    raise ValueError(f"‚ùå ÈîôËØØ: Âú®Êï∞ÊçÆÈõÜ‰∏≠Êú™ÊâæÂà∞ÊåáÂÆöÁöÑ Sample ID {SAMPLE_IDS_TO_USE}„ÄÇ")

# CodeGuardian: ÂÖ≥ÈîÆ‰øÆÂ§çÁÇπ„ÄÇÊàë‰ª¨‰ΩøÁî®Á≠õÈÄâÂêé DataFrame ÁöÑ `.index`Ôºå
# ÂÆÉ‰ª£Ë°®‰∫ÜËøô‰∫õË°åÂú®ÂéüÂßã `nodes_df` Âíå `.npy` Êñá‰ª∂‰∏≠ÁöÑÁúüÂÆûË°åÂè∑„ÄÇ
original_row_indices = smoke_nodes_df.index.to_numpy()

smoke_img_embeds = img_embeds[original_row_indices]
smoke_txt_embeds = txt_embeds[original_row_indices]
logging.info(f"Â∑≤Ê†πÊçÆË°åÂè∑Ê≠£Á°ÆÁ≠õÈÄâÂá∫ {len(smoke_nodes_df)} ‰∏™ËäÇÁÇπÂèäÂÖ∂ÂØπÂ∫îÁöÑÂµåÂÖ•„ÄÇ")

# ÈáçÊò†Â∞Ñ Tile ID
smoke_nodes_df = smoke_nodes_df.rename(columns={'tile_id': 'original_tile_id'})
smoke_nodes_df.reset_index(drop=True, inplace=True)
smoke_nodes_df['tile_id'] = smoke_nodes_df.index
old_to_new_tile_id_map = pd.Series(smoke_nodes_df.index, index=smoke_nodes_df['original_tile_id']).to_dict()
logging.info("Â∑≤Â∞ÜÁ®ÄÁñèÁöÑ tile_id ÈáçÊò†Â∞Ñ‰∏∫‰ªé 0 ÂºÄÂßãÁöÑËøûÁª≠Á¥¢Âºï„ÄÇ")

# Á≠õÈÄâÂπ∂ÈáçÊò†Â∞Ñ Edges
valid_original_tile_ids = set(old_to_new_tile_id_map.keys())
smoke_edges_df = edges_df[
    edges_df['src_tile_id'].isin(valid_original_tile_ids) & 
    edges_df['nbr_tile_id'].isin(valid_original_tile_ids)
].copy()
smoke_edges_df['src_tile_id'] = smoke_edges_df['src_tile_id'].map(old_to_new_tile_id_map)
smoke_edges_df['nbr_tile_id'] = smoke_edges_df['nbr_tile_id'].map(old_to_new_tile_id_map)
logging.info(f"Â∑≤Á≠õÈÄâÂá∫ {len(smoke_edges_df)} Êù°ÂÜÖÈÉ®ËæπÔºåÂπ∂Êõ¥Êñ∞‰∫ÜÂÖ∂ ID„ÄÇ")

# ‰øùÂ≠ò
logging.info("\n--- Ê≠£Âú®‰øùÂ≠òÂÜíÁÉüÊµãËØïÊï∞ÊçÆÈõÜ... ---")
final_nodes_to_save = smoke_nodes_df.drop(columns=['original_tile_id'])
pq.write_table(pa.Table.from_pandas(final_nodes_to_save, preserve_index=False), SMOKE_TEST_DIR / "nodes.parquet")
pq.write_table(pa.Table.from_pandas(smoke_edges_df, preserve_index=False), SMOKE_TEST_DIR / "edges.parquet")
np.save(SMOKE_TEST_DIR / "image_embeds.npy", smoke_img_embeds)
np.save(SMOKE_TEST_DIR / "text_embeds.npy", smoke_txt_embeds)
logging.info(f"‚úÖ ÊâÄÊúâÊñá‰ª∂Â∑≤ÊàêÂäü‰øùÂ≠òËá≥: {SMOKE_TEST_DIR}")



# ===== Cell 6: Markdown =====
# Validation

# ===== Cell 7: Code =====

# ==============================================================================
# Ê≠•È™§ 6: È™åËØÅÊñ∞ÁîüÊàêÁöÑÂÜíÁÉüÊµãËØïÈõÜ
# ==============================================================================
logging.info("\n--- Ê≠•È™§ 6: Ê≠£Âú®ÂØπÊñ∞ÁîüÊàêÁöÑÂÜíÁÉüÊµãËØïÊñá‰ª∂ËøõË°åÂÖ®Èù¢È™åËØÅ ---")
try:
    final_nodes = pd.read_parquet(SMOKE_TEST_DIR / "nodes.parquet")
    final_edges = pd.read_parquet(SMOKE_TEST_DIR / "edges.parquet")
    final_img_embeds = np.load(SMOKE_TEST_DIR / "image_embeds.npy")

    num_final_nodes = len(final_nodes)
    logging.info(f"ÂÜíÁÉüÊµãËØïÈõÜÂåÖÂê´ {num_final_nodes} ‰∏™ËäÇÁÇπ„ÄÇ")

    assert num_final_nodes == final_img_embeds.shape[0], "ËäÇÁÇπÊï∞‰∏éÂõæÂÉèÂµåÂÖ•Êï∞‰∏çÂåπÈÖçÔºÅ"
    logging.info("‚úÖ È™åËØÅÈÄöËøá: ËäÇÁÇπÊï∞‰∏éÂµåÂÖ•Áª¥Â∫¶‰∏ÄËá¥„ÄÇ")

    assert final_nodes['tile_id'].min() == 0, "ÊúÄÂ∞è tile_id ‰∏ç‰∏∫ 0ÔºÅ"
    assert final_nodes['tile_id'].max() == num_final_nodes - 1, "ÊúÄÂ§ß tile_id ‰∏çÊòØ N-1ÔºÅ"
    assert final_nodes['tile_id'].is_unique, "Tile ID ‰∏çÂîØ‰∏ÄÔºÅ"
    logging.info("‚úÖ È™åËØÅÈÄöËøá: Tile ID ÊòØ‰ªé 0 Âà∞ N-1 ÁöÑËøûÁª≠ÂîØ‰∏ÄÊï¥Êï∞„ÄÇ")

    if not final_edges.empty:
        valid_node_ids = set(final_nodes['tile_id'])
        assert final_edges['src_tile_id'].isin(valid_node_ids).all(), "ËæπË°®‰∏≠Â≠òÂú®Êó†ÊïàÁöÑÊ∫êËäÇÁÇπ IDÔºÅ"
        assert final_edges['nbr_tile_id'].isin(valid_node_ids).all(), "ËæπË°®‰∏≠Â≠òÂú®Êó†ÊïàÁöÑÁõÆÊ†áËäÇÁÇπ IDÔºÅ"
        logging.info("‚úÖ È™åËØÅÈÄöËøá: ÊâÄÊúâËæπÈÉΩËøûÊé•ÁùÄÊúâÊïàÁöÑËäÇÁÇπ„ÄÇ")
    else:
        logging.warning("‚ö†Ô∏è ËæπË°®‰∏∫Á©∫ÔºåË∑≥ËøáÂºïÁî®ÂÆåÊï¥ÊÄßÊ£ÄÊü•„ÄÇ")

    print("\n" + "="*50)
    logging.info("üéâ È™åËØÅÊàêÂäüÔºÅÂÜíÁÉüÊµãËØïÊï∞ÊçÆÈõÜÂ∑≤ÂèØÁî®‰∫éËÆ≠ÁªÉ„ÄÇ")
    print("="*50)
except Exception as e:
    logging.error(f"‚ùå È™åËØÅÂ§±Ë¥•ÔºÅÈîôËØØ: {e}", exc_info=True)



# ===== Cell 8: Code (execution_count: 26) =====
# ===== Cell 4: Visual Reconstruction and Validation (Multi-Sample SOTA Version) =====
# CodeGuardian: This cell provides the ultimate "proof" of dataset integrity.
# It now correctly handles multiple sample_ids by reconstructing each one on its own canvas,
# confirming that the spatial information for every sample in the subset is valid.

import matplotlib.pyplot as plt
from PIL import Image
from tqdm.notebook import tqdm
import pandas as pd
from pathlib import Path
import logging

def reconstruct_slide_from_nodes_df(nodes_df: pd.DataFrame, sample_id_for_logging: str):
    """
    Reads a pandas DataFrame containing nodes for a SINGLE sample_id 
    and reconstructs its spatial arrangement of image tiles.

    Args:
        nodes_df: A DataFrame filtered to contain only nodes for one sample_id.
        sample_id_for_logging: The sample_id being processed, for clear logging.

    Returns:
        A PIL.Image object of the reconstructed slide, or None if an error occurs.
    """
    if nodes_df.empty:
        logging.warning(f"‚ö†Ô∏è DataFrame for sample '{sample_id_for_logging}' is empty. Cannot reconstruct.")
        return None

    # 1. Infer Tile Size from the first image
    try:
        first_image_path = nodes_df.iloc[0]['image_path']
        with Image.open(first_image_path) as first_tile:
            tile_w, tile_h = first_tile.size
        logging.info(f"[{sample_id_for_logging}] Inferred tile size: {tile_w}x{tile_h} pixels.")
    except Exception as e:
        logging.error(f"‚ùå [{sample_id_for_logging}] Could not open the first image tile to infer size: {e}")
        return None
        
    # 2. Calculate the bounding box for THIS SPECIFIC SAMPLE
    min_coord_x, max_coord_x = nodes_df['x'].min(), nodes_df['x'].max()
    min_coord_y, max_coord_y = nodes_df['y'].min(), nodes_df['y'].max()
    canvas_width = (max_coord_x - min_coord_x) + tile_w
    canvas_height = (max_coord_y - min_coord_y) + tile_h
    logging.info(f"[{sample_id_for_logging}] Calculated canvas size: {canvas_width}x{canvas_height} pixels.")
    
    canvas = Image.new('RGB', (canvas_width, canvas_height), 'black')
    
    # 3. Iterate and paste tiles for THIS SPECIFIC SAMPLE
    logging.info(f"[{sample_id_for_logging}] Reconstructing image from {len(nodes_df)} tiles...")
    for _, row in tqdm(nodes_df.iterrows(), total=len(nodes_df), desc=f"Pasting '{sample_id_for_logging}'"):
        try:
            with Image.open(row['image_path']) as tile:
                center_x, center_y = row['x'], row['y']
                top_left_x_abs = center_x - tile_w // 2
                top_left_y_abs = center_y - tile_h // 2
                paste_x = top_left_x_abs - min_coord_x
                paste_y = top_left_y_abs - min_coord_y
                canvas.paste(tile, (paste_x, paste_y))
        except FileNotFoundError:
            logging.warning(f"Skipping tile: image file not found at {row['image_path']}")
        except Exception as e:
            logging.warning(f"Skipping tile for sample '{sample_id_for_logging}' due to an error: {e}")
            
    return canvas

# --- Main Execution Block for Multi-Sample Reconstruction ---

# 1. Load the nodes data from the newly created smoke test set
nodes_file_path = SMOKE_TEST_DIR / "nodes.parquet"
if not nodes_file_path.exists():
    logging.error(f"‚ùå Nodes file not found at: {nodes_file_path}. Cannot perform reconstruction.")
else:
    final_nodes_df = pd.read_parquet(nodes_file_path)
    
    # 2. Identify all unique sample_ids in the smoke test set
    unique_sample_ids = final_nodes_df['sample_id'].unique()
    logging.info(f"\nFound {len(unique_sample_ids)} unique sample(s) in the smoke test set: {list(unique_sample_ids)}")
    
    reconstructed_images = []
    # 3. Loop through each sample_id and reconstruct it individually
    for sample_id in unique_sample_ids:
        logging.info(f"\n--- Processing reconstruction for sample: '{sample_id}' ---")
        # Filter the DataFrame for the current sample
        sample_df = final_nodes_df[final_nodes_df['sample_id'] == sample_id]
        
        # Call the reconstruction function on the filtered DataFrame
        reconstructed_image = reconstruct_slide_from_nodes_df(sample_df, sample_id)
        if reconstructed_image:
            reconstructed_images.append({'id': sample_id, 'image': reconstructed_image})

    # 4. Display all reconstructed images in a single plot for comparison
    if reconstructed_images:
        num_images = len(reconstructed_images)
        fig, axes = plt.subplots(1, num_images, figsize=(8 * num_images, 8), squeeze=False)
        fig.suptitle("Visual Reconstruction of All Samples in Smoke Test Dataset", fontsize=20, y=1.02)
        
        for i, result in enumerate(reconstructed_images):
            ax = axes[0, i]
            ax.imshow(result['image'])
            ax.set_title(f"Sample ID: {result['id']}", fontsize=14)
            ax.grid(False)
            ax.set_xlabel("Relative X Coordinate")
            ax.set_ylabel("Relative Y Coordinate")
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show()
    else:
        logging.error("‚ùå Failed to generate any reconstructed images.")

# ===== Cell 9: Code =====


# ===== Cell 10: Code =====



===== configs/train.yaml =====
# configs/train.yaml

# CodeGuardian: ËøôÊòØÈ°πÁõÆÁöÑ‚ÄúÊÄªÈÖçÊñπ‚Äù„ÄÇÂÆÉÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂÆåÊï¥ÂÆûÈ™åÊâÄÈúÄÁöÑÊâÄÊúâÁªÑ‰ª∂ÁöÑÈªòËÆ§Êù•Ê∫ê„ÄÇ
# _self_ ÂøÖÈ°ªÊîæÂú®Á¨¨‰∏Ä‰ΩçÔºåËøôÊòØ Hydra ÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ
defaults:
  - _self_
  - data: spatial          # ÈªòËÆ§‰ΩøÁî® spatial Êï∞ÊçÆ
  - model: spatial_clip    # ÈªòËÆ§‰ΩøÁî® spatial_clip Ê®°Âûã
  - loss: spatial          # ÈªòËÆ§‰ΩøÁî® spatial loss ÂáΩÊï∞
  - optimizer: adamw       # ÈªòËÆ§‰ΩøÁî® adamw ‰ºòÂåñÂô®
  - scheduler: cosine      # ÈªòËÆ§‰ΩøÁî® cosine Ë∞ÉÂ∫¶Âô®
  - callbacks: default     # ÈªòËÆ§‰ΩøÁî® default ÂõûË∞É
  - logger: aim            # ÈªòËÆ§‰ΩøÁî® aim Êó•ÂøóËÆ∞ÂΩïÂô®
  - trainer: default       # ÈªòËÆ§‰ΩøÁî® default ËÆ≠ÁªÉÂô®ÈÖçÁΩÆ
  - paths: default
  - extras: default
  - hydra: default

  # --- ÂèØÈÄâÁöÑË¶ÜÁõñÁªÑ ---
  # ÂÆûÈ™åÈÖçÁΩÆÂ∞ÜË¶ÜÁõñ‰∏äËø∞ÈªòËÆ§ÂÄº„ÄÇ`null` ÊòØ‰∏Ä‰∏™Âç†‰ΩçÁ¨¶ÔºåÂèØ‰ª•ÈÄöËøáÂëΩ‰ª§Ë°åÂ°´ÂÖÖ„ÄÇ
  - experiment: null
  
  # Áî®‰∫éË∂ÖÂèÇÊï∞ÊêúÁ¥¢
  - optional hparams_search: null
  
  # Áî®‰∫éÊú¨Âú∞Êú∫Âô®ÁâπÂÆöËÆæÁΩÆÔºàÊ≠§Êñá‰ª∂‰∏çÂú®ÁâàÊú¨ÊéßÂà∂‰∏≠Ôºâ
  - optional local: default
  
  # Áî®‰∫éË∞ÉËØï
  - debug: null

# --- ÂÖ®Â±ÄÂèÇÊï∞ ---
task_name: "train"
tags: ["dev"]
name: "default_run"  # ÈªòËÆ§ÁöÑËøêË°åÂêçÁß∞

train: True
test: True  # Âú®ËÆ≠ÁªÉÂêéËøêË°åÊµãËØï

ckpt_path: null

seed: 42

optimized_metric: "val/loss" # Áî®‰∫éË∂ÖÂèÇÊï∞ÊêúÁ¥¢ÁöÑÁõÆÊ†áÊåáÊ†á
===== configs/eval.yaml =====
# @package _global_

defaults:
  - _self_
  - data: mnist # choose datamodule with `test_dataloader()` for evaluation
  - model: mnist
  - logger: null
  - trainer: default
  - paths: default
  - extras: default
  - hydra: default

task_name: "eval"

tags: ["dev"]

# passing checkpoint path is necessary for evaluation
ckpt_path: ???

===== configs/__init__.py =====
# this file is needed here to include configs when building project as a package

===== configs/test_datamodule.yaml =====
# configs/test_datamodule.yaml

# Êàë‰ª¨Âè™ÂÖ≥ÂøÉ 'data' ÈÉ®ÂàÜÁöÑÈÖçÁΩÆÔºåÊâÄ‰ª•Âè™ÂåÖÂê´ÂÆÉ
defaults:
  - _self_
  - data: spatial # <--- Áõ¥Êé•ÂºïÁî®Êàë‰ª¨‰∏∫ DataModule ÂàõÂª∫ÁöÑÈÖçÁΩÆ

# Ê∑ªÂä† paths ÈÖçÁΩÆÔºåÂõ†‰∏∫ DataModule ÁöÑÈÖçÁΩÆ‰∏≠ÂèØËÉΩ‰ºöÁî®Âà∞
# ÊØîÂ¶Ç ${paths.data_dir} ËøôÊ†∑ÁöÑÂèòÈáè
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
===== configs/experiment/spatial_v1.yaml =====
# configs/experiment/spatial_v1.yaml
# @package _global_

# CodeGuardian: ÂÆûÈ™åÈÖçÁΩÆÊñá‰ª∂ÁöÑÂîØ‰∏ÄËÅåË¥£ÊòØ‚ÄúË¶ÜÁõñ‚ÄùÈªòËÆ§ÂÄº„ÄÇ
# ÂÆÉ‰∏çÂåÖÂê´Ëá™Â∑±ÁöÑ `defaults` ÂàóË°®„ÄÇ
# ÁªìÊûÑÊ∏ÖÊô∞Ôºå‰∏ÄÁõÆ‰∫ÜÁÑ∂Âú∞Â±ïÁ§∫‰∫ÜÊú¨Ê¨°ÂÆûÈ™å‰∏éÈªòËÆ§ÈÖçÁΩÆÁöÑÊâÄÊúâ‰∏çÂêå‰πãÂ§Ñ„ÄÇ

# --- ÂÆûÈ™åÂÖÉÊï∞ÊçÆ ---
tags: ["Spatial-CLIP", "ViT-B-32", "Spatial-Loss", "Baseline"]
name: "spatial_v1_baseline_run_1" # Aim/WandB ‰∏≠ÁöÑËøêË°åÂêçÁß∞

# --- ÈíàÂØπÊú¨Ê¨°ÂÆûÈ™åÁöÑÂèÇÊï∞Ë¶ÜÁõñ ---
# Áõ¥Êé•ÊåáÂÆöË¶ÅË¶ÜÁõñÁöÑÁªÑ‰ª∂ÂèäÂÖ∂ÂèÇÊï∞

trainer:
  devices: 1
  max_epochs: 100

data:
  batch_size: 1500
  num_workers: 16

optimizer:
  lr: 1e-4

scheduler:
  num_warmup_steps: 1000

# Â¶ÇÊûúÈúÄË¶ÅË¶ÜÁõñ logger ÁöÑÁâπÂÆöÂèÇÊï∞Ôºå‰πüÂèØ‰ª•Âú®ËøôÈáåËøõË°å
# logger:
#   aim:
#     experiment: "Spatial-CLIP-High-Batch-Size"
===== configs/experiment/spatial_v2_multi_gpu.yaml =====
# configs/experiment/spatial_v2.yaml
# @package _global_

# CodeGuardian: ÂÆûÈ™åÈÖçÁΩÆÊñá‰ª∂ÁöÑÂîØ‰∏ÄËÅåË¥£ÊòØ‚ÄúË¶ÜÁõñ‚ÄùÈªòËÆ§ÂÄº„ÄÇ
# ÂÆÉ‰∏çÂåÖÂê´Ëá™Â∑±ÁöÑ `defaults` ÂàóË°®„ÄÇ
# ÁªìÊûÑÊ∏ÖÊô∞Ôºå‰∏ÄÁõÆ‰∫ÜÁÑ∂Âú∞Â±ïÁ§∫‰∫ÜÊú¨Ê¨°ÂÆûÈ™å‰∏éÈªòËÆ§ÈÖçÁΩÆÁöÑÊâÄÊúâ‰∏çÂêå‰πãÂ§Ñ„ÄÇ

# --- ÂÆûÈ™åÂÖÉÊï∞ÊçÆ ---
tags: ["Spatial-CLIP", "ViT-B-32", "Spatial-Loss", "Baseline"]
name: "spatial_v1_baseline_run_1" # Aim/WandB ‰∏≠ÁöÑËøêË°åÂêçÁß∞

# --- ÈíàÂØπÊú¨Ê¨°ÂÆûÈ™åÁöÑÂèÇÊï∞Ë¶ÜÁõñ ---
# Áõ¥Êé•ÊåáÂÆöË¶ÅË¶ÜÁõñÁöÑÁªÑ‰ª∂ÂèäÂÖ∂ÂèÇÊï∞

trainer:
  devices: 2
  max_epochs: 100

data:
  batch_size: 1500
  num_workers: 16

optimizer:
  lr: 1e-4

scheduler:
  num_warmup_steps: 1000

# Â¶ÇÊûúÈúÄË¶ÅË¶ÜÁõñ logger ÁöÑÁâπÂÆöÂèÇÊï∞Ôºå‰πüÂèØ‰ª•Âú®ËøôÈáåËøõË°å
# logger:
#   aim:
#     experiment: "Spatial-CLIP-High-Batch-Size"
===== configs/experiment/spatial_v3_multigpu_moreepochs.yaml =====
# @package _global_

# --- ÂÆûÈ™åÂÖÉÊï∞ÊçÆ ---
tags: ["Spatial-CLIP", "ViT-B-32", "Spatial-Loss", "Baseline"]
name: "spatial_v1_baseline_run_1"

# --- ÈíàÂØπÊú¨Ê¨°ÂÆûÈ™åÁöÑÂèÇÊï∞Ë¶ÜÁõñ ---
trainer:
  devices: 2
  max_epochs: 10
  limit_train_batches: 0.1

data:
  batch_size: 1500
  num_workers: 16

optimizer:
  lr: 1e-4

scheduler:
  num_warmup_steps: 1000

# The 'logger: csv' line has been REMOVED.
===== configs/logger/csv.yaml =====
# csv logger built in lightning

csv:
  _target_: lightning.pytorch.loggers.csv_logs.CSVLogger
  save_dir: "${paths.output_dir}"
  name: "csv/"
  prefix: ""

===== configs/logger/aim.yaml =====
# configs/logger/aim.yaml
_target_: lightning.pytorch.loggers.AimLogger

experiment: "Spatial-CLIP-Experiments" # Aim UI ‰∏≠ÊòæÁ§∫ÁöÑÈ°πÁõÆÂêçÁß∞
name: ${name} # ÊØè‰∏™ËøêË°åÁöÑÂêçÁß∞ÔºåÂ∞Ü‰ªé‰∏ªÈÖçÁΩÆ‰∏≠Âä®ÊÄÅ‰º†ÂÖ•
repo: ${paths.log_dir}/aim # Aim Êï∞ÊçÆÂ∫ìÁöÑÂ≠òÂÇ®‰ΩçÁΩÆ
===== configs/logger/neptune.yaml =====
# https://neptune.ai

neptune:
  _target_: lightning.pytorch.loggers.neptune.NeptuneLogger
  api_key: ${oc.env:NEPTUNE_API_TOKEN} # api key is loaded from environment variable
  project: username/lightning-hydra-template
  # name: ""
  log_model_checkpoints: True
  prefix: ""

===== configs/logger/wandb.yaml =====
# https://wandb.ai

wandb:
  _target_: lightning.pytorch.loggers.wandb.WandbLogger
  # name: "" # name of the run (normally generated by wandb)
  save_dir: "${paths.output_dir}"
  offline: False
  id: null # pass correct id to resume experiment!
  anonymous: null # enable anonymous logging
  project: "lightning-hydra-template"
  log_model: False # upload lightning ckpts
  prefix: "" # a string to put at the beginning of metric keys
  # entity: "" # set to name of your wandb team
  group: ""
  tags: []
  job_type: ""

===== configs/logger/mlflow.yaml =====
# https://mlflow.org

mlflow:
  _target_: lightning.pytorch.loggers.mlflow.MLFlowLogger
  # experiment_name: ""
  # run_name: ""
  tracking_uri: ${paths.log_dir}/mlflow/mlruns # run `mlflow ui` command inside the `logs/mlflow/` dir to open the UI
  tags: null
  # save_dir: "./mlruns"
  prefix: ""
  artifact_location: null
  # run_id: ""

===== configs/logger/tensorboard.yaml =====
# https://www.tensorflow.org/tensorboard/

tensorboard:
  _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
  save_dir: "${paths.output_dir}/tensorboard/"
  name: null
  log_graph: False
  default_hp_metric: True
  prefix: ""
  # version: ""

===== configs/logger/comet.yaml =====
# https://www.comet.ml

comet:
  _target_: lightning.pytorch.loggers.comet.CometLogger
  api_key: ${oc.env:COMET_API_TOKEN} # api key is loaded from environment variable
  save_dir: "${paths.output_dir}"
  project_name: "lightning-hydra-template"
  rest_api_key: null
  # experiment_name: ""
  experiment_key: null # set to resume experiment
  offline: False
  prefix: ""

===== configs/logger/many_loggers.yaml =====
# train with many loggers at once

defaults:
  # - comet
  - csv
  # - mlflow
  # - neptune
  - tensorboard
  - wandb

===== configs/paths/default.yaml =====
# path to root directory
# this requires PROJECT_ROOT environment variable to exist
# you can replace it with "." if you want the root to be the current working directory
root_dir: ${oc.env:PROJECT_ROOT}

# path to data directory
data_dir: ${paths.root_dir}/data/
assets_dir: ${paths.root_dir}/assets/
# path to logging directory
log_dir: ${paths.root_dir}/logs/

# path to output directory, created dynamically by hydra
# path generation pattern is specified in `configs/hydra/default.yaml`
# use it to store all files generated during the run, like ckpts and metrics
output_dir: ${hydra:runtime.output_dir}

# path to working directory
work_dir: ${hydra:runtime.cwd}

===== configs/hydra/default.yaml =====
# https://hydra.cc/docs/configure_hydra/intro/

# enable color logging
defaults:
  - override hydra_logging: colorlog
  - override job_logging: colorlog

# output directory, generated dynamically on each run
run:
  dir: ${paths.log_dir}/${task_name}/runs/${now:%Y-%m-%d}_${now:%H-%M-%S}
sweep:
  dir: ${paths.log_dir}/${task_name}/multiruns/${now:%Y-%m-%d}_${now:%H-%M-%S}
  subdir: ${hydra.job.num}

job_logging:
  handlers:
    file:
      # Incorporates fix from https://github.com/facebookresearch/hydra/pull/2242
      filename: ${hydra.runtime.output_dir}/${task_name}.log

===== configs/scheduler/cosine.yaml =====
# ‰ΩøÁî® huggingface transformers ÁöÑË∞ÉÂ∫¶Âô®ÔºåÂäüËÉΩÊõ¥Âº∫Â§ß
_target_: transformers.get_cosine_schedule_with_warmup
_partial_: true
num_warmup_steps: 2000 
# num_training_steps Â∞ÜÂú®‰ª£Á†Å‰∏≠Âä®ÊÄÅËÆæÁΩÆ
===== configs/callbacks/default.yaml =====
defaults:
  - model_checkpoint
  - early_stopping
  - model_summary
  - rich_progress_bar
  - _self_

model_checkpoint:
  dirpath: ${paths.output_dir}/checkpoints
  filename: "epoch_{epoch:03d}"
  monitor: "val/R@1"
  mode: "max"
  save_last: True
  auto_insert_metric_name: False

early_stopping:
  monitor: "val/R@1"
  patience: 100
  mode: "max"

model_summary:
  max_depth: -1

===== configs/callbacks/early_stopping.yaml =====
# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html

early_stopping:
  _target_: lightning.pytorch.callbacks.EarlyStopping
  monitor: ??? # quantity to be monitored, must be specified !!!
  min_delta: 0. # minimum change in the monitored quantity to qualify as an improvement
  patience: 3 # number of checks with no improvement after which training will be stopped
  verbose: False # verbosity mode
  mode: "min" # "max" means higher metric value is better, can be also "min"
  strict: True # whether to crash the training if monitor is not found in the validation metrics
  check_finite: True # when set True, stops training when the monitor becomes NaN or infinite
  stopping_threshold: null # stop training immediately once the monitored quantity reaches this threshold
  divergence_threshold: null # stop training as soon as the monitored quantity becomes worse than this threshold
  check_on_train_epoch_end: null # whether to run early stopping at the end of the training epoch
  # log_rank_zero_only: False  # this keyword argument isn't available in stable version

===== configs/callbacks/rich_progress_bar.yaml =====
# https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.RichProgressBar.html

rich_progress_bar:
  _target_: lightning.pytorch.callbacks.RichProgressBar

===== configs/callbacks/model_checkpoint.yaml =====
# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html

model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: null # directory to save the model file
  filename: null # checkpoint filename
  monitor: null # name of the logged metric which determines when model is improving
  verbose: False # verbosity mode
  save_last: null # additionally always save an exact copy of the last checkpoint to a file last.ckpt
  save_top_k: 1 # save k best models (determined by above metric)
  mode: "min" # "max" means higher metric value is better, can be also "min"
  auto_insert_metric_name: True # when True, the checkpoints filenames will contain the metric name
  save_weights_only: False # if True, then only the model‚Äôs weights will be saved
  every_n_train_steps: null # number of training steps between checkpoints
  train_time_interval: null # checkpoints are monitored at the specified time interval
  every_n_epochs: null # number of epochs between checkpoints
  save_on_train_epoch_end: null # whether to run checkpointing at the end of the training epoch or the end of validation

===== configs/callbacks/model_summary.yaml =====
# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.RichModelSummary.html

model_summary:
  _target_: lightning.pytorch.callbacks.RichModelSummary
  max_depth: 1 # the maximum depth of layer nesting that the summary will include

===== configs/callbacks/none.yaml =====

===== configs/extras/default.yaml =====
# disable python warnings if they annoy you
ignore_warnings: False

# ask user for tags if none are provided in the config
enforce_tags: True

# pretty print config tree at the start of the run using Rich library
print_config: True

===== configs/metrics/contrastive.yaml =====
# configs/metrics/contrastive.yaml
_target_: src.models.components.metrics.ContrastiveMetrics
# The 'num_classes' and 'prefix' arguments will be passed from the model config.
===== configs/trainer/default.yaml =====
# configs/trainer/default.yaml

_target_: lightning.pytorch.Trainer


# --- ËÆ≠ÁªÉÊµÅÁ®ãÊéßÂà∂ ---
min_epochs: 1
max_epochs: 50

# --- Á°¨‰ª∂‰∏éÊÄßËÉΩ ---
# CodeGuardian: ‰ΩøÁî® "auto" ËÆ© Lightning Êô∫ËÉΩÈÄâÊã©ÊúÄ‰Ω≥ÂêéÁ´Ø
accelerator: "auto"
devices: "auto"
# Âú®ÊîØÊåÅÁöÑÁ°¨‰ª∂‰∏äÔºàÂ¶Ç A100Ôºâ‰ΩøÁî® 'bf16-mixed'ÔºåÂê¶Âàô‰ΩøÁî® "16-mixed"
precision: "bf16-mixed"

# --- ÂÅ•Â£ÆÊÄß ---
# Ê∑ªÂä†Ê¢ØÂ∫¶Ë£ÅÂâ™ÔºåÈò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏
gradient_clip_val: 1.0

# --- È™åËØÅ‰∏éÊó•Âøó ---
check_val_every_n_epoch: 1
log_every_n_steps: 50 # Â¢ûÂä†Êó•ÂøóËÆ∞ÂΩïÈ¢ëÁéá

# --- Ë∞ÉËØï‰∏éÂ§çÁé∞ÊÄß ---
# CodeGuardian: ÂÖ≥ÈîÆ‰øÆÂ§çÔºÅÂú®Ê≠§Â§ÑÂ∞Ü fast_dev_run ÂÆö‰πâ‰∏∫ trainer ÁöÑ‰∏Ä‰∏™ÂêàÊ≥ïÂèÇÊï∞„ÄÇ
# ÈªòËÆ§ÂÄº‰∏∫ falseÔºåÂÖÅËÆ∏‰ªéÂëΩ‰ª§Ë°åÈÄöËøá trainer.fast_dev_run=True ËøõË°å‚ÄúË¶ÜÁõñ‚ÄùËÄå‰∏çÊòØ‚ÄúÊ∑ªÂä†‚Äù„ÄÇ
fast_dev_run: false
limit_train_batches: 1.0  # Use 1.0 for 100% of batches
limit_val_batches: 1.0
limit_test_batches: 1.0

# for full reproducibility
deterministic: false
===== configs/trainer/gpu.yaml =====
defaults:
  - default

accelerator: gpu
devices: 1

===== configs/trainer/mps.yaml =====
defaults:
  - default

accelerator: mps
devices: 1

===== configs/trainer/cpu.yaml =====
defaults:
  - default

accelerator: cpu
devices: 1

===== configs/trainer/ddp_sim.yaml =====
defaults:
  - default

# simulate DDP on CPU, useful for debugging
accelerator: cpu
devices: 2
strategy: ddp_spawn

===== configs/trainer/ddp.yaml =====
defaults:
  - default

strategy: ddp

accelerator: gpu
devices: 4
num_nodes: 1
sync_batchnorm: True

===== configs/preprocess/default.yaml =====
# ÁªßÊâøÈ°πÁõÆÁ∫ßÁöÑË∑ØÂæÑÈÖçÁΩÆ
defaults:
  - _self_
  - override /paths: default

# --- ËæìÂÖ•Ê∫ê ---
source:
  raw_data_dir: ${paths.data_dir}/hest_1k_original
  hest_metadata_csv: ${.raw_data_dir}/HEST_v1_1_0.csv
  hgnc_path: ${paths.assets_dir}/hgnc_complete_set.txt
  global_hvg_path: ${paths.data_dir}/derived/global_hvgs.txt

# --- ‰∏≠Èó¥‰∫ßÁâ©‰∏éÊúÄÁªàËæìÂá∫ ---
intermediate_dir: ${paths.data_dir}/processed_intermediate
output_dir: ${paths.data_dir}/hest_webdataset

# --- ÊµÅÁ®ãÂèÇÊï∞ ---
params:
  general:
    batch_key: 'sample_id'
    species_filter: "Homo sapiens"

  samples_to_exclude:
    - 'TENX15'
    - 'MEND16'
    - 'MEND15'
    - 'MEND14'
    - 'MEND13'
    - 'MEND12'
    - 'MEND11'
    - 'MEND10'
    - 'MEND9'
    - 'MEND8'
    - 'MEND7'
    - 'MEND2'
    - 'MEND1'
    - 'NCBI657'
    - 'NCBI814'

  gene_alignment:
    keep_status: ["Approved"]
    keep_locus_types: null

  # ÂÖ≥ÈîÆ‰øÆÊ≠£ÔºöËøô‰∫õQCÂèÇÊï∞Áé∞Âú®Âè™Áî®‰∫éHVGËÆ°ÁÆóÁöÑ‰∏¥Êó∂Êï∞ÊçÆÂâØÊú¨Ôºå
  # ‰∏ç‰ºöËøáÊª§ÊúÄÁªàÊï∞ÊçÆÈõÜ„ÄÇÈªòËÆ§ÂÄºËÆæ‰∏∫ÊûÅÂ∫¶ÂÆΩÊùæÔºå‰ª•‰ΩìÁé∞‚ÄúÈªòËÆ§‰∏çÁ≠õÈÄâ‚ÄùÁöÑÁ≠ñÁï•„ÄÇ
  # quality_control_for_hvg:
  #   min_genes_per_spot: 0
  #   max_pct_mt: 100.0

  #   min_spot_fraction_per_gene: 0.000

  hvg:
    n_top_genes: 5000
    flavor: 'seurat_v3_paper'

  sentence_generation:
    n_top_genes: 50

  sharding:
    max_samples_per_shard: 5000


  spatial_graph:
    backend: "squidpy"
    coord_type: "spatial"
    n_rings: 1

  # Êñ∞Â¢ûÔºöÊòéÁ°ÆÁöÑÁì¶ÁâáÁîüÊàêÂèÇÊï∞
  tiling:
    tile_size: 224


# --- ÊÄßËÉΩÊéßÂà∂ ---
performance:
  max_workers: 32
  limit_samples: -1 # -1 Ë°®Á§∫Â§ÑÁêÜÊâÄÊúâÊ†∑Êú¨
===== configs/loss/spatial.yaml =====
# configs/model/loss/spatial.yaml
_target_: src.models.components.losses.SpatialLoss

# --- CodeGuardian: These are the knobs for your SpatialLoss experiment ---
# HOW to compute the loss
local_loss: true
gather_with_grad: true
cap_logit_scale: 40.0
temp_reg_weight: 0.05
neighbor_alpha_scale: 0.5
float32_logits: true
===== configs/loss/clip.yaml =====
# configs/model/loss/clip.yaml
_target_: src.models.components.losses.ClipLoss

# --- CodeGuardian: These are the knobs for your standard ClipLoss experiment ---
# HOW to compute the loss
local_loss: true
gather_with_grad: true
cache_labels: true # Standard CLIP loss can cache labels
===== configs/optimizer/adamw.yaml =====
_target_: torch.optim.AdamW
_partial_: true  # CodeGuardian: ÂÖ≥ÈîÆÔºÅÂª∂ËøüÂÆû‰æãÂåñÔºå‰ª•‰æøÂú®‰ª£Á†Å‰∏≠‰º†ÂÖ• model.parameters()
lr: 0.001
betas: [0.9, 0.98]
eps: 1.0e-6
weight_decay: 0.1
===== configs/model/spatial_clip.yaml =====
# configs/model/spatial_clip.yaml

# CodeGuardian: CLEANED. No more internal 'defaults' list.
# The model config now only defines the model-specific components.
_target_: src.models.spatial_clip_module.SpatialClipLitModule

# --- Components (THE "WHAT" and "WITH") ---
net:
  _target_: src.models.components.spatial_clip_net.SpatialClipNet
  model_name: ViT-B-32
  pretrained: laion2b_s34b_b79k
  aug_cfg:
    _target_: open_clip.AugmentationCfg
    scale: [0.9, 1.0]
    ratio: [0.75, 1.333]
    use_timm: true
    color_jitter: 0.2

# CodeGuardian: CORRECTED. This now references the top-level `loss` config group.
# Hydra will instantiate the object defined in `configs/loss/*.yaml` and pass it
# to the 'loss_fn' argument of the LightningModule.
loss_fn: ${loss}

# --- Metrics ---
train_metrics:
  _target_: src.models.components.metrics.ContrastiveMetrics
  prefix: "train/"

val_metrics:
  _target_: src.models.components.metrics.ContrastiveMetrics
  prefix: "val/"

test_metrics:
  _target_: src.models.components.metrics.ContrastiveMetrics
  prefix: "test/"

# --- Hyperparameters (THE "HOW") ---
optimizer_cfg: ${optimizer}
scheduler_cfg: ${scheduler}
===== configs/model/net/spatial_clip_base.yaml =====
# configs/model/net/spatial_clip_base.yaml
_target_: src.models.components.spatial_clip_net.SpatialClipNet
model_name: ViT-B-32
pretrained: laion2b_s34b_b79k
===== configs/hparams_search/loss_sweep.yaml =====
# @package _global_

defaults:
  - override hydra/sweeper: basic   # Ê≠£Á°ÆËØ≠Ê≥ïÔºö‰∏çÂ∏¶ @ Âà´Âêç

hydra:
  sweeper:
    params:
      loss: spatial,clip
      trainer.max_epochs: 5
      trainer.limit_train_batches: 1
      trainer.limit_val_batches: 1
      logger: aim
      logger.aim.experiment: Loss_Function_Sweep

===== configs/debug/default.yaml =====
# @package _global_

# default debugging setup, runs 1 full epoch
# other debugging configs can inherit from this one

# overwrite task name so debugging logs are stored in separate folder
task_name: "debug"

# disable callbacks and loggers during debugging
callbacks: null
logger: null

extras:
  ignore_warnings: False
  enforce_tags: False

# sets level of all command line loggers to 'DEBUG'
# https://hydra.cc/docs/tutorials/basic/running_your_app/logging/
hydra:
  job_logging:
    root:
      level: DEBUG

  # use this to also set hydra loggers to 'DEBUG'
  # verbose: True

trainer:
  max_epochs: 1
  accelerator: cpu # debuggers don't like gpus
  devices: 1 # debuggers don't like multiprocessing
  detect_anomaly: true # raise exception if NaN or +/-inf is detected in any tensor

data:
  num_workers: 0 # debuggers don't like multiprocessing
  pin_memory: False # disable gpu memory pin

===== configs/debug/profiler.yaml =====
# @package _global_

# runs with execution time profiling

defaults:
  - default

trainer:
  max_epochs: 1
  profiler: "simple"
  # profiler: "advanced"
  # profiler: "pytorch"

===== configs/debug/fdr.yaml =====
# @package _global_

# runs 1 train, 1 validation and 1 test step

defaults:
  - default

trainer:
  fast_dev_run: true

===== configs/debug/overfit.yaml =====
# @package _global_

# overfits to 3 batches

defaults:
  - default

trainer:
  max_epochs: 20
  overfit_batches: 3

# model ckpt and early stopping need to be disabled during overfitting
callbacks: null

===== configs/debug/limit.yaml =====
# @package _global_

# uses only 1% of the training data and 5% of validation/test data

defaults:
  - default

trainer:
  max_epochs: 3
  limit_train_batches: 0.01
  limit_val_batches: 0.05
  limit_test_batches: 0.05

===== scripts/schedule.sh =====
#!/bin/bash
# Schedule execution of many runs
# Run from root folder with: bash scripts/schedule.sh

python src/train.py trainer.max_epochs=5 logger=csv

python src/train.py trainer.max_epochs=10 logger=csv

===== src/__init__.py =====

===== src/train.py =====
# Êñá‰ª∂Ë∑ØÂæÑ: src/train.py

from typing import Any, Dict, List, Optional, Tuple

import hydra
import lightning as L
import rootutils
import torch
from lightning import Callback, LightningDataModule, LightningModule, Trainer
from lightning.pytorch.loggers import Logger
from omegaconf import DictConfig

rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)
# ------------------------------------------------------------------------------------ #
# the setup_root above is equivalent to:
# - adding project root dir to PYTHONPATH
#       (so you don't need to force user to install project as a package)
#       (necessary before importing any local modules e.g. `from src import utils`)
# - setting up PROJECT_ROOT environment variable
#       (which is used as a base for paths in "configs/paths/default.yaml")
#       (this way all filepaths are the same no matter where you run the code)
# - loading environment variables from ".env" in root dir
#
# you can remove it if you:
# 1. either install project as a package or move entry files to project root dir
# 2. set `root_dir` to "." in "configs/paths/default.yaml"
#
# more info: https://github.com/ashleve/rootutils
# ------------------------------------------------------------------------------------ #

from src.utils import (
    RankedLogger,
    extras,
    get_metric_value,
    instantiate_callbacks,
    instantiate_loggers,
    log_hyperparameters,
    task_wrapper,
)

log = RankedLogger(__name__, rank_zero_only=True)


@task_wrapper
def train(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """Trains the model. Can additionally evaluate on a testset, using best weights obtained during
    training.

    This method is wrapped in optional @task_wrapper decorator, that controls the behavior during
    failure. Useful for multiruns, saving info about the crash, etc.

    :param cfg: A DictConfig configuration composed by Hydra.
    :return: A tuple with metrics and dict with all instantiated objects.
    """
    # set seed for random number generators in pytorch, numpy and python.random
    if cfg.get("seed"):
        L.seed_everything(cfg.seed, workers=True)

    log.info(f"Instantiating datamodule <{cfg.data._target_}>")
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)

    log.info(f"Instantiating model <{cfg.model._target_}>")
    model: LightningModule = hydra.utils.instantiate(cfg.model)

    # <<< CodeGuardian's Architectural Fix: The "Handshake" >>>
    # The DataModule needs the model's tokenizer and preprocess functions.
    # This is the correct place to link them, before the Trainer starts its lifecycle hooks.
    # We access them via `model.net` because the `SpatialClipNet` component holds these objects.
    log.info("Performing model-datamodule handshake for tokenizer and preprocessing functions.")
    if hasattr(model, "net") and hasattr(datamodule, "preprocess_fn"):
        datamodule.preprocess_fn = model.net.preprocess_train  # Use train transforms for training
    if hasattr(model, "net") and hasattr(datamodule, "tokenizer"):
        datamodule.tokenizer = model.net.tokenizer
    # <<< End of Fix >>>

    log.info("Instantiating callbacks...")
    callbacks: List[Callback] = instantiate_callbacks(cfg.get("callbacks"))

    log.info("Instantiating loggers...")
    logger: List[Logger] = instantiate_loggers(cfg.get("logger"))

    log.info(f"Instantiating trainer <{cfg.trainer._target_}>")
    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=logger)

    object_dict = {
        "cfg": cfg,
        "datamodule": datamodule,
        "model": model,
        "callbacks": callbacks,
        "logger": logger,
        "trainer": trainer,
    }

    if logger:
        log.info("Logging hyperparameters!")
        log_hyperparameters(object_dict)

    if cfg.get("train"):
        log.info("Starting training!")
        trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))

    train_metrics = trainer.callback_metrics

    if cfg.get("test"):
        log.info("Starting testing!")
        ckpt_path = trainer.checkpoint_callback.best_model_path
        if ckpt_path == "":
            log.warning("Best ckpt not found! Using current weights for testing...")
            ckpt_path = None
        
        #
        # CodeGuardian Note: When testing after training, it's crucial to use the validation/test transforms.
        # We need to perform another handshake to update the datamodule's preprocessor.
        if hasattr(model, "net") and hasattr(datamodule, "preprocess_fn"):
            log.info("Performing model-datamodule handshake for test-time preprocessing.")
            datamodule.preprocess_fn = model.net.preprocess_val
        #
        
        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
        log.info(f"Best ckpt path: {ckpt_path}")

    test_metrics = trainer.callback_metrics

    # merge train and test metrics
    metric_dict = {**train_metrics, **test_metrics}

    return metric_dict, object_dict


@hydra.main(version_base="1.3", config_path="../configs", config_name="train.yaml")
def main(cfg: DictConfig) -> Optional[float]:
    """Main entry point for training.

    :param cfg: DictConfig configuration composed by Hydra.
    :return: Optional[float] with optimized metric value.
    """
    # apply extra utilities
    # (e.g. ask for tags if none are provided in cfg, print cfg tree, etc.)
    extras(cfg)

    # train the model
    metric_dict, _ = train(cfg)

    # safely retrieve metric value for hydra-based hyperparameter optimization
    metric_value = get_metric_value(
        metric_dict=metric_dict, metric_name=cfg.get("optimized_metric")
    )

    # return optimized metric
    return metric_value


if __name__ == "__main__":
    main()
===== src/eval.py =====
from typing import Any, Dict, List, Tuple

import hydra
import rootutils
from lightning import LightningDataModule, LightningModule, Trainer
from lightning.pytorch.loggers import Logger
from omegaconf import DictConfig

rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)
# ------------------------------------------------------------------------------------ #
# the setup_root above is equivalent to:
# - adding project root dir to PYTHONPATH
#       (so you don't need to force user to install project as a package)
#       (necessary before importing any local modules e.g. `from src import utils`)
# - setting up PROJECT_ROOT environment variable
#       (which is used as a base for paths in "configs/paths/default.yaml")
#       (this way all filepaths are the same no matter where you run the code)
# - loading environment variables from ".env" in root dir
#
# you can remove it if you:
# 1. either install project as a package or move entry files to project root dir
# 2. set `root_dir` to "." in "configs/paths/default.yaml"
#
# more info: https://github.com/ashleve/rootutils
# ------------------------------------------------------------------------------------ #

from src.utils import (
    RankedLogger,
    extras,
    instantiate_loggers,
    log_hyperparameters,
    task_wrapper,
)

log = RankedLogger(__name__, rank_zero_only=True)


@task_wrapper
def evaluate(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """Evaluates given checkpoint on a datamodule testset.

    This method is wrapped in optional @task_wrapper decorator, that controls the behavior during
    failure. Useful for multiruns, saving info about the crash, etc.

    :param cfg: DictConfig configuration composed by Hydra.
    :return: Tuple[dict, dict] with metrics and dict with all instantiated objects.
    """
    assert cfg.ckpt_path

    log.info(f"Instantiating datamodule <{cfg.data._target_}>")
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)

    log.info(f"Instantiating model <{cfg.model._target_}>")
    model: LightningModule = hydra.utils.instantiate(cfg.model)

    log.info("Instantiating loggers...")
    logger: List[Logger] = instantiate_loggers(cfg.get("logger"))

    log.info(f"Instantiating trainer <{cfg.trainer._target_}>")
    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)

    object_dict = {
        "cfg": cfg,
        "datamodule": datamodule,
        "model": model,
        "logger": logger,
        "trainer": trainer,
    }

    if logger:
        log.info("Logging hyperparameters!")
        log_hyperparameters(object_dict)

    log.info("Starting testing!")
    trainer.test(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)

    # for predictions use trainer.predict(...)
    # predictions = trainer.predict(model=model, dataloaders=dataloaders, ckpt_path=cfg.ckpt_path)

    metric_dict = trainer.callback_metrics

    return metric_dict, object_dict


@hydra.main(version_base="1.3", config_path="../configs", config_name="eval.yaml")
def main(cfg: DictConfig) -> None:
    """Main entry point for evaluation.

    :param cfg: DictConfig configuration composed by Hydra.
    """
    # apply extra utilities
    # (e.g. ask for tags if none are provided in cfg, print cfg tree, etc.)
    extras(cfg)

    evaluate(cfg)


if __name__ == "__main__":
    main()

===== src/test_datamodule.py =====
# src/test_datamodule.py (ÊúÄÁªàÊ≠£Á°ÆÁâàÊú¨ v2)

import rootutils
rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)

import hydra
import torch
import open_clip
from omegaconf import DictConfig
from PIL import Image
import torchvision.transforms as T

def get_dummy_preprocess():
    return T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    ])

# --- Ê†∏ÂøÉ‰øÆÊîπÂú®ËøôÈáåÔºöÂä†ËΩΩÊàë‰ª¨Ëá™Â∑±ÁöÑÊµãËØïÈÖçÁΩÆ ---
@hydra.main(version_base=None, config_path="../configs", config_name="test_datamodule.yaml")
def main(cfg: DictConfig) -> None:
    # Áé∞Âú® cfg Â∞±ÊòØ‰∏Ä‰∏™Âπ≤ÂáÄÁöÑÈÖçÁΩÆÔºåÂè™ÂåÖÂê´ data Âíå paths
    print("--- Ê≠£Âú®‰ΩøÁî®‰ª•‰∏ãÊï∞ÊçÆÈÖçÁΩÆËøõË°åÊµãËØï ---")
    print(cfg.data)
    
    # ÂÆû‰æãÂåñ DataModule
    datamodule = hydra.utils.instantiate(cfg.data)
    
    # Ê®°Êãü‰ªéÊ®°ÂûãÊ®°Âùó‰º†ÂÖ•È¢ÑÂ§ÑÁêÜÂô®ÂíåÂàÜËØçÂô®
    datamodule.preprocess_fn = get_dummy_preprocess()
    datamodule.tokenizer = open_clip.get_tokenizer('ViT-B-32')
    
    # ËøêË°å setup
    datamodule.setup(stage="fit")
    
    # Ëé∑Âèñ‰∏Ä‰∏™ËÆ≠ÁªÉÊâπÊ¨°
    print("\n--- Ê≠£Âú®Ëé∑Âèñ‰∏Ä‰∏™ËÆ≠ÁªÉÊâπÊ¨°... ---")
    train_loader = datamodule.train_dataloader()
    batch = next(iter(train_loader))
    
    print("‚úÖ ÊàêÂäüËé∑Âèñ‰∏Ä‰∏™ÊâπÊ¨°ÔºÅ")
    print("--- ÊâπÊ¨°ÂÜÖÂÆπÊ£ÄÊü• ---")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            print(f"  - {key}: shape={value.shape}, dtype={value.dtype}")
        else:
            print(f"  - {key}: type={type(value)}")

    # Ê£ÄÊü•ÈÇªÂ±ÖÂ°´ÂÖÖÊòØÂê¶Ê≠£Á°Æ
    k = cfg.data.k_neighbors
    b = cfg.data.batch_size
    assert batch["neighbor_tile_ids"].shape == (b, k)
    assert batch["neighbor_alphas"].shape == (b, k)
    print(f"\n‚úÖ ÈÇªÂ±ÖÂº†ÈáèÂΩ¢Áä∂Ê≠£Á°Æ (batch_size, k) -> ({b}, {k})")

if __name__ == "__main__":
    main()
===== src/open_clip_train/spatial_data.py =====
import logging
from pathlib import Path
from typing import Any, Callable, Dict, List

import numpy as np
import pandas as pd
import torch
from PIL import Image, ImageFile
from torch.utils.data import Dataset

Image.MAX_IMAGE_PIXELS = None
ImageFile.LOAD_TRUNCATED_IMAGES = True

class SpatiallyAwareDataset(Dataset):
    _nodes_map_cache = None

    # --- ËøôÊòØÊñ∞ÁöÑ„ÄÅÈ´òÊïàÁöÑ __init__ ÊñπÊ≥ï ---
    # --- ËØ∑Áî®ÂÆÉÊõøÊç¢Êéâ spatial_data.py ‰∏≠ÁöÑÊóßÁâàÊú¨ ---
    def __init__(self, artifacts_dir: Path, k_neighbors: int, preprocess_fn: Callable, tokenizer: Callable):
        self.k = k_neighbors
        self.preprocess = preprocess_fn
        self.tokenizer = tokenizer
        logging.info(f"Initializing SpatiallyAwareDataset with k={k_neighbors}...")

        nodes_path = artifacts_dir / "nodes.parquet"
        edges_path = artifacts_dir / "edges.parquet"
        assert nodes_path.exists() and edges_path.exists(), f"Nodes or Edges file not found in {artifacts_dir}."

        self.nodes = pd.read_parquet(nodes_path)
        edges = pd.read_parquet(edges_path)
        
        # --- ‰ºòÂåñÂºÄÂßã ---
        # ËøôÊòØ‰∏Ä‰∏™Âçï‰∏ÄÁöÑ„ÄÅÂêëÈáèÂåñÁöÑÊìç‰ΩúÔºåÈÄüÂ∫¶ÊûÅÂø´
        logging.info(f"Performing vectorized top-k neighbor selection on {len(edges)} edges...")
        
        # 1. È¶ñÂÖàÊåâ alpha ÈôçÂ∫èÂØπÊï¥‰∏™ edges DataFrame ËøõË°åÊéíÂ∫è
        edges = edges.sort_values('alpha', ascending=False)
        
        # 2. ÁÑ∂ÂêéÂØπÊéíÂ∫èÂêéÁöÑÁªìÊûúËøõË°åÂàÜÁªÑÔºåÂπ∂ÂèñÊØè‰∏™ÁªÑÁöÑÂ§¥ k ‰∏™ÂÖÉÁ¥†
        top_k_edges = edges.groupby('src_tile_id').head(self.k)
        
        # 3. Áé∞Âú®Ôºå‰ªéËøô‰∏™Â∑≤ÁªèÂ§ßÂπÖÁº©Â∞èÁöÑ DataFrame È´òÊïàÂú∞ÊûÑÂª∫Â≠óÂÖ∏
        self.edges_map = {
            tile_id: group.reset_index(drop=True)
            for tile_id, group in top_k_edges.groupby('src_tile_id')
        }
        logging.info("Vectorized selection complete. Dictionary created.")
        # --- ‰ºòÂåñÁªìÊùü ---
        
        if SpatiallyAwareDataset._nodes_map_cache is None:
            logging.info("Creating and caching nodes_map for all workers...")
            SpatiallyAwareDataset._nodes_map_cache = self.nodes.set_index('tile_id')
        
        logging.info(f"Dataset ready. Total anchors: {len(self.nodes)}.")

    def __len__(self):
        return len(self.nodes)

    # --- ‰øÆÊîπ __getitem__ ---
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        anchor_info = self.nodes.iloc[idx]
        anchor_id = anchor_info['tile_id']
        
        # Âä†ËΩΩÂíåÂ§ÑÁêÜÂõæÂÉè
        anchor_image = self.preprocess(Image.open(anchor_info['image_path']).convert("RGB"))
        # ÂàÜËØçÊñáÊú¨
        anchor_text = self.tokenizer([anchor_info['gene_sentence']])[0]

        neighbors = self.edges_map.get(anchor_id, pd.DataFrame())
        
        nbr_ids = neighbors['nbr_tile_id'].tolist() if not neighbors.empty else []
        nbr_alphas = neighbors['alpha'].tolist() if not neighbors.empty else []
            
        pad_count = self.k - len(nbr_ids)
        if pad_count > 0:
            nbr_ids.extend([-1] * pad_count)
            nbr_alphas.extend([0.0] * pad_count)
            
        return {
            "image": anchor_image,
            "text": anchor_text,
            "anchor_tile_id": anchor_id,
            "neighbor_tile_ids": nbr_ids,
            "neighbor_alphas": nbr_alphas,
        }


class SpatiallyAwareCollate:
    # --- ÁßªÈô§ __init__ ‰∏≠ÁöÑ preprocess Âíå tokenizer ---
    def __init__(self, **kwargs):
        # ‰∏çÂÜçÈúÄË¶Å preprocess_fn Âíå tokenizer
        pass

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        # batch ‰∏≠ÁöÑ 'image' Âíå 'text' Â∑≤ÁªèÊòØ tensors
        images = torch.stack([item['image'] for item in batch])
        texts = torch.stack([item['text'] for item in batch])
        
        anchor_tile_ids = [item['anchor_tile_id'] for item in batch]
        neighbor_tile_ids = [item['neighbor_tile_ids'] for item in batch]
        neighbor_alphas = [item['neighbor_alphas'] for item in batch]

        return {
            "images": images,
            "texts": texts,
            "image_tile_ids": torch.tensor(anchor_tile_ids, dtype=torch.long),
            "text_tile_ids": torch.tensor(anchor_tile_ids, dtype=torch.long),
            "neighbor_tile_ids": torch.tensor(neighbor_tile_ids, dtype=torch.long),
            "neighbor_alphas": torch.tensor(neighbor_alphas, dtype=torch.float32),
        }
===== src/open_clip_train/zero_shot.py =====
import logging

import torch
from tqdm import tqdm

from open_clip import get_input_dtype, get_tokenizer, build_zero_shot_classifier, \
    IMAGENET_CLASSNAMES, OPENAI_IMAGENET_TEMPLATES
from open_clip_train.precision import get_autocast


def accuracy(output, target, topk=(1,)):
    pred = output.topk(max(topk), 1, True, True)[1].t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))
    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]


def run(model, classifier, dataloader, args):
    device = torch.device(args.device)
    autocast = get_autocast(args.precision, device_type=device.type)
    input_dtype = get_input_dtype(args.precision)

    with torch.inference_mode():
        top1, top5, n = 0., 0., 0.
        for images, target in tqdm(dataloader, unit_scale=args.batch_size):
            images = images.to(device=device, dtype=input_dtype)
            target = target.to(device)

            with autocast():
                # predict
                output = model(image=images)
                image_features = output['image_features'] if isinstance(output, dict) else output[0]
                logits = 100. * image_features @ classifier

            # measure accuracy
            acc1, acc5 = accuracy(logits, target, topk=(1, 5))
            top1 += acc1
            top5 += acc5
            n += images.size(0)

    top1 = (top1 / n)
    top5 = (top5 / n)
    return top1, top5


def zero_shot_eval(model, data, epoch, args, tokenizer=None):
    if 'imagenet-val' not in data and 'imagenet-v2' not in data:
        return {}
    if args.zeroshot_frequency == 0:
        return {}
    if (epoch % args.zeroshot_frequency) != 0 and epoch != args.epochs:
        return {}
    if args.distributed and not args.horovod:
        model = model.module

    logging.info('Starting zero-shot imagenet.')
    if tokenizer is None:
        tokenizer = get_tokenizer(args.model)

    logging.info('Building zero-shot classifier')
    device = torch.device(args.device)
    autocast = get_autocast(args.precision, device_type=device.type)
    with autocast():
        classifier = build_zero_shot_classifier(
            model,
            tokenizer=tokenizer,
            classnames=IMAGENET_CLASSNAMES,
            templates=OPENAI_IMAGENET_TEMPLATES,
            num_classes_per_batch=10,
            device=device,
            use_tqdm=True,
        )

    logging.info('Using classifier')
    results = {}
    if 'imagenet-val' in data:
        top1, top5 = run(model, classifier, data['imagenet-val'].dataloader, args)
        results['imagenet-zeroshot-val-top1'] = top1
        results['imagenet-zeroshot-val-top5'] = top5
    if 'imagenet-v2' in data:
        top1, top5 = run(model, classifier, data['imagenet-v2'].dataloader, args)
        results['imagenetv2-zeroshot-val-top1'] = top1
        results['imagenetv2-zeroshot-val-top5'] = top5

    logging.info('Finished zero-shot imagenet.')

    return results

===== src/open_clip_train/scheduler.py =====
import math


def assign_learning_rate(optimizer, new_lr):
    for param_group in optimizer.param_groups:
        param_group["lr"] = new_lr


def _warmup_lr(base_lr, warmup_length, step):
    return base_lr * (step + 1) / warmup_length


def const_lr(optimizer, base_lr, warmup_length, steps):
    def _lr_adjuster(step):
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            lr = base_lr
        assign_learning_rate(optimizer, lr)
        return lr

    return _lr_adjuster


def const_lr_cooldown(optimizer, base_lr, warmup_length, steps, cooldown_steps, cooldown_power=1.0, cooldown_end_lr=0.):
    def _lr_adjuster(step):
        start_cooldown_step = steps - cooldown_steps
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            if step < start_cooldown_step:
                lr = base_lr
            else:
                e = step - start_cooldown_step
                es = steps - start_cooldown_step
                # linear decay if power == 1; polynomial decay otherwise;
                decay = (1 - (e / es)) ** cooldown_power
                lr = decay * (base_lr - cooldown_end_lr) + cooldown_end_lr
        assign_learning_rate(optimizer, lr)
        return lr

    return _lr_adjuster


def cosine_lr(optimizer, base_lr, warmup_length, steps):
    def _lr_adjuster(step):
        if step < warmup_length:
            lr = _warmup_lr(base_lr, warmup_length, step)
        else:
            e = step - warmup_length
            es = steps - warmup_length
            lr = 0.5 * (1 + math.cos(math.pi * e / es)) * base_lr
        assign_learning_rate(optimizer, lr)
        return lr

    return _lr_adjuster


===== src/open_clip_train/data.py =====
import ast
import json
import logging
import math
import os
import random
import sys
import braceexpand
from dataclasses import dataclass
from multiprocessing import Value

import numpy as np
import pandas as pd
import torch
import torchvision.datasets as datasets
import webdataset as wds
from PIL import Image
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, IterableDataset, get_worker_info
from torch.utils.data.distributed import DistributedSampler
from webdataset.filters import _shuffle
from webdataset.tariterators import base_plus_ext, url_opener, tar_file_expander, valid_sample

try:
    import horovod.torch as hvd
except ImportError:
    hvd = None


class CsvDataset(Dataset):
    def __init__(self, input_filename, transforms, img_key, caption_key, sep="\t", tokenizer=None):
        logging.debug(f'Loading csv data from {input_filename}.')
        df = pd.read_csv(input_filename, sep=sep)

        self.images = df[img_key].tolist()
        self.captions = df[caption_key].tolist()
        self.transforms = transforms
        logging.debug('Done loading data.')

        self.tokenize = tokenizer

    def __len__(self):
        return len(self.captions)

    def __getitem__(self, idx):
        images = self.transforms(Image.open(str(self.images[idx])))
        texts = self.tokenize([str(self.captions[idx])])[0]
        return images, texts


class SharedEpoch:
    def __init__(self, epoch: int = 0):
        self.shared_epoch = Value('i', epoch)

    def set_value(self, epoch):
        self.shared_epoch.value = epoch

    def get_value(self):
        return self.shared_epoch.value


@dataclass
class DataInfo:
    dataloader: DataLoader
    sampler: DistributedSampler = None
    shared_epoch: SharedEpoch = None

    def set_epoch(self, epoch):
        if self.shared_epoch is not None:
            self.shared_epoch.set_value(epoch)
        if self.sampler is not None and isinstance(self.sampler, DistributedSampler):
            self.sampler.set_epoch(epoch)


def expand_urls(urls, weights=None):
    if weights is None:
        expanded_urls = wds.shardlists.expand_urls(urls)
        return expanded_urls, None
    if isinstance(urls, str):
        urllist = urls.split("::")
        weights = weights.split('::')
        assert len(weights) == len(urllist),\
            f"Expected the number of data components ({len(urllist)}) and weights({len(weights)}) to match."
        weights = [float(weight) for weight in weights]
        all_urls, all_weights = [], []
        for url, weight in zip(urllist, weights):
            expanded_url = list(braceexpand.braceexpand(url))
            expanded_weights = [weight for _ in expanded_url]
            all_urls.extend(expanded_url)
            all_weights.extend(expanded_weights)
        return all_urls, all_weights
    else:
        all_urls = list(urls)
        return all_urls, weights


def get_dataset_size(shards):
    shards_list, _ = expand_urls(shards)
    dir_path = os.path.dirname(shards_list[0])
    sizes_filename = os.path.join(dir_path, 'sizes.json')
    len_filename = os.path.join(dir_path, '__len__')
    if os.path.exists(sizes_filename):
        sizes = json.load(open(sizes_filename, 'r'))
        total_size = sum([int(sizes[os.path.basename(shard)]) for shard in shards_list])
    elif os.path.exists(len_filename):
        # FIXME this used to be eval(open(...)) but that seemed rather unsafe
        total_size = ast.literal_eval(open(len_filename, 'r').read())
    else:
        total_size = None  # num samples undefined
        # some common dataset sizes (at time of authors last download)
        # CC3M (train): 2905954
        # CC12M: 10968539
        # LAION-400M: 407332084
        # LAION-2B (english): 2170337258
    num_shards = len(shards_list)
    return total_size, num_shards


def get_imagenet(args, preprocess_fns, split):
    assert split in ["train", "val", "v2"]
    is_train = split == "train"
    preprocess_train, preprocess_val = preprocess_fns

    if split == "v2":
        from imagenetv2_pytorch import ImageNetV2Dataset
        dataset = ImageNetV2Dataset(location=args.imagenet_v2, transform=preprocess_val)
    else:
        if is_train:
            data_path = args.imagenet_train
            preprocess_fn = preprocess_train
        else:
            data_path = args.imagenet_val
            preprocess_fn = preprocess_val
        assert data_path

        dataset = datasets.ImageFolder(data_path, transform=preprocess_fn)

    if is_train:
        idxs = np.zeros(len(dataset.targets))
        target_array = np.array(dataset.targets)
        k = 50
        for c in range(1000):
            m = target_array == c
            n = len(idxs[m])
            arr = np.zeros(n)
            arr[:k] = 1
            np.random.shuffle(arr)
            idxs[m] = arr

        idxs = idxs.astype('int')
        sampler = SubsetRandomSampler(np.where(idxs)[0])
    else:
        sampler = None

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        num_workers=args.workers,
        sampler=sampler,
    )

    return DataInfo(dataloader=dataloader, sampler=sampler)


def count_samples(dataloader):
    os.environ["WDS_EPOCH"] = "0"
    n_elements, n_batches = 0, 0
    for images, texts in dataloader:
        n_batches += 1
        n_elements += len(images)
        assert len(images) == len(texts)
    return n_elements, n_batches


def filter_no_caption_or_no_image(sample):
    has_caption = ('txt' in sample)
    has_image = ('png' in sample or 'jpg' in sample or 'jpeg' in sample or 'webp' in sample)
    return has_caption and has_image


def log_and_continue(exn):
    """Call in an exception handler to ignore any exception, issue a warning, and continue."""
    logging.warning(f'Handling webdataset error ({repr(exn)}). Ignoring.')
    return True


def group_by_keys_nothrow(data, keys=base_plus_ext, lcase=True, suffixes=None, handler=None):
    """Return function over iterator that groups key, value pairs into samples.

    :param keys: function that splits the key into key and extension (base_plus_ext)
    :param lcase: convert suffixes to lower case (Default value = True)
    """
    current_sample = None
    for filesample in data:
        assert isinstance(filesample, dict)
        fname, value = filesample["fname"], filesample["data"]
        prefix, suffix = keys(fname)
        if prefix is None:
            continue
        if lcase:
            suffix = suffix.lower()
        # FIXME webdataset version throws if suffix in current_sample, but we have a potential for
        #  this happening in the current LAION400m dataset if a tar ends with same prefix as the next
        #  begins, rare, but can happen since prefix aren't unique across tar files in that dataset
        if current_sample is None or prefix != current_sample["__key__"] or suffix in current_sample:
            if valid_sample(current_sample):
                yield current_sample
            current_sample = dict(__key__=prefix, __url__=filesample["__url__"])
        if suffixes is None or suffix in suffixes:
            current_sample[suffix] = value
    if valid_sample(current_sample):
        yield current_sample


def tarfile_to_samples_nothrow(src, handler=log_and_continue):
    # NOTE this is a re-impl of the webdataset impl with group_by_keys that doesn't throw
    streams = url_opener(src, handler=handler)
    files = tar_file_expander(streams, handler=handler)
    samples = group_by_keys_nothrow(files, handler=handler)
    return samples


def pytorch_worker_seed(increment=0):
    """get dataloader worker seed from pytorch"""
    worker_info = get_worker_info()
    if worker_info is not None:
        # favour using the seed already created for pytorch dataloader workers if it exists
        seed = worker_info.seed
        if increment:
            # space out seed increments so they can't overlap across workers in different iterations
            seed += increment * max(1, worker_info.num_workers)
        return seed
    # fallback to wds rank based seed
    return wds.utils.pytorch_worker_seed()


_SHARD_SHUFFLE_SIZE = 2000
_SHARD_SHUFFLE_INITIAL = 500
_SAMPLE_SHUFFLE_SIZE = 5000
_SAMPLE_SHUFFLE_INITIAL = 1000


class detshuffle2(wds.PipelineStage):
    def __init__(
            self,
            bufsize=1000,
            initial=100,
            seed=0,
            epoch=-1,
    ):
        self.bufsize = bufsize
        self.initial = initial
        self.seed = seed
        self.epoch = epoch

    def run(self, src):
        if isinstance(self.epoch, SharedEpoch):
            epoch = self.epoch.get_value()
        else:
            # NOTE: this is epoch tracking is problematic in a multiprocess (dataloader workers or train)
            # situation as different workers may wrap at different times (or not at all).
            self.epoch += 1
            epoch = self.epoch
        rng = random.Random()
        if self.seed < 0:
            # If seed is negative, we use the worker's seed, this will be different across all nodes/workers
            seed = pytorch_worker_seed(epoch)
        else:
            # This seed to be deterministic AND the same across all nodes/workers in each epoch
            seed = self.seed + epoch
        rng.seed(seed)
        return _shuffle(src, self.bufsize, self.initial, rng)


class ResampledShards2(IterableDataset):
    """An iterable dataset yielding a list of urls."""

    def __init__(
        self,
        urls,
        weights=None,
        nshards=sys.maxsize,
        worker_seed=None,
        deterministic=False,
        epoch=-1,
    ):
        """Sample shards from the shard list with replacement.

        :param urls: a list of URLs as a Python list or brace notation string
        """
        super().__init__()
        urls, weights = expand_urls(urls, weights)
        self.urls = urls
        self.weights = weights
        if self.weights is not None:
            assert len(self.urls) == len(self.weights),\
                f"Number of urls {len(self.urls)} and weights {len(self.weights)} should match."
        assert isinstance(self.urls[0], str)
        self.nshards = nshards
        self.rng = random.Random()
        self.worker_seed = worker_seed
        self.deterministic = deterministic
        self.epoch = epoch

    def __iter__(self):
        """Return an iterator over the shards."""
        if isinstance(self.epoch, SharedEpoch):
            epoch = self.epoch.get_value()
        else:
            # NOTE: this is epoch tracking is problematic in a multiprocess (dataloader workers or train)
            # situation as different workers may wrap at different times (or not at all).
            self.epoch += 1
            epoch = self.epoch
        if self.deterministic:
            # reset seed w/ epoch if deterministic
            if self.worker_seed is None:
                # pytorch worker seed should be deterministic due to being init by arg.seed + rank + worker id
                seed = pytorch_worker_seed(epoch)
            else:
                seed = self.worker_seed() + epoch
            self.rng.seed(seed)
        for _ in range(self.nshards):
            if self.weights is None:
                yield dict(url=self.rng.choice(self.urls))
            else:
                yield dict(url=self.rng.choices(self.urls, weights=self.weights, k=1)[0])


def get_wds_dataset(args, preprocess_img, is_train, epoch=0, floor=False, tokenizer=None):
    input_shards = args.train_data if is_train else args.val_data
    assert input_shards is not None
    resampled = getattr(args, 'dataset_resampled', False) and is_train

    num_shards = None
    if is_train:
        if args.train_num_samples is not None:
            num_samples = args.train_num_samples
        else:
            num_samples, num_shards = get_dataset_size(input_shards)
            if not num_samples:
                raise RuntimeError(
                    'Currently, the number of dataset samples must be specified for the training dataset. '
                    'Please specify it via `--train-num-samples` if no dataset length info is present.')
    else:
        # Eval will just exhaust the iterator if the size is not specified.
        num_samples = args.val_num_samples or 0 

    shared_epoch = SharedEpoch(epoch=epoch)  # create a shared epoch store to sync epoch to dataloader worker proc

    if is_train and args.train_data_upsampling_factors is not None:
        assert resampled, "--train_data_upsampling_factors is only supported when sampling with replacement (with --dataset-resampled)."
    
    if resampled:
        pipeline = [ResampledShards2(
            input_shards,
            weights=args.train_data_upsampling_factors,
            deterministic=True,
            epoch=shared_epoch,
        )]
    else:
        pipeline = [wds.SimpleShardList(input_shards)]

    # at this point we have an iterator over all the shards
    if is_train:
        if not resampled:
            pipeline.extend([
                detshuffle2(
                    bufsize=_SHARD_SHUFFLE_SIZE,
                    initial=_SHARD_SHUFFLE_INITIAL,
                    seed=args.seed,
                    epoch=shared_epoch,
                ),
                wds.split_by_node,
                wds.split_by_worker,
            ])
        pipeline.extend([
            # at this point, we have an iterator over the shards assigned to each worker at each node
            tarfile_to_samples_nothrow,  # wds.tarfile_to_samples(handler=log_and_continue),
            wds.shuffle(
                bufsize=_SAMPLE_SHUFFLE_SIZE,
                initial=_SAMPLE_SHUFFLE_INITIAL,
            ),
        ])
    else:
        pipeline.extend([
            wds.split_by_worker,
            # at this point, we have an iterator over the shards assigned to each worker
            wds.tarfile_to_samples(handler=log_and_continue),
        ])
    pipeline.extend([
        wds.select(filter_no_caption_or_no_image),
        wds.decode("pilrgb", handler=log_and_continue),
        wds.rename(image="jpg;png;jpeg;webp", text="txt"),
        wds.map_dict(image=preprocess_img, text=lambda text: tokenizer(text)[0]),
        wds.to_tuple("image", "text"),
        wds.batched(args.batch_size, partial=not is_train)
    ])

    dataset = wds.DataPipeline(*pipeline)

    if is_train:
        if not resampled:
            num_shards = num_shards or len(expand_urls(input_shards)[0])
            assert num_shards >= args.workers * args.world_size, 'number of shards must be >= total workers'
        # roll over and repeat a few samples to get same number of full batches on each node
        round_fn = math.floor if floor else math.ceil
        global_batch_size = args.batch_size * args.world_size
        num_batches = round_fn(num_samples / global_batch_size)
        num_workers = max(1, args.workers)
        num_worker_batches = round_fn(num_batches / num_workers)  # per dataloader worker
        num_batches = num_worker_batches * num_workers
        num_samples = num_batches * global_batch_size
        dataset = dataset.with_epoch(num_worker_batches)  # each worker is iterating over this
    else:
        # last batches are partial, eval is done on single (master) node
        num_batches = math.ceil(num_samples / args.batch_size)

    dataloader = wds.WebLoader(
        dataset,
        batch_size=None,
        shuffle=False,
        num_workers=args.workers,
        persistent_workers=args.workers > 0,
    )

    # FIXME not clear which approach is better, with_epoch before vs after dataloader?
    # hoping to resolve via https://github.com/webdataset/webdataset/issues/169
    # if is_train:
    #     # roll over and repeat a few samples to get same number of full batches on each node
    #     global_batch_size = args.batch_size * args.world_size
    #     num_batches = math.ceil(num_samples / global_batch_size)
    #     num_workers = max(1, args.workers)
    #     num_batches = math.ceil(num_batches / num_workers) * num_workers
    #     num_samples = num_batches * global_batch_size
    #     dataloader = dataloader.with_epoch(num_batches)
    # else:
    #     # last batches are partial, eval is done on single (master) node
    #     num_batches = math.ceil(num_samples / args.batch_size)

    # add meta-data to dataloader instance for convenience
    dataloader.num_batches = num_batches
    dataloader.num_samples = num_samples

    return DataInfo(dataloader=dataloader, shared_epoch=shared_epoch)


def get_csv_dataset(args, preprocess_fn, is_train, epoch=0, tokenizer=None):
    input_filename = args.train_data if is_train else args.val_data
    assert input_filename
    dataset = CsvDataset(
        input_filename,
        preprocess_fn,
        img_key=args.csv_img_key,
        caption_key=args.csv_caption_key,
        sep=args.csv_separator,
        tokenizer=tokenizer
    )
    num_samples = len(dataset)
    sampler = DistributedSampler(dataset) if args.distributed and is_train else None
    shuffle = is_train and sampler is None

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=shuffle,
        num_workers=args.workers,
        pin_memory=True,
        sampler=sampler,
        drop_last=is_train,
    )
    dataloader.num_samples = num_samples
    dataloader.num_batches = len(dataloader)

    return DataInfo(dataloader, sampler)


class SyntheticDataset(Dataset):

    def __init__(
            self,
            transform=None,
            image_size=(224, 224),
            caption="Dummy caption",
            dataset_size=100,
            tokenizer=None,
    ):
        self.transform = transform
        self.image_size = image_size
        self.caption = caption
        self.image = Image.new('RGB', image_size)
        self.dataset_size = dataset_size

        self.preprocess_txt = lambda text: tokenizer(text)[0]

    def __len__(self):
        return self.dataset_size

    def __getitem__(self, idx):
        if self.transform is not None:
            image = self.transform(self.image)
        return image, self.preprocess_txt(self.caption)


def get_synthetic_dataset(args, preprocess_fn, is_train, epoch=0, tokenizer=None):
    image_size = preprocess_fn.transforms[0].size
    dataset = SyntheticDataset(
        transform=preprocess_fn, image_size=image_size, dataset_size=args.train_num_samples, tokenizer=tokenizer)
    num_samples = len(dataset)
    sampler = DistributedSampler(dataset) if args.distributed and is_train else None
    shuffle = is_train and sampler is None

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=shuffle,
        num_workers=args.workers,
        pin_memory=True,
        sampler=sampler,
        drop_last=is_train,
    )
    dataloader.num_samples = num_samples
    dataloader.num_batches = len(dataloader)

    return DataInfo(dataloader, sampler)


def get_dataset_fn(data_path, dataset_type):
    if dataset_type == "webdataset":
        return get_wds_dataset
    elif dataset_type == "csv":
        return get_csv_dataset
    elif dataset_type == "synthetic":
        return get_synthetic_dataset
    elif dataset_type == "auto":
        ext = data_path.split('.')[-1]
        if ext in ['csv', 'tsv']:
            return get_csv_dataset
        elif ext in ['tar']:
            return get_wds_dataset
        else:
            raise ValueError(
                f"Tried to figure out dataset type, but failed for extension {ext}.")
    else:
        raise ValueError(f"Unsupported dataset type: {dataset_type}")
    

def get_data(args, preprocess_fns, epoch=0, tokenizer=None):
    preprocess_train, preprocess_val = preprocess_fns
    data = {}

    if args.train_data or args.dataset_type == "synthetic":
        data["train"] = get_dataset_fn(args.train_data, args.dataset_type)(
            args, preprocess_train, is_train=True, epoch=epoch, tokenizer=tokenizer)

    if args.val_data:
        data["val"] = get_dataset_fn(args.val_data, args.dataset_type)(
            args, preprocess_val, is_train=False, tokenizer=tokenizer)

    if args.imagenet_val is not None:
        data["imagenet-val"] = get_imagenet(args, preprocess_fns, "val")

    if args.imagenet_v2 is not None:
        data["imagenet-v2"] = get_imagenet(args, preprocess_fns, "v2")

    return data

===== src/open_clip_train/precision.py =====
import torch
from contextlib import suppress
from functools import partial


def get_autocast(precision, device_type='cuda'):
    if precision =='amp':
        amp_dtype = torch.float16
    elif precision == 'amp_bfloat16' or precision == 'amp_bf16':
        amp_dtype = torch.bfloat16
    else:
        return suppress

    return partial(torch.amp.autocast, device_type=device_type, dtype=amp_dtype)
===== src/open_clip_train/__init__.py =====
# --- Âú®Êñá‰ª∂Êú´Â∞æÊ∑ªÂä†‰ª•‰∏ãÂÜÖÂÆπ ---
from .spatial_data import SpatiallyAwareDataset, SpatiallyAwareCollate
from .spatial_loss import GlobalMappingMultiPositiveClipLoss
===== src/open_clip_train/main.py =====
import copy
import glob
import logging
import os
import re
import subprocess
import sys
import random
from datetime import datetime
from functools import partial

import numpy as np
import torch
from torch import optim

try:
    import wandb
except ImportError:
    wandb = None

try:
    import torch.utils.tensorboard as tensorboard
except ImportError:
    tensorboard = None

try:
    import horovod.torch as hvd
except ImportError:
    hvd = None

from open_clip import create_model_and_transforms, trace_model, get_tokenizer, create_loss
# --- Âú®‰∏ãÊñπÊ∑ªÂä† ---
from open_clip_train.spatial_data import SpatiallyAwareDataset, SpatiallyAwareCollate
from open_clip_train.spatial_loss import GlobalMappingMultiPositiveClipLoss
from pathlib import Path
# --- Ê∑ªÂä†ÁªìÊùü ---
from open_clip_train.data import get_data
from open_clip_train.distributed import is_master, init_distributed_device, broadcast_object
from open_clip_train.logger import setup_logging
from open_clip_train.params import parse_args
from open_clip_train.scheduler import cosine_lr, const_lr, const_lr_cooldown
from open_clip_train.train import train_one_epoch, evaluate
from open_clip_train.file_utils import pt_load, check_exists, start_sync_process, remote_sync


LATEST_CHECKPOINT_NAME = "epoch_latest.pt"


def random_seed(seed=42, rank=0):
    torch.manual_seed(seed + rank)
    np.random.seed(seed + rank)
    random.seed(seed + rank)


def natural_key(string_):
    """See http://www.codinghorror.com/blog/archives/001018.html"""
    return [int(s) if s.isdigit() else s for s in re.split(r'(\d+)', string_.lower())]


def get_latest_checkpoint(path: str, remote : bool):
    # as writen, this glob recurses, so can pick up checkpoints across multiple sub-folders
    if remote:
        result = subprocess.run(["aws", "s3", "ls", path + "/"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        print(result)
        if result.returncode == 1:
            return None
        checkpoints = [os.path.join(path, x.split(' ')[-1]) for x in result.stdout.decode().split('\n')[:-1]]
    else:
        checkpoints = glob.glob(path + '**/*.pt', recursive=True)
    if checkpoints:
        checkpoints = sorted(checkpoints, key=natural_key)
        return checkpoints[-1]
    return None


def main(args):
    args = parse_args(args)

    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than
        # float16 and almost as accurate as float32
        # This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    # fully initialize distributed device environment
    device = init_distributed_device(args)

    # get the name of the experiments
    if args.name is None:
        # sanitize model name for filesystem / uri use, easier if we don't use / in name as a rule?
        model_name_safe = args.model.replace('/', '-')
        date_str = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
        if args.distributed:
            # sync date_str from master to all ranks
            date_str = broadcast_object(args, date_str)
        args.name = '-'.join([
            date_str,
            f"model_{model_name_safe}",
            f"lr_{args.lr}",
            f"b_{args.batch_size}",
            f"j_{args.workers}",
            f"p_{args.precision}",
        ])

    resume_latest = args.resume == 'latest'
    log_base_path = os.path.join(args.logs, args.name)
    args.log_path = None
    if is_master(args, local=args.log_local):
        os.makedirs(log_base_path, exist_ok=True)
        log_filename = f'out-{args.rank}' if args.log_local else 'out.log'
        args.log_path = os.path.join(log_base_path, log_filename)
        if os.path.exists(args.log_path) and not resume_latest:
            print(
                "Error. Experiment already exists. Use --name {} to specify a new experiment."
            )
            return -1

    # Setup text logger
    args.log_level = logging.DEBUG if args.debug else logging.INFO
    setup_logging(args.log_path, args.log_level)

    # Setup wandb, tensorboard, checkpoint logging
    args.wandb = 'wandb' in args.report_to or 'all' in args.report_to
    args.tensorboard = 'tensorboard' in args.report_to or 'all' in args.report_to
    args.checkpoint_path = os.path.join(log_base_path, "checkpoints")
    if is_master(args):
        args.tensorboard_path = os.path.join(log_base_path, "tensorboard") if args.tensorboard else ''
        for dirname in [args.tensorboard_path, args.checkpoint_path]:
            if dirname:
                os.makedirs(dirname, exist_ok=True)
    else:
        args.tensorboard_path = ''

    if resume_latest:
        resume_from = None
        checkpoint_path = args.checkpoint_path
        # If using remote_sync, need to check the remote instead of the local checkpoints folder.
        if args.remote_sync is not None:
            checkpoint_path = os.path.join(args.remote_sync, args.name, "checkpoints")
            if args.save_most_recent:
                print('Error. Cannot use save-most-recent with remote_sync and resume latest.')
                return -1
            if args.remote_sync_protocol != 's3':
                print('Error. Sync protocol not supported when using resume latest.')
                return -1
        if is_master(args):
            # Checking for existing checkpoint via master rank only. It is possible for
            # different rank processes to see different files if a shared file-system is under
            # stress, however it's very difficult to fully work around such situations.
            if args.save_most_recent:
                # if --save-most-recent flag is set, look for latest at a fixed filename
                resume_from = os.path.join(checkpoint_path, LATEST_CHECKPOINT_NAME)
                if not os.path.exists(resume_from):
                    # If no latest checkpoint has been saved yet, don't try to resume
                    resume_from = None
            else:
                # otherwise, list checkpoint dir contents and pick the newest checkpoint
                resume_from = get_latest_checkpoint(checkpoint_path, remote=args.remote_sync is not None)
            if resume_from:
                logging.info(f'Found latest resume checkpoint at {resume_from}.')
            else:
                logging.info(f'No latest resume checkpoint found in {checkpoint_path}.')
        if args.distributed:
            # sync found checkpoint path to all ranks
            resume_from = broadcast_object(args, resume_from)
        args.resume = resume_from

    if args.copy_codebase:
        copy_codebase(args)

    # start the sync proces if remote-sync is not None
    remote_sync_process = None
    if is_master(args) and args.remote_sync is not None:
        # first make sure it works
        result = remote_sync(
            os.path.join(args.logs, args.name), 
            os.path.join(args.remote_sync, args.name), 
            args.remote_sync_protocol
        )
        if result:
            logging.info('remote sync successful.')
        else:
            logging.info('Error: remote sync failed. Exiting.')
            return -1
        # if all looks good, start a process to do this every args.remote_sync_frequency seconds
        remote_sync_process = start_sync_process(
            args.remote_sync_frequency,
            os.path.join(args.logs, args.name), 
            os.path.join(args.remote_sync, args.name), 
            args.remote_sync_protocol
        )
        remote_sync_process.start()

    if args.precision == 'fp16':
        logging.warning(
            'It is recommended to use AMP mixed-precision instead of FP16. '
            'FP16 support needs further verification and tuning, especially for train.')

    if args.horovod:
        logging.info(
            f'Running in horovod mode with multiple processes / nodes. Device: {args.device}.'
            f'Process (global: {args.rank}, local {args.local_rank}), total {args.world_size}.')
    elif args.distributed:
        logging.info(
            f'Running in distributed mode with multiple processes. Device: {args.device}.'
            f'Process (global: {args.rank}, local {args.local_rank}), total {args.world_size}.')
    else:
        logging.info(f'Running with a single process. Device {args.device}.')

    dist_model = None
    args.distill = args.distill_model is not None and args.distill_pretrained is not None
    if args.distill:
        #FIXME: support distillation with grad accum.
        assert args.accum_freq == 1
        #FIXME: support distillation with coca.
        assert 'coca' not in args.model.lower()

    if isinstance(args.force_image_size, (tuple, list)) and len(args.force_image_size) == 1:
        # arg is nargs, single (square) image size list -> int
        args.force_image_size = args.force_image_size[0]
    random_seed(args.seed, 0)
    model_kwargs = {}
    if args.siglip:
        model_kwargs['init_logit_scale'] = np.log(10)  # different from CLIP
        model_kwargs['init_logit_bias'] = -10
    model, preprocess_train, preprocess_val = create_model_and_transforms(
        args.model,
        args.pretrained,
        precision=args.precision,
        device=device,
        jit=args.torchscript,
        force_quick_gelu=args.force_quick_gelu,
        force_custom_text=args.force_custom_text,
        force_patch_dropout=args.force_patch_dropout,
        force_image_size=args.force_image_size,
        force_context_length=args.force_context_length,
        image_mean=args.image_mean,
        image_std=args.image_std,
        image_interpolation=args.image_interpolation,
        image_resize_mode=args.image_resize_mode,  # only effective for inference
        aug_cfg=args.aug_cfg,
        pretrained_image=args.pretrained_image,
        output_dict=True,
        cache_dir=args.cache_dir,
        **model_kwargs,
    )
    if args.distill:
        # FIXME: currently assumes the model you're distilling from has the same tokenizer & transforms.
        dist_model, _, _ = create_model_and_transforms(
            args.distill_model, 
            args.distill_pretrained,
            device=device,
            precision=args.precision,
            output_dict=True,
            cache_dir=args.cache_dir,
        )
    if args.use_bnb_linear is not None:
        print('=> using a layer from bitsandbytes.\n'
              '   this is an experimental feature which requires two extra pip installs\n'
              '   pip install bitsandbytes triton'
              '   please make sure to use triton 2.0.0')
        import bitsandbytes as bnb
        from open_clip.utils import replace_linear
        print(f'=> replacing linear layers with {args.use_bnb_linear}')
        linear_replacement_cls = getattr(bnb.nn.triton_based_modules, args.use_bnb_linear)
        replace_linear(model, linear_replacement_cls)
        model = model.to(device)

    random_seed(args.seed, args.rank)

    if args.trace:
        model = trace_model(model, batch_size=args.batch_size, device=device)

    if args.lock_image:
        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991
        model.lock_image_tower(
            unlocked_groups=args.lock_image_unlocked_groups,
            freeze_bn_stats=args.lock_image_freeze_bn_stats)
    if args.lock_text:
        model.lock_text_tower(
            unlocked_layers=args.lock_text_unlocked_layers,
            freeze_layer_norm=args.lock_text_freeze_layer_norm)

    if args.grad_checkpointing:
        model.set_grad_checkpointing()

    if is_master(args):
        logging.info("Model:")
        logging.info(f"{str(model)}")
        logging.info("Params:")
        params_file = os.path.join(args.logs, args.name, "params.txt")
        with open(params_file, "w") as f:
            for name in sorted(vars(args)):
                val = getattr(args, name)
                logging.info(f"  {name}: {val}")
                f.write(f"{name}: {val}\n")

    if args.distributed and not args.horovod:
        if args.use_bn_sync:
            model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
        ddp_args = {}
        if args.ddp_static_graph:
            # this doesn't exist in older PyTorch, arg only added if enabled
            ddp_args['static_graph'] = True
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[device], **ddp_args)
    
        if args.distill:
            dist_model = torch.nn.parallel.DistributedDataParallel(dist_model, device_ids=[device], **ddp_args)

    # create optimizer and scaler
    optimizer = None
    scaler = None

    if args.train_data or args.dataset_type == "synthetic" or args.use_spatial_dataset:
        assert not args.trace, 'Cannot train with traced model'

        opt = getattr(args, 'opt', 'adamw').lower()
        if opt.startswith('timm/'):
            from timm.optim import create_optimizer_v2
            timm_opt = opt.split('timm/')[-1]
            opt_kwargs = {}
            assert (args.beta1 is None) == (args.beta2 is None), \
                'When using timm optimizer, BOTH beta1 and beta2 must be specified (or not specified).'
            if args.beta1 is not None:
                opt_kwargs['betas'] = (args.beta1, args.beta2)
            if args.momentum is not None:
                opt_kwargs['momentum'] = args.momentum
            optimizer = create_optimizer_v2(
                model,
                timm_opt,
                lr=args.lr,
                weight_decay=args.wd,
                eps=args.eps,
                **opt_kwargs,
            )
        else:
            # If some params are not passed, we use the default values based on model name.
            exclude = lambda n, p: p.ndim < 2 or "bn" in n or "ln" in n or "bias" in n or 'logit_scale' in n
            include = lambda n, p: not exclude(n, p)

            named_parameters = list(model.named_parameters())
            gain_or_bias_params = [p for n, p in named_parameters if exclude(n, p) and p.requires_grad]
            rest_params = [p for n, p in named_parameters if include(n, p) and p.requires_grad]

            if opt == 'adamw':
                optimizer = optim.AdamW(
                    [
                        {"params": gain_or_bias_params, "weight_decay": 0.},
                        {"params": rest_params, "weight_decay": args.wd},
                    ],
                    lr=args.lr,
                    betas=(args.beta1, args.beta2),
                    eps=args.eps,
                )
            else:
                assert False, f'Unknown optimizer {opt}'

        if is_master(args):
            defaults = copy.deepcopy(optimizer.defaults)
            defaults['weight_decay'] = args.wd
            defaults = ', '.join([f'{k}: {v}' for k, v in defaults.items()])
            logging.info(
                f'Created {type(optimizer).__name__} ({args.opt}) optimizer: {defaults}'
            )

        if args.horovod:
            optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())
            hvd.broadcast_parameters(model.state_dict(), root_rank=0)
            hvd.broadcast_optimizer_state(optimizer, root_rank=0)

        scaler = None
        if args.precision == "amp":
            try:
                scaler = torch.amp.GradScaler(device=device)
            except (AttributeError, TypeError) as e:
                scaler = torch.cuda.amp.GradScaler()

    # optionally resume from a checkpoint
    start_epoch = 0
    if args.resume is not None:
        checkpoint = pt_load(args.resume, map_location='cpu')
        if 'epoch' in checkpoint:
            # resuming a train checkpoint w/ epoch and optimizer state
            start_epoch = checkpoint["epoch"]
            sd = checkpoint["state_dict"]
            if not args.distributed and next(iter(sd.items()))[0].startswith('module'):
                sd = {k[len('module.'):]: v for k, v in sd.items()}
            model.load_state_dict(sd)
            if optimizer is not None:
                optimizer.load_state_dict(checkpoint["optimizer"])
            if scaler is not None and 'scaler' in checkpoint:
                scaler.load_state_dict(checkpoint['scaler'])
            logging.info(f"=> resuming checkpoint '{args.resume}' (epoch {start_epoch})")
        else:
            # loading a bare (model only) checkpoint for fine-tune or evaluation
            model.load_state_dict(checkpoint)
            logging.info(f"=> loaded checkpoint '{args.resume}' (epoch {start_epoch})")

    # initialize datasets
    tokenizer = get_tokenizer(args.model, cache_dir=args.cache_dir, context_length=args.force_context_length)

    # --- ÊõøÊç¢ data = get_data(...) ---
    if args.use_spatial_dataset:
        assert args.spatial_data_dir is not None, "--spatial-data-dir must be provided."
        # --- ‰øÆÊîπ Dataset Âíå Collate ÁöÑÂÆû‰æãÂåñ ---
        dataset = SpatiallyAwareDataset(
            Path(args.spatial_data_dir), 
            k_neighbors=args.k_neighbors,
            preprocess_fn=preprocess_train,  # <-- ‰º†Áªô Dataset
            tokenizer=tokenizer             # <-- ‰º†Áªô Dataset
        )
        spatial_collate = SpatiallyAwareCollate() # <-- ‰∏çÂÜçÈúÄË¶ÅÂèÇÊï∞
        
        sampler = torch.utils.data.distributed.DistributedSampler(dataset) if args.distributed else None
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=args.batch_size, shuffle=False if args.distributed else True,
            num_workers=args.workers, pin_memory=True, sampler=sampler,
            drop_last=True, collate_fn=spatial_collate)
        dataloader.num_samples = len(dataset)
        dataloader.num_batches = len(dataloader)
        from open_clip_train.data import DataInfo
        data = {'train': DataInfo(dataloader, sampler)}
        if args.val_data:
            val_data_info = get_data(args, (preprocess_train, preprocess_val), epoch=start_epoch, tokenizer=tokenizer)
            if "val" in val_data_info: data["val"] = val_data_info["val"]
    else:
        data = get_data(args, (preprocess_train, preprocess_val), epoch=start_epoch, tokenizer=tokenizer)
        
    # --- ÊõøÊç¢ÁªìÊùü ---
    assert len(data), 'At least one train or eval dataset must be specified.'

    # create scheduler if train
    scheduler = None
    if 'train' in data and optimizer is not None:
        total_steps = (data["train"].dataloader.num_batches // args.accum_freq) * args.epochs
        if args.lr_scheduler == "cosine":
            scheduler = cosine_lr(optimizer, args.lr, args.warmup, total_steps)
        elif args.lr_scheduler == "const":
            scheduler = const_lr(optimizer, args.lr, args.warmup, total_steps)
        elif args.lr_scheduler == "const-cooldown":
            assert args.epochs_cooldown is not None,\
                "Please specify the number of cooldown epochs for this lr schedule."
            cooldown_steps = (data["train"].dataloader.num_batches // args.accum_freq) * args.epochs_cooldown
            scheduler = const_lr_cooldown(
                optimizer, args.lr, args.warmup, total_steps,
                cooldown_steps, args.lr_cooldown_power, args.lr_cooldown_end)
        else:
            logging.error(
                f'Unknown scheduler, {args.lr_scheduler}. Available options are: cosine, const, const-cooldown.')
            exit(1)

    # determine if this worker should save logs and checkpoints. only do so if it is rank == 0
    args.save_logs = args.logs and args.logs.lower() != 'none' and is_master(args)
    writer = None
    if args.save_logs and args.tensorboard:
        assert tensorboard is not None, "Please install tensorboard."
        writer = tensorboard.SummaryWriter(args.tensorboard_path)

    if args.wandb and is_master(args):
        assert wandb is not None, 'Please install wandb.'
        logging.debug('Starting wandb.')
        args.train_sz = data["train"].dataloader.num_samples
        if args.val_data is not None:
            args.val_sz = data["val"].dataloader.num_samples
        # you will have to configure this for your project!
        wandb.init(
            project=args.wandb_project_name,
            name=args.name,
            id=args.name,
            notes=args.wandb_notes,
            tags=[],
            resume='auto' if args.resume == "latest" else None,
            config=vars(args),
        )
        if args.debug:
            wandb.watch(model, log='all')
        wandb.save(params_file)
        logging.debug('Finished loading wandb.')

    # Pytorch 2.0 adds '_orig_mod.' prefix to keys of state_dict() of compiled models.
    # For compatibility, we save state_dict() of the original model, which shares the
    # weights without the prefix.
    original_model = model
    if args.torchcompile:
        logging.info('Compiling model...')

        if args.grad_checkpointing and args.distributed:
            logging.info('Disabling DDP dynamo optimizer when grad checkpointing enabled.')
            # As of now (~PyTorch 2.4/2.5), compile + grad checkpointing work, but DDP optimizer must be disabled
            torch._dynamo.config.optimize_ddp = False

        model = torch.compile(original_model)

    if 'train' not in data:
        # If using int8, convert to inference mode.
        if args.use_bnb_linear is not None:
            from open_clip.utils import convert_int8_model_to_inference_mode
            convert_int8_model_to_inference_mode(model)
        # Evaluate.
        evaluate(model, data, start_epoch, args, tb_writer=writer, tokenizer=tokenizer)
        return

    # --- ÊõøÊç¢ loss = create_loss(args) ---
    if args.use_spatial_loss:
        print('=> using spatial loss.')
        loss = GlobalMappingMultiPositiveClipLoss(
            local_loss=args.local_loss,
            gather_with_grad=args.gather_with_grad,
            cache_labels=False,
            rank=args.rank,
            world_size=args.world_size,
            use_horovod=args.horovod,
            # Êñ∞Â¢ûÂºÄÂÖ≥
            cap_logit_scale=getattr(args, "logit_scale_cap", None),
            temp_reg_weight=getattr(args, "temp_reg_weight", 0.0),
            float32_logits=getattr(args, "float32_logits", False),
            neighbor_alpha_scale=getattr(args, "neighbor_alpha_scale", 1.0),
        )
    else:
        print('=> using standard loss.')
        loss = create_loss(args)
    # --- ÊõøÊç¢ÁªìÊùü ---

    for epoch in range(start_epoch, args.epochs):
        if is_master(args):
            logging.info(f'Start epoch {epoch}')

        train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
        completed_epoch = epoch + 1

        if any(v in data for v in ('val', 'imagenet-val', 'imagenet-v2')):
            evaluate(model, data, completed_epoch, args, tb_writer=writer, tokenizer=tokenizer)

        # Saving checkpoints.
        if args.save_logs:
            checkpoint_dict = {
                "epoch": completed_epoch,
                "name": args.name,
                "state_dict": original_model.state_dict(),
                "optimizer": optimizer.state_dict(),
            }
            if scaler is not None:
                checkpoint_dict["scaler"] = scaler.state_dict()

            if completed_epoch == args.epochs or (
                args.save_frequency > 0 and (completed_epoch % args.save_frequency) == 0
            ):
                torch.save(
                    checkpoint_dict,
                    os.path.join(args.checkpoint_path, f"epoch_{completed_epoch}.pt"),
                )
            if args.delete_previous_checkpoint:
                previous_checkpoint = os.path.join(args.checkpoint_path, f"epoch_{completed_epoch - 1}.pt")
                if os.path.exists(previous_checkpoint):
                    os.remove(previous_checkpoint)

            if args.save_most_recent:
                # try not to corrupt the latest checkpoint if save fails
                tmp_save_path = os.path.join(args.checkpoint_path, "tmp.pt")
                latest_save_path = os.path.join(args.checkpoint_path, LATEST_CHECKPOINT_NAME)
                torch.save(checkpoint_dict, tmp_save_path)
                os.replace(tmp_save_path, latest_save_path)

    if args.wandb and is_master(args):
        wandb.finish()

    # run a final sync.
    if remote_sync_process is not None:
        logging.info('Final remote sync.')
        remote_sync_process.terminate()
        result = remote_sync(
            os.path.join(args.logs, args.name), 
            os.path.join(args.remote_sync, args.name), 
            args.remote_sync_protocol
        )
        if result:
            logging.info('Final remote sync successful.')
        else:
            logging.info('Final remote sync failed.')
    

def copy_codebase(args):
    from shutil import copytree, ignore_patterns
    new_code_path = os.path.join(args.logs, args.name, "code")
    if os.path.exists(new_code_path):
        print(
            f"Error. Experiment already exists at {new_code_path}. Use --name to specify a new experiment."
        )
        return -1
    print(f"Copying codebase to {new_code_path}")
    current_code_path = os.path.realpath(__file__)
    for _ in range(3):
        current_code_path = os.path.dirname(current_code_path)
    copytree(current_code_path, new_code_path, ignore=ignore_patterns('log', 'logs', 'wandb'))
    print("Done copying code.")
    return 1


if __name__ == "__main__":
    main(sys.argv[1:])

===== src/open_clip_train/params.py =====
import argparse
import ast


def get_default_params(model_name):
    # Params from paper (https://arxiv.org/pdf/2103.00020.pdf)
    model_name = model_name.lower()
    if "vit" in model_name:
        return {"lr": 5.0e-4, "beta1": 0.9, "beta2": 0.98, "eps": 1.0e-6}
    else:
        return {"lr": 5.0e-4, "beta1": 0.9, "beta2": 0.999, "eps": 1.0e-8}


class ParseKwargs(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        kw = {}
        for value in values:
            key, value = value.split('=')
            try:
                kw[key] = ast.literal_eval(value)
            except ValueError:
                kw[key] = str(value)  # fallback to string (avoid need to escape on command line)
        setattr(namespace, self.dest, kw)


def parse_args(args):
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-data",
        type=str,
        default=None,
        help="Path to file(s) with training data. When using webdataset, multiple datasources can be combined using the `::` separator.",
    )
    parser.add_argument(
        "--train-data-upsampling-factors",
        type=str,
        default=None,
        help=(
            "When using multiple data sources with webdataset and sampling with replacement, this can be used to upsample specific data sources. "
            "Similar to --train-data, this should be a string with as many numbers as there are data sources, separated by `::` (e.g. 1::2::0.5) "
            "By default, datapoints are sampled uniformly regardless of the dataset sizes."
        )
    )
    parser.add_argument(
        "--val-data",
        type=str,
        default=None,
        help="Path to file(s) with validation data",
    )
    parser.add_argument(
        "--train-num-samples",
        type=int,
        default=None,
        help="Number of samples in dataset. Required for webdataset if not available in info file.",
    )
    parser.add_argument(
        "--val-num-samples",
        type=int,
        default=None,
        help="Number of samples in dataset. Useful for webdataset if not available in info file.",
    )
    parser.add_argument(
        "--dataset-type",
        choices=["webdataset", "csv", "synthetic", "auto"],
        default="auto",
        help="Which type of dataset to process."
    )
    parser.add_argument(
        "--dataset-resampled",
        default=False,
        action="store_true",
        help="Whether to use sampling with replacement for webdataset shard selection."
    )
    parser.add_argument(
        "--csv-separator",
        type=str,
        default="\t",
        help="For csv-like datasets, which separator to use."
    )
    parser.add_argument(
        "--csv-img-key",
        type=str,
        default="filepath",
        help="For csv-like datasets, the name of the key for the image paths."
    )
    parser.add_argument(
        "--csv-caption-key",
        type=str,
        default="title",
        help="For csv-like datasets, the name of the key for the captions."
    )
    parser.add_argument(
        "--imagenet-val",
        type=str,
        default=None,
        help="Path to imagenet val set for conducting zero shot evaluation.",
    )
    parser.add_argument(
        "--imagenet-v2",
        type=str,
        default=None,
        help="Path to imagenet v2 for conducting zero shot evaluation.",
    )
    parser.add_argument(
        "--cache-dir",
        type=str,
        default=None,
        help="Override system default cache path for model & tokenizer file downloads.",
    )
    parser.add_argument(
        "--logs",
        type=str,
        default="./logs/",
        help="Where to store tensorboard logs. Use None to avoid storing logs.",
    )
    parser.add_argument(
        "--log-local",
        action="store_true",
        default=False,
        help="log files on local master, otherwise global master only.",
    )
    parser.add_argument(
        "--name",
        type=str,
        default=None,
        help="Optional identifier for the experiment when storing logs. Otherwise use current time.",
    )
    parser.add_argument(
        "--workers", type=int, default=4, help="Number of dataloader workers per GPU."
    )
    parser.add_argument(
        "--batch-size", type=int, default=64, help="Batch size per GPU."
    )
    parser.add_argument(
        "--epochs", type=int, default=32, help="Number of epochs to train for."
    )
    parser.add_argument(
        "--epochs-cooldown", type=int, default=None,
        help="When scheduler w/ cooldown used, perform cooldown from total_epochs - cooldown_epochs onwards."
    )
    parser.add_argument("--lr", type=float, default=None, help="Learning rate.")
    parser.add_argument("--beta1", type=float, default=None, help="Adam beta 1.")
    parser.add_argument("--beta2", type=float, default=None, help="Adam beta 2.")
    parser.add_argument("--eps", type=float, default=None, help="Adam epsilon.")
    parser.add_argument("--wd", type=float, default=0.2, help="Weight decay.")
    parser.add_argument("--momentum", type=float, default=None, help="Momentum (for timm optimizers).")
    parser.add_argument(
        "--warmup", type=int, default=10000, help="Number of steps to warmup for."
    )
    parser.add_argument(
        "--opt", type=str, default='adamw',
        help="Which optimizer to use. Choices are ['adamw', or any timm optimizer 'timm/{opt_name}']."
    )
    parser.add_argument(
        "--use-bn-sync",
        default=False,
        action="store_true",
        help="Whether to use batch norm sync.")
    parser.add_argument(
        "--skip-scheduler",
        action="store_true",
        default=False,
        help="Use this flag to skip the learning rate decay.",
    )
    parser.add_argument(
        "--lr-scheduler",
        type=str,
        default='cosine',
        help="LR scheduler. One of: 'cosine', 'const' (constant), 'const-cooldown' (constant w/ cooldown). Default: cosine",
    )
    parser.add_argument(
        "--lr-cooldown-end", type=float, default=0.0,
        help="End learning rate for cooldown schedule. Default: 0"
    )
    parser.add_argument(
        "--lr-cooldown-power", type=float, default=1.0,
        help="Power for polynomial cooldown schedule. Default: 1.0 (linear decay)"
    )
    parser.add_argument(
        "--save-frequency", type=int, default=1, help="How often to save checkpoints."
    )
    parser.add_argument(
        "--save-most-recent",
        action="store_true",
        default=False,
        help="Always save the most recent model trained to epoch_latest.pt.",
    )
    parser.add_argument(
        "--zeroshot-frequency", type=int, default=2, help="How often to run zero shot."
    )
    parser.add_argument(
        "--val-frequency", type=int, default=1, help="How often to run evaluation with val data."
    )
    parser.add_argument(
        "--resume",
        default=None,
        type=str,
        help="path to latest checkpoint (default: none)",
    )
    parser.add_argument(
        "--precision",
        choices=["amp", "amp_bf16", "amp_bfloat16", "bf16", "fp16", "pure_bf16", "pure_fp16", "fp32"],
        default="amp",
        help="Floating point precision."
    )
    parser.add_argument(
        "--model",
        type=str,
        default="RN50",
        help="Name of the vision backbone to use.",
    )
    parser.add_argument(
        "--pretrained",
        default='',
        type=str,
        help="Use a pretrained CLIP model weights with the specified tag or file path.",
    )
    parser.add_argument(
        "--pretrained-image",
        default=False,
        action='store_true',
        help="Load imagenet pretrained weights for image tower backbone if available.",
    )
    parser.add_argument(
        "--lock-image",
        default=False,
        action='store_true',
        help="Lock full image tower by disabling gradients.",
    )
    parser.add_argument(
        "--lock-image-unlocked-groups",
        type=int,
        default=0,
        help="Leave last n image tower layer groups unlocked.",
    )
    parser.add_argument(
        "--lock-image-freeze-bn-stats",
        default=False,
        action='store_true',
        help="Freeze BatchNorm running stats in image tower for any locked layers.",
    )
    parser.add_argument(
        '--image-mean', type=float, nargs='+', default=None, metavar='MEAN',
        help='Override default image mean value of dataset')
    parser.add_argument(
        '--image-std', type=float, nargs='+', default=None, metavar='STD',
        help='Override default image std deviation of of dataset')
    parser.add_argument(
        '--image-interpolation',
        default=None, type=str, choices=['bicubic', 'bilinear', 'random'],
        help="Override default image resize interpolation"
    )
    parser.add_argument(
        '--image-resize-mode',
        default=None, type=str, choices=['shortest', 'longest', 'squash'],
        help="Override default image resize (& crop) mode during inference"
    )
    parser.add_argument('--aug-cfg', nargs='*', default={}, action=ParseKwargs)
    parser.add_argument(
        "--grad-checkpointing",
        default=False,
        action='store_true',
        help="Enable gradient checkpointing.",
    )
    parser.add_argument(
        "--local-loss",
        default=False,
        action="store_true",
        help="calculate loss w/ local features @ global (instead of realizing full global @ global matrix)"
    )
    parser.add_argument(
        "--gather-with-grad",
        default=False,
        action="store_true",
        help="enable full distributed gradient for feature gather"
    )
    parser.add_argument(
        '--force-context-length', type=int, default=None,
        help='Override default context length'
    )
    parser.add_argument(
        '--force-image-size', type=int, nargs='+', default=None,
        help='Override default image size'
    )
    parser.add_argument(
        "--force-quick-gelu",
        default=False,
        action='store_true',
        help="Force use of QuickGELU activation for non-OpenAI transformer models.",
    )
    parser.add_argument(
        "--force-patch-dropout",
        default=None,
        type=float,
        help="Override the patch dropout during training, for fine tuning with no dropout near the end as in the paper",
    )
    parser.add_argument(
        "--force-custom-text",
        default=False,
        action='store_true',
        help="Force use of CustomTextCLIP model (separate text-tower).",
    )
    parser.add_argument(
        "--torchscript",
        default=False,
        action='store_true',
        help="torch.jit.script the model, also uses jit version of OpenAI models if pretrained=='openai'",
    )
    parser.add_argument(
        "--torchcompile",
        default=False,
        action='store_true',
        help="torch.compile() the model, requires pytorch 2.0 or later.",
    )
    parser.add_argument(
        "--trace",
        default=False,
        action='store_true',
        help="torch.jit.trace the model for inference / eval only",
    )
    parser.add_argument(
        "--accum-freq", type=int, default=1, help="Update the model every --acum-freq steps."
    )
    parser.add_argument(
        "--device", default="cuda", type=str, help="Accelerator to use."
    )
    # arguments for distributed training
    parser.add_argument(
        "--dist-url",
        default=None,
        type=str,
        help="url used to set up distributed training",
    )
    parser.add_argument(
        "--dist-backend",
        default=None,
        type=str,
        help="distributed backend. \"nccl\" for GPU, \"hccl\" for Ascend NPU"
    )
    parser.add_argument(
        "--report-to",
        default='',
        type=str,
        help="Options are ['wandb', 'tensorboard', 'wandb,tensorboard']"
    )
    parser.add_argument(
        "--wandb-notes",
        default='',
        type=str,
        help="Notes if logging with wandb"
    )
    parser.add_argument(
        "--wandb-project-name",
        type=str,
        default='open-clip',
        help="Name of the project if logging with wandb.",
    )
    parser.add_argument(
        "--debug",
        default=False,
        action="store_true",
        help="If true, more information is logged."
    )
    parser.add_argument(
        "--copy-codebase",
        default=False,
        action="store_true",
        help="If true, we copy the entire base on the log directory, and execute from there."
    )
    parser.add_argument(
        "--horovod",
        default=False,
        action="store_true",
        help="Use horovod for distributed training."
    )
    parser.add_argument(
        "--ddp-static-graph",
        default=False,
        action='store_true',
        help="Enable static graph optimization for DDP in PyTorch >= 1.11.",
    )
    parser.add_argument(
        "--no-set-device-rank",
        default=False,
        action="store_true",
        help="Don't set device index from local rank (when CUDA_VISIBLE_DEVICES restricted to one per proc)."
    )
    parser.add_argument(
        "--seed", type=int, default=0, help="Default random seed."
    )
    parser.add_argument(
        "--grad-clip-norm", type=float, default=None, help="Gradient clip."
    )
    parser.add_argument(
        "--lock-text",
        default=False,
        action='store_true',
        help="Lock full text tower by disabling gradients.",
    )
    parser.add_argument(
        "--lock-text-unlocked-layers",
        type=int,
        default=0,
        help="Leave last n text tower layer groups unlocked.",
    )
    parser.add_argument(
        "--lock-text-freeze-layer-norm",
        default=False,
        action='store_true',
        help="Freeze LayerNorm running stats in text tower for any locked layers.",
    )
    parser.add_argument(
        "--log-every-n-steps",
        type=int,
        default=100,
        help="Log every n steps to tensorboard/console/wandb.",
    )
    parser.add_argument(
        "--coca-caption-loss-weight",
        type=float,
        default=2.0,
        help="Weight assigned to caption loss in CoCa."
    )
    parser.add_argument(
        "--coca-contrastive-loss-weight",
        type=float,
        default=1.0,
        help="Weight assigned to contrastive loss when training CoCa."
    )
    parser.add_argument(
        "--remote-sync",
        type=str,
        default=None,
        help="Optinoally sync with a remote path specified by this arg",
    )
    parser.add_argument(
        "--remote-sync-frequency",
        type=int,
        default=300,
        help="How frequently to sync to a remote directly if --remote-sync is not None.",
    )
    parser.add_argument(
        "--remote-sync-protocol",
        choices=["s3", "fsspec"],
        default="s3",
        help="How to do the remote sync backup if --remote-sync is not None.",
    )
    parser.add_argument(
        "--delete-previous-checkpoint",
        default=False,
        action="store_true",
        help="If true, delete previous checkpoint after storing a new one."
    )
    parser.add_argument(
        "--distill-model",
        default=None,
        help='Which model arch to distill from, if any.'
    )
    parser.add_argument(
        "--distill-pretrained",
        default=None,
        help='Which pre-trained weights to distill from, if any.'
    )
    parser.add_argument(
        "--use-bnb-linear",
        default=None,
        help='Replace the network linear layers from the bitsandbytes library. '
        'Allows int8 training/inference, etc.'
    )
    parser.add_argument(
        "--siglip",
        default=False,
        action="store_true",
        help='Use SigLip (sigmoid) loss.'
    )
    parser.add_argument(
        "--loss-dist-impl",
        default=None,
        type=str,
        help='A string to specify a specific distributed loss implementation.'
    )



    # --- Âú®ËøôÈáåÁ≤òË¥¥ÊÇ®ÁöÑ‰ª£Á†Å ---
    group = parser.add_argument_group('Spatial Training Arguments')
    group.add_argument(
        "--use-spatial-dataset", action="store_true", default=False,
        help="Use SpatiallyAwareDataset instead of default.")
    group.add_argument(
        "--spatial-data-dir", type=str, default=None,
        help="Directory with nodes.parquet and edges.parquet.")
    group.add_argument(
        "--k-neighbors", type=int, default=8,
        help="Number of neighbors to use for each anchor.")
    group.add_argument(
        "--use-spatial-loss", action="store_true", default=False,
        help="Use multi positive clip loss.")
    # --- Á≤òË¥¥ÁªìÊùü ---

    # --- Logit / Temperature controls (NEW) ---
    temp = parser.add_argument_group('Logit / Temperature Controls')
    temp.add_argument(
        "--logit-scale-cap", type=float, default=None,
        help="ÂØπÊúâÊïàÊ∏©Â∫¶ s Âä†‰∏äÈôêÔºà‰æãÂ¶Ç 40Ôºâ„ÄÇNone Ë°®Á§∫‰∏çÈôêÂà∂„ÄÇ"
    )
    temp.add_argument(
        "--temp-reg-weight", type=float, default=0.0,
        help="Ê∏©Â∫¶Ê≠£ÂàôÊùÉÈáçÔºà0 ÂÖ≥Èó≠ÔºõÂª∫ËÆÆÂ∞ùËØï 0.02~0.1Ôºâ„ÄÇ"
    )
    temp.add_argument(
        "--neighbor-alpha-scale", type=float, default=1.0,
        help="ÈÇªÂ±ÖÊùÉÈáçÊï¥‰ΩìÁº©ÊîæÁ≥ªÊï∞Ôºà‰æãÂ¶Ç 0.3~0.5 Èôç‰ΩéÊó©ÊúüÂô™Â£∞Ôºâ„ÄÇ"
    )
    temp.add_argument(
        "--float32-logits", action="store_true", default=False,
        help="Âú® log-softmax ÂâçÊää logits ËΩ¨Êàê float32ÔºåÊèêÂçáÊï∞ÂÄºÁ®≥ÂÆöÊÄß„ÄÇ"
    )


    args = parser.parse_args(args)

    if 'timm' not in args.opt:
        # set default opt params based on model name (only if timm optimizer not used)
        default_params = get_default_params(args.model)
        for name, val in default_params.items():
            if getattr(args, name) is None:
                setattr(args, name, val)

    return args

===== src/open_clip_train/file_utils.py =====
import logging
import os
import multiprocessing
import subprocess
import time
import fsspec
import torch
from tqdm import tqdm

def remote_sync_s3(local_dir, remote_dir):
    # skip epoch_latest which can change during sync.
    result = subprocess.run(["aws", "s3", "sync", local_dir, remote_dir, '--exclude', '*epoch_latest.pt'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if result.returncode != 0:
        logging.error(f"Error: Failed to sync with S3 bucket {result.stderr.decode('utf-8')}")
        return False
        
    logging.info(f"Successfully synced with S3 bucket")
    return True

def remote_sync_fsspec(local_dir, remote_dir):
    # FIXME currently this is slow and not recommended. Look into speeding up.
    a = fsspec.get_mapper(local_dir)
    b = fsspec.get_mapper(remote_dir)

    for k in a:
        # skip epoch_latest which can change during sync.
        if 'epoch_latest.pt' in k:
            continue

        logging.info(f'Attempting to sync {k}')
        if k in b and len(a[k]) == len(b[k]):
            logging.debug(f'Skipping remote sync for {k}.')
            continue

        try:
            logging.info(f'Successful sync for {k}.')
            b[k] = a[k]
        except Exception as e:
            logging.info(f'Error during remote sync for {k}: {e}')
            return False

    return True

def remote_sync(local_dir, remote_dir, protocol):
    logging.info('Starting remote sync.')
    if protocol == 's3':
        return remote_sync_s3(local_dir, remote_dir)
    elif protocol == 'fsspec':
        return remote_sync_fsspec(local_dir, remote_dir)
    else:
        logging.error('Remote protocol not known')
        return False

def keep_running_remote_sync(sync_every, local_dir, remote_dir, protocol):
    while True:
        time.sleep(sync_every)
        remote_sync(local_dir, remote_dir, protocol)

def start_sync_process(sync_every, local_dir, remote_dir, protocol):
    p = multiprocessing.Process(target=keep_running_remote_sync, args=(sync_every, local_dir, remote_dir, protocol))
    return p

# Note: we are not currently using this save function.
def pt_save(pt_obj, file_path):
    of = fsspec.open(file_path, "wb")
    with of as f:
        torch.save(pt_obj, file_path)

def pt_load(file_path, map_location=None):
    if file_path.startswith('s3'):
        logging.info('Loading remote checkpoint, which may take a bit.')
    of = fsspec.open(file_path, "rb")
    with of as f:
        out = torch.load(f, map_location=map_location)
    return out

def check_exists(file_path):
    try:
        with fsspec.open(file_path):
            pass
    except FileNotFoundError:
        return False
    return True

===== src/open_clip_train/train.py =====
import json
import logging
import math
import os
import time

import numpy as np
import torch
import torch.nn.functional as F

try:
    import wandb
except ImportError:
    wandb = None

from open_clip import get_input_dtype
from open_clip_train.distributed import is_master
from open_clip_train.zero_shot import zero_shot_eval
from open_clip_train.precision import get_autocast


class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def unwrap_model(model):
    if hasattr(model, 'module'):
        return model.module
    else:
        return model


def backward(total_loss, scaler):
    if scaler is not None:
        scaler.scale(total_loss).backward()
    else:
        total_loss.backward()


def train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=None):
    device = torch.device(args.device)
    autocast = get_autocast(args.precision, device_type=device.type)
    input_dtype = get_input_dtype(args.precision)

    model.train()
    if args.distill:
        dist_model.eval()

    data['train'].set_epoch(epoch)
    dataloader = data['train'].dataloader
    num_batches_per_epoch = dataloader.num_batches // args.accum_freq
    sample_digits = math.ceil(math.log(dataloader.num_samples + 1, 10))

    if args.accum_freq > 1:
        accum_images, accum_texts, accum_features = [], [], {}

    losses_m = {}
    batch_time_m = AverageMeter()
    data_time_m = AverageMeter()
    end = time.time()
    for i, batch in enumerate(dataloader):
        i_accum = i // args.accum_freq
        step = num_batches_per_epoch * epoch + i_accum

        if not args.skip_scheduler:
            scheduler(step)

        # --- Êï∞ÊçÆËß£ÂåÖÈÄªËæë ---
        # Êó†ËÆ∫ÊòØÂê¶‰ΩøÁî® spatial_lossÔºåÈÉΩÂÖà‰ªé batch ‰∏≠ÊèêÂèñÊï∞ÊçÆ
        if args.use_spatial_dataset:
            images = batch["images"].to(device=device, dtype=input_dtype, non_blocking=True)
            texts = batch["texts"].to(device=device, non_blocking=True)
            
            # Â≠òÂÇ®Á©∫Èó¥Áõ∏ÂÖ≥ÁöÑÂÖÉÊï∞ÊçÆÔºå‰ª•‰æøÂêéÁª≠Ê†πÊçÆ use_spatial_loss ÂÜ≥ÂÆöÊòØÂê¶‰ΩøÁî®
            spatial_metadata = {
                "image_tile_ids": batch["image_tile_ids"].to(device=device, non_blocking=True),
                "text_tile_ids": batch["text_tile_ids"].to(device=device, non_blocking=True),
                "neighbor_tile_ids": batch["neighbor_tile_ids"].to(device=device, non_blocking=True),
                "neighbor_alphas": batch["neighbor_alphas"].to(device=device, non_blocking=True),
            }
        else:
            images, texts = batch
            images = images.to(device=device, dtype=input_dtype, non_blocking=True)
            texts = texts.to(device=device, non_blocking=True)
            spatial_metadata = {} # ÈùûÁ©∫Èó¥Êï∞ÊçÆÈõÜÊó∂‰∏∫Á©∫

        data_time_m.update(time.time() - end)
        optimizer.zero_grad()

        if args.accum_freq == 1:
            with autocast():
                unwrapped_model = unwrap_model(model)

                if args.use_spatial_dataset:
                    # --- Ê†∏ÂøÉ‰øÆÊ≠£ÔºöÁ©∫Èó¥Êï∞ÊçÆÈõÜÁöÑÊçüÂ§±ÂáΩÊï∞Ë∞ÉÁî®ÈÄªËæë ---
                    image_features = unwrapped_model.encode_image(images)
                    text_features = unwrapped_model.encode_text(texts)
                    logit_scale = unwrapped_model.logit_scale.exp()
                    logit_bias = getattr(unwrapped_model, 'logit_bias', None)

                    # 1. ÂáÜÂ§áÊâÄÊúâÊçüÂ§±ÂáΩÊï∞ÈÉΩÈÄöÁî®ÁöÑÂü∫Á°ÄÂèÇÊï∞
                    loss_kwargs = {
                        "image_features": image_features,
                        "text_features": text_features,
                        "logit_scale": logit_scale,
                        "logit_bias": logit_bias,
                    }

                    # 2. Â¶ÇÊûúÊòØ Spatial LossÔºåÂàôÊ∑ªÂä†È¢ùÂ§ñÁöÑÁ©∫Èó¥ÂèÇÊï∞
                    if args.use_spatial_loss:
                        loss_kwargs.update(spatial_metadata)
                    
                    # 3. Ë∞ÉÁî®ÊçüÂ§±ÂáΩÊï∞ÔºàÊó†ËÆ∫Âì™Áßç lossÔºåÈÉΩ‰ºöÊî∂Âà∞ÂêàÈÄÇÁöÑÂèÇÊï∞Ôºâ
                    losses = loss(**loss_kwargs, output_dict=True)
                else:
                    # Ê†áÂáÜËÆ≠ÁªÉÂâçÂêë‰º†Êí≠
                    model_out = model(images, texts)
                    logit_scale = model_out["logit_scale"]
                    if args.distill:
                        with torch.no_grad():
                            dist_model_out = dist_model(images, texts)
                        model_out.update({f'dist_{k}': v for k, v in dist_model_out.items()})
                    losses = loss(**model_out, output_dict=True)

                total_loss = sum(losses.values())
                losses["loss"] = total_loss

            backward(total_loss, scaler)
        else:
            # Ê¢ØÂ∫¶Á¥ØÁßØÈÄªËæë
            if args.use_spatial_dataset:
                # Ê≥®ÊÑèÔºöÂ¶ÇÂâçÊâÄËø∞ÔºåËøôÈáåÁöÑÊ¢ØÂ∫¶Á¥ØÁßØÂ∞öÊú™‰∏∫Á©∫Èó¥Êï∞ÊçÆÈõÜÂÆûÁé∞
                # ‰øùÁïôÂéüÂßãÁöÑ NotImplementedError ÊòØÊúÄÂáÜÁ°ÆÁöÑÂÅöÊ≥ï
                raise NotImplementedError("Gradient accumulation is not yet supported with the spatial dataset.")
            
            # ÂÖàÁî® no_grad Ê®°ÂºèËøõË°åÂâçÂêë‰º†Êí≠‰ª•Á¥ØÁßØÁâπÂæÅ
            with torch.no_grad():
                with autocast():
                    model_out = model(images, texts)
                    # ÂºπÂá∫ÈùûÁ¥ØÁßØÁöÑÈ°π
                    for f in ("logit_scale", "logit_bias"):
                        model_out.pop(f, None)
                    for key, val in model_out.items():
                        if key in accum_features:
                            accum_features[key].append(val)
                        else:
                            accum_features[key] = [val]
                accum_images.append(images)
                accum_texts.append(texts)

            # Â¶ÇÊûúÁ¥ØÁßØÁöÑÊ≠•Êï∞Êú™Êª°ÔºåÂàôÁªßÁª≠‰∏ã‰∏Ä‰∏™ micro-batch
            if ((i + 1) % args.accum_freq) > 0:
                continue

            # ÂΩìÁ¥ØÁßØÊª°ÂêéÔºåËÆ°ÁÆóÊ¢ØÂ∫¶
            optimizer.zero_grad()
            for j in range(args.accum_freq):
                images = accum_images[j]
                texts = accum_texts[j]
                with autocast():
                    model_out = model(images, texts)
                    inputs_no_accum = {}
                    inputs_no_accum["logit_scale"] = logit_scale = model_out.pop("logit_scale")
                    if "logit_bias" in model_out:
                        inputs_no_accum["logit_bias"] = model_out.pop("logit_bias")
                    
                    inputs = {}
                    for key, val in accum_features.items():
                        accumulated = accum_features[key]
                        # ÊãºÊé•Èô§‰∫ÜÂΩìÂâç micro-batch ‰πãÂ§ñÁöÑÊâÄÊúâÁ¥ØÁßØÁâπÂæÅÔºåÂπ∂ÊèíÂÖ•ÂΩìÂâç micro-batch ÁöÑÊñ∞ËÆ°ÁÆóÁªìÊûú
                        inputs[key] = torch.cat(accumulated[:j] + [model_out[key]] + accumulated[j + 1:])
                    
                    losses = loss(**inputs, **inputs_no_accum, output_dict=True)
                    del inputs
                    del inputs_no_accum
                    total_loss = sum(losses.values())
                    losses["loss"] = total_loss
                backward(total_loss, scaler)

        if scaler is not None:
            if args.horovod:
                optimizer.synchronize()
                scaler.unscale_(optimizer)
                if args.grad_clip_norm is not None:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip_norm, norm_type=2.0)
                with optimizer.skip_synchronize():
                    scaler.step(optimizer)
            else:
                if args.grad_clip_norm is not None:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip_norm, norm_type=2.0)
                scaler.step(optimizer)
            scaler.update()
        else:
            if args.grad_clip_norm is not None:
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip_norm, norm_type=2.0)
            optimizer.step()

        # Ê∏ÖÁ©∫Á¥ØÁßØÁöÑÁºìÂ≠ò
        if args.accum_freq > 1:
            accum_images, accum_texts, accum_features = [], [], {}

        # Ê¢ØÂ∫¶Êõ¥Êñ∞ÂêéÔºåÂØπ logit_scale ËøõË°åËåÉÂõ¥ÈôêÂà∂
        with torch.no_grad():
            unwrap_model(model).logit_scale.clamp_(0, math.log(100))

        batch_time_m.update(time.time() - end)
        end = time.time()
        batch_count = i_accum + 1
        if is_master(args) and (i_accum % args.log_every_n_steps == 0 or batch_count == num_batches_per_epoch):
            batch_size = len(images)
            num_samples = batch_count * batch_size * args.accum_freq * args.world_size
            samples_per_epoch = dataloader.num_samples
            percent_complete = 100.0 * batch_count / num_batches_per_epoch

            # Êõ¥Êñ∞Âπ∂ËÆ∞ÂΩïÊçüÂ§±
            for key, val in losses.items():
                if key not in losses_m:
                    losses_m[key] = AverageMeter()
                losses_m[key].update(val.item(), batch_size)

            logit_scale_scalar = logit_scale.item()
            loss_log = " ".join(
                [
                    f"{loss_name.capitalize()}: {loss_m.val:#.5g} ({loss_m.avg:#.5g})" 
                    for loss_name, loss_m in losses_m.items()
                ]
            )
            samples_per_second = args.accum_freq * args.batch_size * args.world_size / batch_time_m.val
            samples_per_second_per_gpu = args.accum_freq * args.batch_size / batch_time_m.val
            logging.info(
                f"Train Epoch: {epoch} [{num_samples:>{sample_digits}}/{samples_per_epoch} ({percent_complete:.0f}%)] "
                f"Data (t): {data_time_m.avg:.3f} "
                f"Batch (t): {batch_time_m.avg:.3f}, {samples_per_second:#g}/s, {samples_per_second_per_gpu:#g}/s/gpu "
                f"LR: {optimizer.param_groups[0]['lr']:.5e} "
                f"Logit Scale: {logit_scale_scalar:.3f} " + loss_log
            )

            # ÂáÜÂ§áÁî®‰∫é TensorBoard Âíå WandB ÁöÑÊó•ÂøóÊï∞ÊçÆ
            log_data = {
                "data_time": data_time_m.val,
                "batch_time": batch_time_m.val,
                "samples_per_second": samples_per_second,
                "samples_per_second_per_gpu": samples_per_second_per_gpu,
                "scale": logit_scale_scalar,
                "lr": optimizer.param_groups[0]["lr"]
            }            
            log_data.update({name:val.val for name,val in losses_m.items()})
            log_data = {"train/" + name: val for name, val in log_data.items()}

            if tb_writer is not None:
                for name, val in log_data.items():
                    tb_writer.add_scalar(name, val, step)
            
            if args.wandb:
                assert wandb is not None, 'Please install wandb.'
                wandb.log(log_data, step=step)
            
            # ÈáçÁΩÆËÆ°Êó∂Âô®
            batch_time_m.reset()
            data_time_m.reset()

def evaluate(model, data, epoch, args, tb_writer=None, tokenizer=None):
    metrics = {}
    if not is_master(args):
        return metrics
    device = torch.device(args.device)
    model.eval()

    zero_shot_metrics = zero_shot_eval(model, data, epoch, args, tokenizer=tokenizer)
    metrics.update(zero_shot_metrics)

    autocast = get_autocast(args.precision, device_type=device.type)
    input_dtype = get_input_dtype(args.precision)

    if 'val' in data and (args.val_frequency and ((epoch % args.val_frequency) == 0 or epoch == args.epochs)):
        dataloader = data['val'].dataloader
        num_samples = 0
        samples_per_val = dataloader.num_samples

        cumulative_loss = 0.0
        cumulative_gen_loss = 0.0
        all_image_features, all_text_features = [], []
        with torch.inference_mode():
            for i, batch in enumerate(dataloader):
                images, texts = batch
                images = images.to(device=device, dtype=input_dtype, non_blocking=True)
                texts = texts.to(device=device, non_blocking=True)

                with autocast():
                    model_out = model(images, texts)
                    image_features = model_out["image_features"]
                    text_features = model_out["text_features"]
                    logit_scale = model_out["logit_scale"]
                    
                    all_image_features.append(image_features.cpu())
                    all_text_features.append(text_features.cpu())
                    logit_scale = logit_scale.mean()
                    logits_per_image = logit_scale * image_features @ text_features.t()
                    logits_per_text = logits_per_image.t()

                    batch_size = images.shape[0]
                    labels = torch.arange(batch_size, device=device).long()
                    total_loss = (
                        F.cross_entropy(logits_per_image, labels) +
                        F.cross_entropy(logits_per_text, labels)
                    ) / 2

                    gen_loss = maybe_compute_generative_loss(model_out)

                cumulative_loss += total_loss * batch_size
                num_samples += batch_size
                if is_master(args) and (i % 100) == 0:
                    logging.info(
                        f"Eval Epoch: {epoch} [{num_samples} / {samples_per_val}]\t"
                        f"Clip Loss: {cumulative_loss / num_samples:.6f}\t")

                    if gen_loss is not None:
                        cumulative_gen_loss += gen_loss * batch_size
                        logging.info(
                            f"Generative Loss: {cumulative_gen_loss / num_samples:.6f}\t")

            val_metrics = get_clip_metrics(
                image_features=torch.cat(all_image_features),
                text_features=torch.cat(all_text_features),
                logit_scale=logit_scale.cpu(),
            )
            loss = cumulative_loss / num_samples
            metrics.update(
                {**val_metrics, "clip_val_loss": loss.item(), "epoch": epoch, "num_samples": num_samples}
            )
            if gen_loss is not None:
                gen_loss = cumulative_gen_loss / num_samples
                metrics.update({"val_generative_loss": gen_loss.item()})

    if not metrics:
        return metrics

    logging.info(
        f"Eval Epoch: {epoch} "
        + "\t".join([f"{k}: {round(v, 4):.4f}" for k, v in metrics.items()])
    )

    log_data = {"val/" + name: val for name, val in metrics.items()}

    if args.save_logs:
        if tb_writer is not None:
            for name, val in log_data.items():
                tb_writer.add_scalar(name, val, epoch)

        with open(os.path.join(args.checkpoint_path, "results.jsonl"), "a+") as f:
            f.write(json.dumps(metrics))
            f.write("\n")

    if args.wandb:
        assert wandb is not None, 'Please install wandb.'
        if 'train' in data:
            num_batches_per_epoch = data['train'].dataloader.num_batches // args.accum_freq
            step = num_batches_per_epoch * epoch
        else:
            step = None
        log_data['epoch'] = epoch
        wandb.log(log_data, step=step)

    return metrics


def get_clip_metrics(image_features, text_features, logit_scale):
    metrics = {}
    logits_per_image = (logit_scale * image_features @ text_features.t()).detach().cpu()
    logits_per_text = logits_per_image.t().detach().cpu()

    logits = {"image_to_text": logits_per_image, "text_to_image": logits_per_text}
    ground_truth = torch.arange(len(text_features)).view(-1, 1)

    for name, logit in logits.items():
        ranking = torch.argsort(logit, descending=True)
        preds = torch.where(ranking == ground_truth)[1]
        preds = preds.detach().cpu().numpy()
        metrics[f"{name}_mean_rank"] = preds.mean() + 1
        metrics[f"{name}_median_rank"] = np.floor(np.median(preds)) + 1
        for k in [1, 5, 10]:
            metrics[f"{name}_R@{k}"] = np.mean(preds < k)

    return metrics


def maybe_compute_generative_loss(model_out):
    if "logits" in model_out and "labels" in model_out:
        token_logits = model_out["logits"]
        token_labels = model_out["labels"]
        return F.cross_entropy(token_logits.permute(0, 2, 1), token_labels)
===== src/open_clip_train/distributed.py =====
import os
import warnings
from typing import Optional

import torch
import torch.distributed as dist

try:
    import horovod.torch as hvd
except ImportError:
    hvd = None


def is_global_master(args):
    return args.rank == 0


def is_local_master(args):
    return args.local_rank == 0


def is_master(args, local=False):
    return is_local_master(args) if local else is_global_master(args)


def is_device_available(device):
    device_type = torch.device(device).type
    is_avail = False
    is_known = False
    if device_type == 'cuda':
        is_avail = torch.cuda.is_available()
        is_known = True
    elif device_type == 'npu':
        # NOTE autoload device extension needed for this not to error out on this check
        is_avail = torch.npu.is_available()
        is_known = True
    elif device_type == 'mps':
        is_avail = torch.backends.mps.is_available()
        is_known = True
    elif device_type == 'cpu':
        is_avail = True
        is_known = True

    return is_avail, is_known


def set_device(device):
    if device.startswith('cuda:'):
        torch.cuda.set_device(device)
    elif device.startswith('npu:'):
        torch.npu.set_device(device)


def is_using_horovod():
    # NOTE w/ horovod run, OMPI vars should be set, but w/ SLURM PMI vars will be set
    # Differentiating between horovod and DDP use via SLURM may not be possible, so horovod arg still required...
    ompi_vars = ["OMPI_COMM_WORLD_RANK", "OMPI_COMM_WORLD_SIZE"]
    pmi_vars = ["PMI_RANK", "PMI_SIZE"]
    if all([var in os.environ for var in ompi_vars]) or all([var in os.environ for var in pmi_vars]):
        return True
    else:
        return False


def is_using_distributed():
    if 'WORLD_SIZE' in os.environ:
        return int(os.environ['WORLD_SIZE']) > 1
    if 'SLURM_NTASKS' in os.environ:
        return int(os.environ['SLURM_NTASKS']) > 1
    return False


def world_info_from_env():
    local_rank = 0
    for v in ('LOCAL_RANK', 'MPI_LOCALRANKID', 'SLURM_LOCALID', 'OMPI_COMM_WORLD_LOCAL_RANK'):
        if v in os.environ:
            local_rank = int(os.environ[v])
            break
    global_rank = 0
    for v in ('RANK', 'PMI_RANK', 'SLURM_PROCID', 'OMPI_COMM_WORLD_RANK'):
        if v in os.environ:
            global_rank = int(os.environ[v])
            break
    world_size = 1
    for v in ('WORLD_SIZE', 'PMI_SIZE', 'SLURM_NTASKS', 'OMPI_COMM_WORLD_SIZE'):
        if v in os.environ:
            world_size = int(os.environ[v])
            break

    return local_rank, global_rank, world_size


def init_distributed_device(args):
    # Distributed training = training on more than one GPU.
    # Works in both single and multi-node scenarios.
    args.distributed = False
    args.world_size = 1
    args.rank = 0  # global rank
    args.local_rank = 0
    result = init_distributed_device_so(
        device=getattr(args, 'device', 'cuda'),
        dist_backend=getattr(args, 'dist_backend', None),
        dist_url=getattr(args, 'dist_url', None),
        horovod=getattr(args, 'horovod', False),
        no_set_device_rank=getattr(args, 'no_set_device_rank', False),
    )
    args.device = result['device']
    args.world_size = result['world_size']
    args.rank = result['global_rank']
    args.local_rank = result['local_rank']
    args.distributed = result['distributed']
    device = torch.device(args.device)
    return device


def init_distributed_device_so(
        device: str = 'cuda',
        dist_backend: Optional[str] = None,
        dist_url: Optional[str] = None,
        horovod: bool = False,
        no_set_device_rank: bool = False,
):
    # Distributed training = training on more than one GPU.
    # Works in both single and multi-node scenarios.
    distributed = False
    world_size = 1
    global_rank = 0
    local_rank = 0
    device_type, *device_idx = device.split(':', maxsplit=1)
    is_avail, is_known = is_device_available(device_type)
    if not is_known:
        warnings.warn(f"Device {device} was not known and checked for availability, trying anyways.")
    elif not is_avail:
        warnings.warn(f"Device {device} was not available, falling back to CPU.")
        device_type = device = 'cpu'

    if horovod:
        import horovod.torch as hvd
        assert hvd is not None, "Horovod is not installed"
        hvd.init()
        local_rank = int(hvd.local_rank())
        global_rank = hvd.rank()
        world_size = hvd.size()
        distributed = True
    elif is_using_distributed():
        if dist_backend is None:
            dist_backends = {
                "cuda": "nccl",
                "hpu": "hccl",
                "npu": "hccl",
                "xpu": "ccl",
            }
            dist_backend = dist_backends.get(device_type, 'gloo')

        dist_url = dist_url or 'env://'

        if 'SLURM_PROCID' in os.environ:
            # DDP via SLURM
            local_rank, global_rank, world_size = world_info_from_env()
            # SLURM var -> torch.distributed vars in case needed
            os.environ['LOCAL_RANK'] = str(local_rank)
            os.environ['RANK'] = str(global_rank)
            os.environ['WORLD_SIZE'] = str(world_size)
            torch.distributed.init_process_group(
                backend=dist_backend,
                init_method=dist_url,
                world_size=world_size,
                rank=global_rank,
            )
        else:
            # DDP via torchrun, torch.distributed.launch
            local_rank, _, _ = world_info_from_env()
            torch.distributed.init_process_group(
                backend=dist_backend,
                init_method=dist_url,
            )
            world_size = torch.distributed.get_world_size()
            global_rank = torch.distributed.get_rank()
        distributed = True

    if distributed and not no_set_device_rank and device_type not in ('cpu', 'mps'):
        # Ignore manually specified device index in distributed mode and
        # override with resolved local rank, fewer headaches in most setups.
        if device_idx:
            warnings.warn(f'device index {device_idx[0]} removed from specified ({device}).')
        device = f'{device_type}:{local_rank}'
        set_device(device)

    return dict(
        device=device,
        global_rank=global_rank,
        local_rank=local_rank,
        world_size=world_size,
        distributed=distributed,
    )


def broadcast_object(args, obj, src=0):
    # broadcast a pickle-able python object from rank-0 to all ranks
    if args.horovod:
        return hvd.broadcast_object(obj, root_rank=src)
    else:
        if args.rank == src:
            objects = [obj]
        else:
            objects = [None]
        dist.broadcast_object_list(objects, src=src)
        return objects[0]


def all_gather_object(args, obj, dst=0):
    # gather a pickle-able python object across all ranks
    if args.horovod:
        return hvd.allgather_object(obj)
    else:
        objects = [None for _ in range(args.world_size)]
        dist.all_gather_object(objects, obj)
        return objects

===== src/open_clip_train/spatial_loss.py =====
# /path/to/your/open_clip/src/open_clip_train/spatial_loss.py
import torch
import torch.distributed as dist
import torch.nn.functional as F

from open_clip import ClipLoss
from open_clip.loss import gather_features


class GlobalMappingMultiPositiveClipLoss(ClipLoss):
    """
    Á©∫Èó¥Â§öÊ≠£Ê†∑Êú¨Áâà CLIP ÊçüÂ§±Ôºö
      - ÊîØÊåÅÊääÈÇªÂ±ÖÊùÉÈáçÊò†Â∞ÑÂà∞‚ÄúÂÖ®Â±Ä‚Äùlogits ‰∏äÔºàË∑® GPUÔºâ„ÄÇ
      - Ê∏©Â∫¶ s ÁöÑ‰∏äÈôêÈááÁî®Áõ¥ÈÄö‰º∞ËÆ°ÔºàSTEÔºâÔºöÂâçÂêëË£ÅÂâ™„ÄÅÂèçÂêëÂØπÂéüÂÄºÊ±ÇÊ¢ØÂ∫¶„ÄÇ
      - ÂèØÈÄâÊää logits ËΩ¨‰∏∫ float32 ÂÅö log_softmaxÔºåÊèêÂçáÊï∞ÂÄºÁ®≥ÂÆöÊÄßÔºàÊ∑∑Á≤æ‰∏ãÂª∫ËÆÆÂºÄÔºâ„ÄÇ
      - ÂèØÈÄâÊ∏©Â∫¶Ê≠£ÂàôÔºöÊää E_{p_s}[z] ÊãâÂêë <q,z>ÔºåÈÅøÂÖç s ËøáÁÉ≠„ÄÇ
    ËøîÂõûÔºö
      - ÂΩì output_dict=TrueÔºå‰ªÖËøîÂõû {"contrastive_loss": total_loss}
        ÔºàÈÅøÂÖç‰Ω†ÁöÑËÆ≠ÁªÉÂæ™ÁéØÊääÊåáÊ†á‰πüÁõ∏Âä†ËøõÊÄªÊçüÂ§±Ôºâ
    """

    def __init__(
        self,
        *args,
        cap_logit_scale: float = None,     # ÊúâÊïàÊ∏©Â∫¶‰∏äÈôê s_maxÔºàÂØπ exp ÂêéÁöÑ s ÁîüÊïàÔºâ
        temp_reg_weight: float = 0.0,      # Ê∏©Â∫¶Ê≠£ÂàôÊùÉÈáçÔºà0 ÂÖ≥Èó≠Ôºâ
        float32_logits: bool = False,      # Âú® log-softmax ÂâçÊää logits ËΩ¨‰∏∫ fp32
        neighbor_alpha_scale: float = 1.0, # ÈÇªÂ±ÖÊùÉÈáçÊï¥‰ΩìÁº©Êîæ
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.cap_logit_scale = cap_logit_scale
        self.temp_reg_weight = temp_reg_weight
        self.float32_logits = float32_logits
        self.neighbor_alpha_scale = neighbor_alpha_scale

    def forward(
        self,
        image_features: torch.Tensor,
        text_features: torch.Tensor,
        image_tile_ids: torch.Tensor,
        text_tile_ids: torch.Tensor,
        neighbor_tile_ids: torch.Tensor,
        neighbor_alphas: torch.Tensor,
        logit_scale: torch.Tensor,         # Ê≥®ÊÑèÔºöËøôÈáåÊé•Êî∂ÁöÑÊòØ exp ÂêéÁöÑ sÔºàÊ†áÈáèÂº†ÈáèÊàñÂèØÂπøÊí≠Ôºâ
        logit_bias: torch.Tensor = None,
        output_dict: bool = False,
    ):
        device = image_features.device

        # ===== 1) Ë∑® GPU gatherÔºà‰∏é OpenCLIP ÁöÑÂÆûÁé∞‰øùÊåÅ‰∏ÄËá¥Ôºâ=====
        if self.world_size > 1:
            all_image_features, all_text_features = gather_features(
                image_features=image_features,
                text_features=text_features,
                local_loss=self.local_loss,
                gather_with_grad=self.gather_with_grad,
                rank=self.rank,
                world_size=self.world_size,
                use_horovod=self.use_horovod,
            )
            # ÂêåÊ≠• tile_ids Áî®Êù•ÂÆö‰ΩçÂÖ®Â±ÄÂàóÂè∑
            gathered_img_ids = [torch.empty_like(image_tile_ids) for _ in range(self.world_size)]
            gathered_txt_ids = [torch.empty_like(text_tile_ids) for _ in range(self.world_size)]
            dist.all_gather(gathered_img_ids, image_tile_ids)
            dist.all_gather(gathered_txt_ids, text_tile_ids)
            all_image_tile_ids = torch.cat(gathered_img_ids)
            all_text_tile_ids = torch.cat(gathered_txt_ids)
        else:
            all_image_features, all_text_features = image_features, text_features
            all_image_tile_ids, all_text_tile_ids = image_tile_ids, text_tile_ids

        # ===== 2) ÊúâÊïàÊ∏©Â∫¶ s ÁöÑ‚ÄúËΩØ‰∏äÈôê‚ÄùÔºàÁõ¥ÈÄö‰º∞ËÆ°ÔºåÂâçÂêëË£ÅÂâ™„ÄÅÂèçÂêë‰øùÊ¢ØÂ∫¶Ôºâ=====
        s_eff = logit_scale
        if self.cap_logit_scale is not None:
            s_clipped = torch.clamp(logit_scale, max=self.cap_logit_scale)
            # STEÔºö (s_clipped - s).detach() ‰∏çÂèÇ‰∏éÂèç‰º†ÔºåÁ≠â‰ª∑‰∫éÂâçÂêëÁî®Ë£ÅÂâ™ÂÄºÔºåÂèçÂêëÂØπÂéü s Ê±ÇÊ¢ØÂ∫¶
            s_eff = logit_scale + (s_clipped - logit_scale).detach()

        # ===== 3) Áõ∏‰ººÂ∫¶ -> logitsÔºàÂèØÈÄâËΩ¨ fp32 ÂÜçÂÅö log-softmax Êõ¥Á®≥Ôºâ=====
        z_i_t = image_features @ all_text_features.T   # [B, G]
        z_t_i = text_features  @ all_image_features.T  # [B, G]
        logits_per_image = s_eff * z_i_t
        logits_per_text  = s_eff * z_t_i
        if logit_bias is not None:
            logits_per_image = logits_per_image + logit_bias
            logits_per_text  = logits_per_text  + logit_bias
        if self.float32_logits:
            logits_per_image = logits_per_image.float()
            logits_per_text  = logits_per_text.float()

        # ===== 4) ÊûÑÈÄ†ËΩØÊ†áÁ≠æÔºàÊääÈÇªÂ±ÖÊùÉÈáçÊò†Â∞ÑÂà∞‚ÄúÂÖ®Â±ÄÂàó‚ÄùÔºâ=====
        B_local = image_features.size(0)
        N_global = all_image_features.size(0)

        # Âª∫Á´ã tile_id -> ÂÖ®Â±ÄÂàóÂè∑ ÁöÑÊü•ÊâæË°®
        txt_id_to_idx = {tid.item(): i for i, tid in enumerate(all_text_tile_ids)}
        img_id_to_idx = {tid.item(): i for i, tid in enumerate(all_image_tile_ids)}

        # ÂØπËßíÔºàÂêå‰ΩçÔºâÊ≠£Ê†∑Êú¨ÁöÑÂÖ®Â±ÄÂàóÂè∑Ôºàlocal_loss ËØ≠‰πâÔºöË°åÊòØÊú¨Âú∞Ê†∑Êú¨ÔºåÂàóÊòØÂÖ®Â±ÄÔºâ
        ground_truth = torch.arange(B_local, device=device) + B_local * self.rank

        # ÂàùÂßãÂåñ one-hotÔºåÂÜçÂè†Âä†ÈÇªÂ±ÖÊùÉÈáç
        labels_i_t = torch.zeros(B_local, N_global, device=device)
        labels_t_i = torch.zeros(B_local, N_global, device=device)
        labels_i_t.scatter_(1, ground_truth.unsqueeze(1), 1.0)
        labels_t_i.scatter_(1, ground_truth.unsqueeze(1), 1.0)

        # ÈÇªÂ±ÖÊï¥‰ΩìÁº©Êîæ & ÈùûË¥üË£ÅÂâ™
        if self.neighbor_alpha_scale != 1.0:
            neighbor_alphas = neighbor_alphas * self.neighbor_alpha_scale
        neighbor_alphas = neighbor_alphas.clamp_min(0)

        # Â∞ÜÈÇªÂ±ÖÊùÉÈáçÂä†Âà∞ÂØπÂ∫îÂÖ®Â±ÄÂàó
        for i in range(B_local):
            for nbr_id, alpha in zip(neighbor_tile_ids[i], neighbor_alphas[i]):
                if alpha.item() <= 0:
                    continue
                # ÂõæÂÉè->ÊñáÊú¨ÔºàË°åÊòØ imageÔºåÂàóÊòØ all_textÔºâ
                col_txt = txt_id_to_idx.get(int(nbr_id.item()))
                if col_txt is not None:
                    labels_i_t[i, col_txt] += float(alpha.item())
                # ÊñáÊú¨->ÂõæÂÉèÔºàË°åÊòØ textÔºåÂàóÊòØ all_imageÔºâ
                col_img = img_id_to_idx.get(int(nbr_id.item()))
                if col_img is not None:
                    labels_t_i[i, col_img] += float(alpha.item())

        # ÂΩí‰∏ÄÂåñÊàêÂàÜÂ∏É
        labels_i_t = F.normalize(labels_i_t, p=1, dim=1)
        labels_t_i = F.normalize(labels_t_i, p=1, dim=1)

        # ===== 5) ‰∏ªÊçüÂ§±ÔºàÂØπÁß∞ KLÔºö-<q, log p>Ôºâ=====
        loss_i = -torch.sum(F.log_softmax(logits_per_image, dim=1) * labels_i_t, dim=1).mean()
        loss_t = -torch.sum(F.log_softmax(logits_per_text , dim=1) * labels_t_i, dim=1).mean()
        total_loss = 0.5 * (loss_i + loss_t)

        # ===== 6) ÂèØÈÄâÔºöÊ∏©Â∫¶Ê≠£ÂàôÔºàÊää E_{p_s}[z] ÊãâËøë E_q[z]Ôºâ=====
        if self.temp_reg_weight and self.temp_reg_weight > 0:
            # Ê≥®ÊÑèËøôÈáåÁî®‚ÄúÊú™Áº©Êîæ‚ÄùÁöÑ z Êù•Ë°°ÈáèÊ∏©Â∫¶ÂºïËµ∑ÁöÑ‚ÄúËøáÊãüÂêàËΩØÁõÆÊ†á‚ÄùÁöÑÂÅèÁßª
            p_i = F.softmax(logits_per_image, dim=1)
            p_t = F.softmax(logits_per_text , dim=1)

            Ez_p_i = (p_i * z_i_t).sum(dim=1).mean()
            Ez_q_i = (labels_i_t * z_i_t).sum(dim=1).mean()
            Ez_p_t = (p_t * z_t_i).sum(dim=1).mean()
            Ez_q_t = (labels_t_i * z_t_i).sum(dim=1).mean()

            gap = 0.5 * ((Ez_p_i - Ez_q_i) + (Ez_p_t - Ez_q_t))
            total_loss = total_loss + self.temp_reg_weight * (gap ** 2)

        # ===== 7) ËæìÂá∫ =====
        if output_dict:
            # Âè™ËøîÂõûÁúüÊ≠£ÂèÇ‰∏éÂèç‰º†ÁöÑÊçüÂ§±ÔºåÈÅøÂÖçË¢´ËÆ≠ÁªÉÂæ™ÁéØ‚ÄúÂÖ®ÈáèÁõ∏Âä†‚ÄùÂΩ±ÂìçÊÄªÊçüÂ§±
            return {"contrastive_loss": total_loss}
        return total_loss

===== src/open_clip_train/profiler.py =====
import argparse

import torch
import open_clip
import pandas as pd
from torch.utils.flop_counter import FlopCounterMode
try:
    import fvcore
    import fvcore.nn
except:
    fvcore = None

parser = argparse.ArgumentParser(description='OpenCLIP Profiler')

# benchmark specific args
parser.add_argument('--model', metavar='NAME', default='',
                    help='model(s) to profile')
parser.add_argument('--results-file', default='', type=str, metavar='FILENAME',
                    help='Output csv file for results')
parser.add_argument('--profiler', default='torch', type=str, choices=['torch', 'fvcore'])
parser.add_argument('--batch-size', default=1, type=int, help='Batch size for profiling')
parser.add_argument(
    "--device", default="cuda", type=str, help="Accelerator to use."
)

def profile_fvcore(
        model,
        image_input_size=(3, 224, 224),
        text_input_size=(77,),
        batch_size=1,
        detailed=False,
        force_cpu=False
):
    if force_cpu:
        model = model.to('cpu')
    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype
    example_image_input = torch.ones((batch_size,) + image_input_size, device=device, dtype=dtype)
    example_text_input = torch.ones((batch_size,) + text_input_size, device=device, dtype=torch.int64)
    fca = fvcore.nn.FlopCountAnalysis(model, (example_image_input, example_text_input))
    aca = fvcore.nn.ActivationCountAnalysis(model, (example_image_input, example_text_input))
    if detailed:
        fcs = fvcore.nn.flop_count_str(fca)
        print(fcs)
    return fca.total() / batch_size, aca.total() / batch_size


def profile_fvcore_text(
        model,
        text_input_size=(77,),
        batch_size=1,
        detailed=False,
        force_cpu=False
):
    if force_cpu:
        model = model.to('cpu')
    device = next(model.parameters()).device
    example_input = torch.ones((batch_size,) + text_input_size, device=device, dtype=torch.int64)
    fca = fvcore.nn.FlopCountAnalysis(model, example_input)
    aca = fvcore.nn.ActivationCountAnalysis(model, example_input)
    if detailed:
        fcs = fvcore.nn.flop_count_str(fca)
        print(fcs)
    return fca.total() / batch_size, aca.total() / batch_size


def profile_fvcore_image(
        model,
        image_input_size=(3, 224, 224),
        batch_size=1,
        detailed=False,
        force_cpu=False
):
    if force_cpu:
        model = model.to('cpu')
    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype
    example_input = torch.ones((batch_size,) + image_input_size, device=device, dtype=dtype)
    fca = fvcore.nn.FlopCountAnalysis(model, example_input)
    aca = fvcore.nn.ActivationCountAnalysis(model, example_input)
    if detailed:
        fcs = fvcore.nn.flop_count_str(fca)
        print(fcs)
    return fca.total() / batch_size, aca.total() / batch_size


def profile_torch_image(model, image_input_size, batch_size=1, force_cpu=False):
    """Profile the image encoder using torch.utils.flop_counter"""
    if force_cpu:
        model = model.to('cpu')
    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype
    example_input = torch.ones((batch_size,) + image_input_size, device=device, dtype=dtype)

    flop_counter = FlopCounterMode()
    with flop_counter:
        model(example_input)
    total_flops = sum(flop_counter.get_flop_counts()['Global'].values())
    return total_flops / batch_size


def profile_torch_text(model, text_input_size, batch_size=1, force_cpu=False):
    """Profile the text encoder using torch.utils.flop_counter"""
    if force_cpu:
        model = model.to('cpu')
    device = next(model.parameters()).device
    example_input = torch.ones((batch_size,) + text_input_size, device=device, dtype=torch.int64)

    flop_counter = FlopCounterMode()
    with flop_counter:
        model(example_input)
    total_flops = sum(flop_counter.get_flop_counts()['Global'].values())
    return total_flops / batch_size


def profile_torch(model, text_input_size, image_input_size, batch_size=1, force_cpu=False):
    """Profile the full model using torch.utils.flop_counter"""
    if force_cpu:
        model = model.to('cpu')
    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype
    image_input = torch.ones((batch_size,) + image_input_size, device=device, dtype=dtype)
    text_input = torch.ones((batch_size,) + text_input_size, device=device, dtype=torch.int64)

    flop_counter = FlopCounterMode()
    with flop_counter:
        model(image_input, text_input)
    total_flops = sum(flop_counter.get_flop_counts()['Global'].values())
    return total_flops / batch_size


def count_params(model):
    return sum(m.numel() for m in model.parameters())

def profile_model(model_name, batch_size=1, profiler='torch', device="cuda"):
    assert profiler in ['torch', 'fvcore'], 'Only torch and fvcore profilers are supported'
    if profiler == 'fvcore':
        assert fvcore is not None, 'Please install fvcore.'
    model = open_clip.create_model(model_name, force_custom_text=True, pretrained_hf=False)
    model.eval()

    if torch.cuda.is_available():
        model = model.cuda()
    elif device == "npu" and torch.npu.is_available():
        model = model.npu()

    if isinstance(model.visual.image_size, (tuple, list)):
        image_input_size = (3,) + tuple(model.visual.image_size[-2:])
    else:
        image_input_size = (3, model.visual.image_size, model.visual.image_size)

    text_input_size = (77,)
    if hasattr(model, 'context_length') and model.context_length:
        text_input_size = (model.context_length,)

    results = {}
    results['model'] = model_name
    results['image_size'] = image_input_size[1]

    model_cfg = open_clip.get_model_config(model_name)
    if model_cfg:
        vision_cfg = open_clip.CLIPVisionCfg(**model_cfg['vision_cfg'])
        text_cfg = open_clip.CLIPTextCfg(**model_cfg['text_cfg'])
        results['image_width'] = int(vision_cfg.width)
        results['text_width'] = int(text_cfg.width)
        results['embed_dim'] = int(model_cfg['embed_dim'])
    else:
        results['image_width'] = 0
        results['text_width'] = 0
        results['embed_dim'] = 0

    retries = 2
    while retries:
        retries -= 1
        try:
            results['mparams'] = round(count_params(model) / 1e6, 2)
            results['image_mparams'] = round(count_params(model.visual) / 1e6, 2)
            results['text_mparams'] = round(count_params(model.text) / 1e6, 2)

            if profiler == 'fvcore':
                macs, acts = profile_fvcore(
                    model, image_input_size=image_input_size, text_input_size=text_input_size, force_cpu=not retries, batch_size=batch_size)

                image_macs, image_acts = profile_fvcore_image(
                    model.visual, image_input_size=image_input_size, force_cpu=not retries, batch_size=batch_size)

                text_macs, text_acts = profile_fvcore_text(
                    model.text, text_input_size=text_input_size, force_cpu=not retries, batch_size=batch_size)

                results['gmacs'] = round(macs / 1e9, 2)
                results['macts'] = round(acts / 1e6, 2)
                
                results['image_gmacs'] = round(image_macs / 1e9, 2)
                results['image_macts'] = round(image_acts / 1e6, 2)
                
                results['text_gmacs'] = round(text_macs / 1e9, 2)
                results['text_macts'] = round(text_acts / 1e6, 2)
            elif profiler == 'torch':
                image_flops = profile_torch_image(
                    model.visual, image_input_size=image_input_size, force_cpu=not retries, batch_size=batch_size)
                text_flops = profile_torch_text(
                    model.text, text_input_size=text_input_size, force_cpu=not retries, batch_size=batch_size)
                total_flops = profile_torch(
                    model, image_input_size=image_input_size, text_input_size=text_input_size, force_cpu=not retries, batch_size=batch_size)

                results['gflops'] = round(total_flops / 1e9, 2)
                results['image_gflops'] = round(image_flops / 1e9, 2)
                results['text_gflops'] = round(text_flops / 1e9, 2)

        except RuntimeError as e:
            pass
    return results


def main():
    args = parser.parse_args()

    # FIXME accept a text file name to allow lists of models in txt/csv
    if args.model == 'all':
        parsed_model = open_clip.list_models()
    else:
        parsed_model = args.model.split(',')

    results = []
    models_with_errors = []
    for m in parsed_model:
        print('='*100)
        print(f'Profiling {m}')
        try:
            row = profile_model(m, batch_size=args.batch_size, profiler=args.profiler, device=args.device)
            results.append(row)
        except Exception as e:
            print(f'Error profiling {m}: {e}')
            import traceback
            traceback.print_exc()
            models_with_errors.append(m)

    df = pd.DataFrame(results, columns=results[0].keys())

    if 'gmacs' in df.columns:
        df = df.sort_values(by=['gmacs', 'mparams', 'model'])
    else:
        df = df.sort_values(by=['gflops', 'mparams', 'model'])

    print('='*100)
    print('Done.')
    print(df)
    if args.results_file:
        df.to_csv(args.results_file, index=False)

    if models_with_errors:
        print('Models with errors:', models_with_errors)


if __name__ == '__main__':
    main()

===== src/open_clip_train/logger.py =====
import logging


def setup_logging(log_file, level, include_host=False):
    if include_host:
        import socket
        hostname = socket.gethostname()
        formatter = logging.Formatter(
            f'%(asctime)s |  {hostname} | %(levelname)s | %(message)s', datefmt='%Y-%m-%d,%H:%M:%S')
    else:
        formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s', datefmt='%Y-%m-%d,%H:%M:%S')

    logging.root.setLevel(level)
    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
    for logger in loggers:
        logger.setLevel(level)

    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)
    logging.root.addHandler(stream_handler)

    if log_file:
        file_handler = logging.FileHandler(filename=log_file)
        file_handler.setFormatter(formatter)
        logging.root.addHandler(file_handler)


===== src/open_clip/hf_model.py =====
""" huggingface model adapter

Wraps HuggingFace transformers (https://github.com/huggingface/transformers) models for use as a text tower in CLIP model.
"""
import re

import torch
import torch.nn as nn
from torch import TensorType

try:
    import transformers
    from transformers import AutoModel, AutoTokenizer, AutoConfig, PretrainedConfig
    from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, \
        BaseModelOutputWithPoolingAndCrossAttentions
except ImportError as e:
    transformers = None


    class BaseModelOutput:
        pass


    class PretrainedConfig:
        pass

from .hf_configs import arch_dict


# utils
def _camel2snake(s):
    return re.sub(r'(?<!^)(?=[A-Z])', '_', s).lower()


# TODO: ?last - for gpt-like models
_POOLERS = {}


def register_pooler(cls):
    """Decorator registering pooler class"""
    _POOLERS[_camel2snake(cls.__name__)] = cls
    return cls


@register_pooler
class MeanPooler(nn.Module):
    """Mean pooling"""

    def forward(self, x: BaseModelOutput, attention_mask: TensorType):
        masked_output = x.last_hidden_state * attention_mask.unsqueeze(-1)
        return masked_output.sum(dim=1) / attention_mask.sum(-1, keepdim=True)


@register_pooler
class MaxPooler(nn.Module):
    """Max pooling"""

    def forward(self, x: BaseModelOutput, attention_mask: TensorType):
        masked_output = x.last_hidden_state.masked_fill(attention_mask.unsqueeze(-1), -torch.inf)
        return masked_output.max(1).values


@register_pooler
class ClsPooler(nn.Module):
    """CLS token pooling"""

    def __init__(self, use_pooler_output=True):
        super().__init__()
        self.cls_token_position = 0
        self.use_pooler_output = use_pooler_output

    def forward(self, x: BaseModelOutput, attention_mask: TensorType):
        if (self.use_pooler_output and
            isinstance(x, (BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions)) and
            (x.pooler_output is not None)
        ):
            return x.pooler_output

        return x.last_hidden_state[:, self.cls_token_position, :]


@register_pooler
class ClsLastHiddenStatePooler(nn.Module):
    """CLS token pooling
    NOTE: this is equivalent to ClsPooler above with use_pooler_output=False
    """

    def __init__(self):
        super().__init__()
        self.cls_token_position = 0

    def forward(self, x: BaseModelOutput, attention_mask: TensorType):
        return x.last_hidden_state[:, self.cls_token_position, :]


class HFTextEncoder(nn.Module):
    """HuggingFace model adapter"""
    output_tokens: torch.jit.Final[bool]

    def __init__(
            self,
            model_name_or_path: str,
            output_dim: int,
            config: PretrainedConfig = None,
            pooler_type: str = None,
            proj_type: str = None,
            pretrained: bool = True,
            output_tokens: bool = False,
    ):
        super().__init__()
        self.output_tokens = output_tokens
        self.output_dim = output_dim

        # TODO: find better way to get this information
        uses_transformer_pooler = (pooler_type == "cls_pooler")

        if transformers is None:
            raise RuntimeError("Please `pip install transformers` to use pre-trained HuggingFace models")
        if config is None:
            self.config = AutoConfig.from_pretrained(model_name_or_path)
            create_func, model_args = (AutoModel.from_pretrained, model_name_or_path) if pretrained else (
                AutoModel.from_config, self.config)
            # TODO: do all model configs have this attribute? PretrainedConfig does so yes??
            if hasattr(self.config, "is_encoder_decoder") and self.config.is_encoder_decoder:
                self.transformer = create_func(model_args)
                self.transformer = self.transformer.encoder
            else:
                self.transformer = create_func(model_args, add_pooling_layer=uses_transformer_pooler)
        else:
            self.config = config
            self.transformer = AutoModel.from_config(config)
        if pooler_type is None:  # get default arch pooler
            pooler_type = (arch_dict[self.config.model_type]["pooler"])

        # FIXME downstream users of OpenCLIP models use these attr, need to verify valid across all models
        self.vocab_size = getattr(self.config, 'vocab_size', 0)
        self.context_length = getattr(self.config, 'max_position_embeddings', 0)

        self.pooler = _POOLERS[pooler_type]()

        d_model = getattr(self.config, arch_dict[self.config.model_type]["config_names"]["width"])
        if (d_model == output_dim) and (proj_type is None):  # do we always need a proj?
            self.proj = nn.Identity()
        elif proj_type == 'linear':
            self.proj = nn.Linear(d_model, output_dim, bias=False)
        elif proj_type == 'mlp':
            hidden_size = (d_model + output_dim) // 2
            self.proj = nn.Sequential(
                nn.Linear(d_model, hidden_size, bias=False),
                nn.GELU(),
                nn.Linear(hidden_size, output_dim, bias=False),
            )

    def forward(self, x: TensorType):
        attn_mask = (x != self.config.pad_token_id).long()
        out = self.transformer(input_ids=x, attention_mask=attn_mask)
        pooled_out = self.pooler(out, attn_mask)
        projected = self.proj(pooled_out)

        seq_len = out.last_hidden_state.shape[1]
        tokens = (
            out.last_hidden_state[:, torch.arange(seq_len) != self.pooler.cls_token_position, :] 
            if type(self.pooler) == ClsPooler 
            else out.last_hidden_state
        )
        
        if self.output_tokens:
            return projected, tokens
        return projected

    def lock(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):
        if not unlocked_layers:  # full freezing
            for n, p in self.transformer.named_parameters():
                p.requires_grad = (not freeze_layer_norm) if "LayerNorm" in n.split(".") else False
            return

        encoder = self.transformer.encoder if hasattr(self.transformer, 'encoder') else self.transformer
        layer_list = getattr(encoder, arch_dict[self.config.model_type]["config_names"]["layer_attr"])
        print(f"Unlocking {unlocked_layers}/{len(layer_list) + 1} layers of hf model")
        embeddings = getattr(
            self.transformer, arch_dict[self.config.model_type]["config_names"]["token_embeddings_attr"])
        modules = [embeddings, *layer_list][:-unlocked_layers]
        # freeze layers
        for module in modules:
            for n, p in module.named_parameters():
                p.requires_grad = (not freeze_layer_norm) if "LayerNorm" in n.split(".") else False

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable=True):
        self.transformer.gradient_checkpointing_enable()

    def init_parameters(self):
        pass

===== src/open_clip/tokenizer.py =====
""" CLIP tokenizer

Copied from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.
"""
import gzip
import html
import os
import random
import string
from functools import lru_cache, partial
from typing import Callable, List, Optional, Union, Dict
import warnings

import ftfy
import numpy as np
import regex as re
import torch

# https://stackoverflow.com/q/62691279
os.environ["TOKENIZERS_PARALLELISM"] = "false"
_nltk_init = False

DEFAULT_CONTEXT_LENGTH = 77  # default context length for OpenAI CLIP


@lru_cache()
def default_bpe():
    return os.path.join(os.path.dirname(os.path.abspath(__file__)), "bpe_simple_vocab_16e6.txt.gz")


@lru_cache()
def bytes_to_unicode():
    """
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a significant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    """
    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("¬°"), ord("¬¨")+1))+list(range(ord("¬Æ"), ord("√ø")+1))
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8+n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))


def get_pairs(word):
    """Return set of symbol pairs in a word.
    Word is represented as tuple of symbols (symbols being variable-length strings).
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs


def basic_clean(text):
    text = ftfy.fix_text(text)
    text = html.unescape(html.unescape(text))
    return text.strip()


def whitespace_clean(text):
    text = " ".join(text.split())
    text = text.strip()
    return text


def _clean_canonicalize(x):
    # basic, remove whitespace, remove punctuation, lower case
    return canonicalize_text(basic_clean(x))


def _clean_lower(x):
    # basic, remove whitespace, lower case
    return whitespace_clean(basic_clean(x)).lower()


def _clean_whitespace(x):
    # basic, remove whitespace
    return whitespace_clean(basic_clean(x))


def get_clean_fn(type: str):
    if type == 'canonicalize':
        return _clean_canonicalize
    elif type == 'lower':
        return _clean_lower
    elif type == 'whitespace':
        return _clean_whitespace
    else:
        assert False, f"Invalid clean function ({type})."


def canonicalize_text(
    text,
    *,
    keep_punctuation_exact_string=None,
    trans_punctuation: dict = str.maketrans("", "", string.punctuation),
):
    """Returns canonicalized `text` (lowercase and punctuation removed).

    From: https://github.com/google-research/big_vision/blob/53f18caf27a9419231bbf08d3388b07671616d3d/big_vision/evaluators/proj/image_text/prompt_engineering.py#L94

    Args:
      text: string to be canonicalized.
      keep_punctuation_exact_string: If provided, then this exact string kept.
        For example providing '{}' will keep any occurrences of '{}' (but will
        still remove '{' and '}' that appear separately).
    """
    text = text.replace("_", " ")
    if keep_punctuation_exact_string:
        text = keep_punctuation_exact_string.join(
            part.translate(trans_punctuation)
            for part in text.split(keep_punctuation_exact_string)
        )
    else:
        text = text.translate(trans_punctuation)
    text = text.lower()
    text = " ".join(text.split())
    return text.strip()


class SimpleTokenizer(object):
    def __init__(
            self,
            bpe_path: str = default_bpe(),
            additional_special_tokens: Optional[List[str]] = None,
            context_length: Optional[int] = DEFAULT_CONTEXT_LENGTH,
            clean: str = 'lower',
            reduction_mask: str = ''
    ):
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        merges = gzip.open(bpe_path).read().decode("utf-8").split('\n')
        merges = merges[1:49152-256-2+1]
        merges = [tuple(merge.split()) for merge in merges]
        vocab = list(bytes_to_unicode().values())
        vocab = vocab + [v+'</w>' for v in vocab]
        for merge in merges:
            vocab.append(''.join(merge))
        special_tokens = ['<start_of_text>', '<end_of_text>']
        if additional_special_tokens:
            special_tokens += additional_special_tokens
        vocab.extend(special_tokens)
        self.encoder = dict(zip(vocab, range(len(vocab))))
        self.decoder = {v: k for k, v in self.encoder.items()}
        self.bpe_ranks = dict(zip(merges, range(len(merges))))
        self.cache = {t:t for t in special_tokens}
        special = "|".join(special_tokens)
        self.pat = re.compile(
            special + r"""|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+""",
            re.IGNORECASE,
        )
        self.vocab_size = len(self.encoder)
        self.all_special_ids = [self.encoder[t] for t in special_tokens]
        self.sot_token_id = self.all_special_ids[0]
        self.eot_token_id = self.all_special_ids[1]
        self.context_length = context_length
        self.clean_fn = get_clean_fn(clean)
        self.reduction_fn = get_reduction_mask_fn(reduction_mask) if reduction_mask else None

    def bpe(self, token):
        if token in self.cache:
            return self.cache[token]
        word = tuple(token[:-1]) + ( token[-1] + '</w>',)
        pairs = get_pairs(word)

        if not pairs:
            return token+'</w>'

        while True:
            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))
            if bigram not in self.bpe_ranks:
                break
            first, second = bigram
            new_word = []
            i = 0
            while i < len(word):
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except Exception:
                    new_word.extend(word[i:])
                    break

                if word[i] == first and i < len(word)-1 and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)
        word = ' '.join(word)
        self.cache[token] = word
        return word

    def encode(self, text):
        bpe_tokens = []
        text = self.clean_fn(text)
        for token in re.findall(self.pat, text):
            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))
            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))
        return bpe_tokens

    def decode(self, tokens):
        text = ''.join([self.decoder[token] for token in tokens])
        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors="replace").replace('</w>', ' ')
        return text

    def __call__(self, texts: Union[str, List[str]], context_length: Optional[int] = None) -> torch.LongTensor:
        """ Returns the tokenized representation of given input string(s)

        Parameters
        ----------
        texts : Union[str, List[str]]
            An input string or a list of input strings to tokenize
        context_length : int
            The context length to use; all CLIP models use 77 as the context length

        Returns
        -------
        A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]
        """
        if isinstance(texts, str):
            texts = [texts]

        context_length = context_length or self.context_length
        assert context_length, 'Please set a valid context length'

        if self.reduction_fn is not None:
            # use reduction strategy for tokenize if set, otherwise default to truncation below
            return self.reduction_fn(
                texts,
                context_length=context_length,
                sot_token_id=self.sot_token_id,
                eot_token_id=self.eot_token_id,
                encode_fn=self.encode,
            )

        all_tokens = [[self.sot_token_id] + self.encode(text) + [self.eot_token_id] for text in texts]
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)

        for i, tokens in enumerate(all_tokens):
            if len(tokens) > context_length:
                tokens = tokens[:context_length]  # Truncate
                tokens[-1] = self.eot_token_id
            result[i, :len(tokens)] = torch.tensor(tokens)

        return result


_tokenizer = SimpleTokenizer()


def decode(output_ids: torch.Tensor):
    output_ids = output_ids.cpu().numpy()
    return _tokenizer.decode(output_ids)


def tokenize(texts: Union[str, List[str]], context_length: int = DEFAULT_CONTEXT_LENGTH) -> torch.LongTensor:
    return _tokenizer(texts, context_length=context_length)


def random_mask_tokenize(
        texts: Union[str, List[str]],
        context_length: int,
        sot_token_id: int,
        eot_token_id: int,
        encode_fn: Callable,
        shuffle: bool = False,
):
    all_tokens = [encode_fn(text) for text in texts]
    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)

    for i, tokens in enumerate(all_tokens):
        tokens = torch.tensor(tokens)
        num_tokens = len(tokens)
        if num_tokens > context_length - 2:  # 2 for sot and eot token
            num_keep = context_length - 2
            indices = torch.randperm(len(tokens))
            indices = indices[:num_keep]
            if not shuffle:
                indices = indices.msort()
            tokens = tokens[indices]
            num_tokens = num_keep
        result[i, 0] = sot_token_id
        result[i, 1:num_tokens + 1] = tokens
        result[i, num_tokens + 1] = eot_token_id

    return result


def simple_mask_tokenize(
        texts: Union[str, List[str]],
        context_length: int,
        sot_token_id: int,
        eot_token_id: int,
        encode_fn: Callable,
):
    all_tokens = [encode_fn(text) for text in texts]
    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)

    for i, tokens in enumerate(all_tokens):
        num_tokens = len(tokens)
        if num_tokens > context_length - 2:  # 2 for sot and eot token
            num_keep = context_length - 2
            start_index = random.randint(0, num_tokens - num_keep)  # high is incl
            tokens = tokens[start_index: start_index + num_keep]
        tokens = [sot_token_id] + tokens + [eot_token_id]
        result[i, :len(tokens)] = torch.tensor(tokens)

    return result


def syntax_mask_tokenize(
        texts: Union[str, List[str]],
        context_length: int,
        sot_token_id: int,
        eot_token_id: int,
        encode_fn: Callable,
) -> torch.LongTensor:
    """ Returns the tokenized representation of given input string(s).
    Apply syntax masking before tokenize.
    """
    import nltk
    global _nltk_init
    if not _nltk_init:
        # run them for the first time
        nltk.download('punkt')
        nltk.download('averaged_perceptron_tagger')
        _nltk_init = True

    def get_order(x):
        if x.startswith('NN'):
            return 1
        elif x.startswith('JJ'):
            return 2
        elif x.startswith('VB'):
            return 3
        else:
            return 4

    # syntax masking
    new_texts = []
    for text in texts:
        list_tokens = nltk.tokenize.word_tokenize(text)
        pos_tags = nltk.pos_tag(list_tokens)
        #  sample the words by get_order method
        order_list = [get_order(tag) for _, tag in pos_tags]
        sorted_ids = np.argsort(np.array(order_list))
        sampled_ids = sorted(sorted_ids[:context_length - 2]) # need 2 slots for sot and eot tokens
        sampled_tokens = np.take(np.array(list_tokens), sampled_ids, axis=0)  # sample the tokens

        new_text = ''
        for token in sampled_tokens:
            new_text = new_text + str(token) + ' '
        new_text = new_text.strip()
        new_texts.append(new_text)
    texts = new_texts

    all_tokens = [[sot_token_id] + encode_fn(text) + [eot_token_id] for text in texts]
    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)

    for i, tokens in enumerate(all_tokens):
        # still need first truncate because some words produces two tokens
        if len(tokens) > context_length:
            tokens = tokens[:context_length]  # Truncate
            tokens[-1] = eot_token_id
        result[i, :len(tokens)] = torch.tensor(tokens)

    return result


def get_reduction_mask_fn(type: str):
    """ Choose strategy for dropping (masking) tokens to achieve target context length"""
    assert type in ('simple', 'random', 'shuffle', 'syntax')
    if type == 'simple':
        return simple_mask_tokenize  # randomly select block [start:end]
    elif type == 'random':
        return random_mask_tokenize  # randomly drop tokens (keep order)
    elif type == 'shuffle':
        return partial(random_mask_tokenize, shuffle=True)  # randomly drop tokens (shuffle order)
    elif type == 'syntax':
        return syntax_mask_tokenize  # randomly drop prioritized by syntax
    else:
        assert False, F'Unknown type {type}.'


class HFTokenizer:
    """HuggingFace tokenizer wrapper with support for custom tokenization modes"""

    def __init__(
            self,
            tokenizer_name: str,
            context_length: Optional[int] = DEFAULT_CONTEXT_LENGTH,
            clean: str = 'whitespace',
            strip_sep_token: bool = False,
            language: Optional[str] = None,
            cache_dir: Optional[str] = None,
            tokenizer_mode: Optional[str] = None,  # None, 'clips'
            **kwargs
    ):
        self.tokenizer_mode = tokenizer_mode or ''
        self.context_length = context_length
        self.clean_fn = get_clean_fn(clean)
        self.strip_sep_token = strip_sep_token

        # NOTE: Left as example of loading custom tokenizer from file for experimentation
        # if self.tokenizer_mode == 'bert_clips':
        #     self.special_tokens = {
        #         "bos_token": 1,
        #         "eos_token": 2,
        #         "cls_token": 101,
        #         "pad_token": 0
        #     }
        #
        #     # For BERT CLIPS mode with vocab file
        #     from tokenizers import BertWordPieceTokenizer
        #     if tokenizer_name.startswith('hf-hub:'):
        #         from huggingface_hub import hf_hub_download
        #         # Format: hf-hub:repo_id/filename
        #         repo_url = tokenizer_name[7:]
        #         parts = repo_url.split('/')
        #         filename = parts[-1]
        #         repo_id = '/'.join(parts[:-1])
        #         vocab_file = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=cache_dir)
        #         self.tokenizer = BertWordPieceTokenizer(lowercase=True)
        #         self.tokenizer = self.tokenizer.from_file(vocab_file)
        #     else:
        #         # Assume tokenizer_name is a local path to a vocab file
        #         self.tokenizer = BertWordPieceTokenizer(lowercase=True)
        #         self.tokenizer = self.tokenizer.from_file(tokenizer_name)

        # Standard HuggingFace tokenizer initialization
        from transformers import AutoTokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_name,
            cache_dir=cache_dir,
            **kwargs
        )

        # Set language function if available
        set_lang_fn = getattr(self.tokenizer, 'set_src_lang_special_tokens', None)
        if callable(set_lang_fn):
            self.set_lang_fn = set_lang_fn
        if language is not None:
            self.set_language(language)

    def save_pretrained(self, dest):
        self.tokenizer.save_pretrained(dest)

    def __call__(self, texts: Union[str, List[str]], context_length: Optional[int] = None) -> torch.Tensor:
        # same cleaning as for default tokenizer, except lowercasing
        # adding lower (for case-sensitive tokenizers) will make it more robust but less sensitive to nuance
        if isinstance(texts, str):
            texts = [texts]

        context_length = context_length or self.context_length
        assert context_length, 'Please set a valid context length in class init or call.'

        texts = [self.clean_fn(text) for text in texts]

        # Handle different tokenization modes
        if self.tokenizer_mode == 'clips':
            return self._clips_tokenize(texts, context_length)
        else:
            # Standard tokenization
            input_ids = self.tokenizer.batch_encode_plus(
                texts,
                return_tensors='pt',
                max_length=context_length,
                padding='max_length',
                truncation=True,
            ).input_ids

            if self.strip_sep_token:
                input_ids = torch.where(
                    input_ids == self.tokenizer.sep_token_id,
                    torch.zeros_like(input_ids),
                    input_ids,
                )

            return input_ids

    def set_language(self, src_lang):
        if hasattr(self, 'set_lang_fn'):
            self.set_lang_fn(src_lang)
        else:
            warnings.warn('Cannot set language for the tokenizer.')

    def _clips_tokenize(self, texts: List[str], context_length: int) -> torch.Tensor:
        """Use standard HF tokenizer but apply custom post-processing"""
        # Use standard tokenizer without special tokens - we'll add our own
        encoded_outputs = self.tokenizer.batch_encode_plus(
            texts,
            add_special_tokens=False,
            padding=False,
            truncation=False,
            return_tensors=None
        )

        encoded = []
        for tokens in encoded_outputs["input_ids"]:
            tokens = tokens[:context_length - 3]  # Leave room for special tokens
            tokens = [self.tokenizer.bos_token_id] + tokens + [self.tokenizer.eos_token_id]
            encoded.append(tokens)

        # Create result tensor and handle padding + class token
        result = torch.zeros(len(encoded), context_length, dtype=torch.long)
        for i, tokens in enumerate(encoded):
            padded_tokens = self._pad_and_add_class_token(
                tokens,
                max_length=context_length,
                pad_token_id=self.tokenizer.pad_token_id,
                cls_token_id=self.tokenizer.cls_token_id,
            )
            result[i, :len(padded_tokens)] = torch.tensor(padded_tokens)

        return result

    def _pad_and_add_class_token(
            self,
            tokens: List[int],
            max_length: int,
            pad_token_id: int = 0,
            cls_token_id: int = 101,
    ) -> List[int]:
        """ Add padding with class token at the end """
        if len(tokens) > max_length - 1:
            tokens = tokens[:max_length - 1]

        # Add padding to reach max_length-1
        if len(tokens) < max_length - 1:
            tokens = tokens + [pad_token_id] * (max_length - 1 - len(tokens))

        # Add class token at the end
        tokens = tokens + [cls_token_id]
        return tokens


class SigLipTokenizer:
    """HuggingFace tokenizer wrapper for SigLIP T5 compatible sentencepiece vocabs

    NOTE: this is not needed in normal library use, but is used to import new sentencepiece tokenizers
    into OpenCLIP. Leaving code here in case future models use new tokenizers.
    """
    VOCAB_FILES = {
        # english, vocab_size=32_000
        "c4-en": "http://storage.googleapis.com/t5-data/vocabs/cc_en.32000/sentencepiece.model",
        # used in multilingual models (mT5, PaLI), vocab_size=250_000
        "mc4": "http://storage.googleapis.com/t5-data/vocabs/mc4.250000.100extra/sentencepiece.model",
        # used in SigLIP2 models, vocab_size=256000
        "gemma": "http://storage.googleapis.com/big_vision/gemma_tokenizer.model",
    }

    def __init__(
            self,
            tokenizer_name: str,
            context_length: Optional[int] = 64,
    ):
        if 'gemma' in tokenizer_name:
            from transformers import GemmaTokenizerFast
            tokenizer_cls = partial(
                GemmaTokenizerFast, padding_side='right', add_bos_token=False, add_eos_token=True)
        else:
            from transformers import T5TokenizerFast
            tokenizer_cls = partial(T5TokenizerFast, extra_ids=0)

        if tokenizer_name in self.VOCAB_FILES:
            # FIXME temporary hack?
            import tempfile
            import fsspec
            vocab_file = self.VOCAB_FILES[tokenizer_name]
            with tempfile.NamedTemporaryFile('wb') as dst:
                with fsspec.open(vocab_file, 'rb') as src:
                    dst.write(src.read())
                self.tokenizer = tokenizer_cls(dst.name, legacy=False)
        else:
            self.tokenizer = tokenizer_cls(tokenizer_name, legacy=False)

        self.tokenizer.pad_token_id = 0 if 'gemma' in tokenizer_name else 1
        self.tokenizer.eos_token_id = 1
        self.context_length = context_length

    def save_pretrained(self, dest):
        self.tokenizer.save_pretrained(dest)

    def __call__(self, texts: Union[str, List[str]], context_length: Optional[int] = None) -> torch.Tensor:
        # same cleaning as for default tokenizer, except lowercasing
        # adding lower (for case-sensitive tokenizers) will make it more robust but less sensitive to nuance
        if isinstance(texts, str):
            texts = [texts]

        context_length = context_length or self.context_length
        assert context_length, 'Please set a valid context length in class init or call.'

        texts = [canonicalize_text(basic_clean(text)) for text in texts]
        output = self.tokenizer(
            texts,
            return_tensors='pt',
            max_length=context_length,
            padding='max_length',
            truncation=True,
        )
        return output.input_ids

===== src/open_clip/hf_configs.py =====
# HF architecture dict:
arch_dict = {
    # https://huggingface.co/docs/transformers/model_doc/roberta#roberta
    "roberta": {
        "config_names": {
            "context_length": "max_position_embeddings",
            "vocab_size": "vocab_size",
            "width": "hidden_size",
            "heads": "num_attention_heads",
            "layers": "num_hidden_layers",
            "layer_attr": "layer",
            "token_embeddings_attr": "embeddings"
        },
        "pooler": "mean_pooler",
    },
    # https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaConfig
    "xlm-roberta": {
        "config_names": {
            "context_length": "max_position_embeddings",
            "vocab_size": "vocab_size",
            "width": "hidden_size",
            "heads": "num_attention_heads",
            "layers": "num_hidden_layers",
            "layer_attr": "layer",
            "token_embeddings_attr": "embeddings"
        },
        "pooler": "mean_pooler",
    },
    # https://huggingface.co/docs/transformers/model_doc/mt5#mt5
    "mt5": {
        "config_names": {
            # unlimited seqlen
            # https://github.com/google-research/text-to-text-transfer-transformer/issues/273
            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/t5/modeling_t5.py#L374
            "context_length": "",
            "vocab_size": "vocab_size",
            "width": "d_model",
            "heads": "num_heads",
            "layers": "num_layers",
            "layer_attr": "block",
            "token_embeddings_attr": "embed_tokens"
        },
        "pooler": "mean_pooler",
    },
    # https://huggingface.co/docs/transformers/model_doc/bert
    "bert": {
        "config_names": {
            "context_length": "max_position_embeddings",
            "vocab_size": "vocab_size",
            "width": "hidden_size",
            "heads": "num_attention_heads",
            "layers": "num_hidden_layers",
        },
        "pooler": "cls_pooler",
    },
    # https://huggingface.co/docs/transformers/model_doc/m2m_100
    "m2m_100": {
        "config_names": {
            "context_length": "max_position_embeddings",
            "vocab_size": "vocab_size",
            "width": "d_model",
            "heads": "encoder_attention_heads",
            "layers": "encoder_layers",
        },
        "pooler": "cls_pooler",
    },
}

===== src/open_clip/push_to_hf_hub.py =====
import argparse
import json
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import Optional, Tuple, Union

import torch

try:
    from huggingface_hub import (
        create_repo,
        get_hf_file_metadata,
        hf_hub_download,
        hf_hub_url,
        repo_type_and_id_from_hf_id,
        upload_folder,
        list_repo_files,
    )
    from huggingface_hub.utils import EntryNotFoundError
    _has_hf_hub = True
except ImportError:
    _has_hf_hub = False

try:
    import safetensors.torch
    _has_safetensors = True
except ImportError:
    _has_safetensors = False

from .constants import HF_WEIGHTS_NAME, HF_SAFE_WEIGHTS_NAME, HF_CONFIG_NAME
from .factory import create_model_from_pretrained, get_model_config, get_tokenizer
from .tokenizer import HFTokenizer, SigLipTokenizer


def save_config_for_hf(
        model,
        config_path: str,
        model_config: Optional[dict],
):
    preprocess_cfg = {
        'mean': model.visual.image_mean,
        'std': model.visual.image_std,
    }
    other_pp = getattr(model.visual, 'preprocess_cfg', {})
    if 'interpolation' in other_pp:
        preprocess_cfg['interpolation'] = other_pp['interpolation']
    if 'resize_mode' in other_pp:
        preprocess_cfg['resize_mode'] = other_pp['resize_mode']
    hf_config = {
        'model_cfg': model_config,
        'preprocess_cfg': preprocess_cfg,
    }

    with config_path.open('w') as f:
        json.dump(hf_config, f, indent=2)


def save_for_hf(
    model,
    tokenizer: HFTokenizer,
    model_config: dict,
    save_directory: str,
    safe_serialization: Union[bool, str] = 'both',
    skip_weights : bool = False,
):
    config_filename = HF_CONFIG_NAME

    save_directory = Path(save_directory)
    save_directory.mkdir(exist_ok=True, parents=True)

    if not skip_weights:
        tensors = model.state_dict()
        if safe_serialization is True or safe_serialization == "both":
            assert _has_safetensors, "`pip install safetensors` to use .safetensors"
            safetensors.torch.save_file(tensors, save_directory / HF_SAFE_WEIGHTS_NAME)
        if safe_serialization is False or safe_serialization == "both":
            torch.save(tensors, save_directory / HF_WEIGHTS_NAME)

    tokenizer.save_pretrained(save_directory)

    config_path = save_directory / config_filename
    save_config_for_hf(model, config_path, model_config=model_config)


def push_to_hf_hub(
    model,
    tokenizer,
    model_config: Optional[dict],
    repo_id: str,
    commit_message: str = 'Add model',
    token: Optional[str] = None,
    revision: Optional[str] = None,
    private: bool = False,
    create_pr: bool = False,
    model_card: Optional[dict] = None,
    safe_serialization: Union[bool, str] = 'both',
):
    if not isinstance(tokenizer, (HFTokenizer, SigLipTokenizer)):
        # FIXME this makes it awkward to push models with new tokenizers, come up with better soln.
        # default CLIP tokenizers use https://huggingface.co/openai/clip-vit-large-patch14
        tokenizer = HFTokenizer('openai/clip-vit-large-patch14')

    # Create repo if it doesn't exist yet
    repo_url = create_repo(repo_id, token=token, private=private, exist_ok=True)

    # Infer complete repo_id from repo_url
    # Can be different from the input `repo_id` if repo_owner was implicit
    _, repo_owner, repo_name = repo_type_and_id_from_hf_id(repo_url)
    repo_id = f"{repo_owner}/{repo_name}"

    # Check if repo already exists and determine what needs updating
    repo_exists = False
    repo_files = {}
    try:
        repo_files = set(list_repo_files(repo_id))
        repo_exists = True
        print('Repo exists', repo_files)
    except Exception as e:
        print('Repo does not exist', e)

    try:
        get_hf_file_metadata(hf_hub_url(repo_id=repo_id, filename="README.md", revision=revision))
        has_readme = True
    except EntryNotFoundError:
        has_readme = False

    # Dump model and push to Hub
    with TemporaryDirectory() as tmpdir:
        # Save model weights and config.
        save_for_hf(
            model,
            tokenizer=tokenizer,
            model_config=model_config,
            save_directory=tmpdir,
            safe_serialization=safe_serialization,
        )

        # Add readme if it does not exist
        if not has_readme:
            model_card = model_card or {}
            model_name = repo_id.split('/')[-1]
            readme_path = Path(tmpdir) / "README.md"
            readme_text = generate_readme(model_card, model_name)
            readme_path.write_text(readme_text)

        # Upload model and return
        return upload_folder(
            repo_id=repo_id,
            folder_path=tmpdir,
            revision=revision,
            create_pr=create_pr,
            commit_message=commit_message,
        )


def push_pretrained_to_hf_hub(
    model_name,
    pretrained: str,
    repo_id: str,
    precision: str = 'fp32',
    image_mean: Optional[Tuple[float, ...]] = None,
    image_std: Optional[Tuple[float, ...]] = None,
    image_interpolation: Optional[str] = None,
    image_resize_mode: Optional[str] = None,  # only effective for inference
    commit_message: str = 'Add model',
    token: Optional[str] = None,
    revision: Optional[str] = None,
    private: bool = False,
    create_pr: bool = False,
    model_card: Optional[dict] = None,
    hf_tokenizer_self: bool = False,
    **kwargs,
):
    model, preprocess_eval = create_model_from_pretrained(
        model_name,
        pretrained=pretrained,
        precision=precision,
        image_mean=image_mean,
        image_std=image_std,
        image_interpolation=image_interpolation,
        image_resize_mode=image_resize_mode,
        **kwargs,
    )
    model_config = get_model_config(model_name)
    if pretrained == 'openai':
        model_config['quick_gelu'] = True
    assert model_config

    tokenizer = get_tokenizer(model_name)
    if hf_tokenizer_self:
        # make hf tokenizer config in the uploaded model point to self instead of original location
        model_config['text_cfg']['hf_tokenizer_name'] = repo_id

    push_to_hf_hub(
        model=model,
        tokenizer=tokenizer,
        model_config=model_config,
        repo_id=repo_id,
        commit_message=commit_message,
        token=token,
        revision=revision,
        private=private,
        create_pr=create_pr,
        model_card=model_card,
        safe_serialization='both',
    )


def generate_readme(model_card: dict, model_name: str):
    tags = model_card.pop('tags', ('clip',))
    pipeline_tag = model_card.pop('pipeline_tag', 'zero-shot-image-classification')
    readme_text = "---\n"
    if tags:
        readme_text += "tags:\n"
        for t in tags:
            readme_text += f"- {t}\n"
    readme_text += "library_name: open_clip\n"
    readme_text += f"pipeline_tag: {pipeline_tag}\n"
    readme_text += f"license: {model_card.get('license', 'mit')}\n"
    if 'details' in model_card and 'Dataset' in model_card['details']:
        readme_text += 'datasets:\n'
        readme_text += f"- {model_card['details']['Dataset'].lower()}\n"
    readme_text += "---\n"
    readme_text += f"# Model card for {model_name}\n"
    if 'description' in model_card:
        readme_text += f"\n{model_card['description']}\n"
    if 'details' in model_card:
        readme_text += f"\n## Model Details\n"
        for k, v in model_card['details'].items():
            if isinstance(v, (list, tuple)):
                readme_text += f"- **{k}:**\n"
                for vi in v:
                    readme_text += f"  - {vi}\n"
            elif isinstance(v, dict):
                readme_text += f"- **{k}:**\n"
                for ki, vi in v.items():
                    readme_text += f"  - {ki}: {vi}\n"
            else:
                readme_text += f"- **{k}:** {v}\n"
    if 'usage' in model_card:
        readme_text += f"\n## Model Usage\n"
        readme_text += model_card['usage']
        readme_text += '\n'

    if 'comparison' in model_card:
        readme_text += f"\n## Model Comparison\n"
        readme_text += model_card['comparison']
        readme_text += '\n'

    if 'citation' in model_card:
        readme_text += f"\n## Citation\n"
        if not isinstance(model_card['citation'], (list, tuple)):
            citations = [model_card['citation']]
        else:
            citations = model_card['citation']
        for c in citations:
            readme_text += f"```bibtex\n{c}\n```\n"

    return readme_text


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Push to Hugging Face Hub")
    parser.add_argument(
        "--model", type=str, help="Name of the model to use.",
    )
    parser.add_argument(
        "--pretrained", type=str,
        help="Use a pretrained CLIP model weights with the specified tag or file path.",
    )
    parser.add_argument(
        "--repo-id", type=str,
        help="Destination HF Hub repo-id ie 'organization/model_id'.",
    )
    parser.add_argument(
        "--precision", type=str, default='fp32',
    )
    parser.add_argument(
        '--image-mean', type=float, nargs='+', default=None, metavar='MEAN',
        help='Override default image mean value of dataset')
    parser.add_argument(
        '--image-std', type=float, nargs='+', default=None, metavar='STD',
        help='Override default image std deviation of of dataset')
    parser.add_argument(
        '--image-interpolation',
        default=None, type=str, choices=['bicubic', 'bilinear', 'random'],
        help="image resize interpolation"
    )
    parser.add_argument(
        '--image-resize-mode',
        default=None, type=str, choices=['shortest', 'longest', 'squash'],
        help="image resize mode during inference"
    )
    parser.add_argument(
        "--hf-tokenizer-self",
        default=False,
        action="store_true",
        help="make hf_tokenizer_name point in uploaded config point to itself"
    )
    args = parser.parse_args()

    print(f'Saving model {args.model} with pretrained weights {args.pretrained} to Hugging Face Hub at {args.repo_id}')

    # FIXME add support to pass model_card json / template from file via cmd line

    push_pretrained_to_hf_hub(
        args.model,
        args.pretrained,
        args.repo_id,
        precision=args.precision,
        image_mean=args.image_mean,  # override image mean/std if trained w/ non defaults
        image_std=args.image_std,
        image_interpolation=args.image_interpolation,
        image_resize_mode=args.image_resize_mode,
        hf_tokenizer_self=args.hf_tokenizer_self,
    )

    print(f'{args.model} saved.')

===== src/open_clip/model.py =====
""" CLIP Model

Adapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.
"""
import copy
import logging
import math
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.checkpoint import checkpoint
from functools import partial

from .hf_model import HFTextEncoder
from .modified_resnet import ModifiedResNet
from .timm_model import TimmModel
from .transformer import (
    LayerNormFp32,
    LayerNorm,
    QuickGELU,
    Attention,
    VisionTransformer,
    TextTransformer,
    text_global_pool,
    lock_text_tower,
)
from .utils import to_2tuple


@dataclass
class CLIPVisionCfg:
    layers: Union[Tuple[int, int, int, int], int] = 12
    width: int = 768
    head_width: int = 64
    mlp_ratio: float = 4.0
    patch_size: int = 16
    image_size: Union[Tuple[int, int], int] = 224

    ls_init_value: Optional[float] = None  # layer scale initial value
    patch_dropout: float = 0.  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results
    attentional_pool: bool = False  # whether to use attentional pooler in the last embedding layer (overrides pool_type)
    attn_pooler_queries: int = 256  # n_queries for attentional pooler
    attn_pooler_heads: int = 8  # n heads for attentional_pooling
    no_ln_pre: bool = False  # disable pre transformer LayerNorm
    pos_embed_type: str = 'learnable'
    final_ln_after_pool: bool = False  # apply final LayerNorm after pooling
    pool_type: str = 'tok'
    output_tokens: bool = False
    act_kwargs: Optional[dict] = None
    norm_kwargs: Optional[dict] = None

    # Custom attention block settings
    block_type: Optional[str] = None  # attention block type ('default', 'custom'), auto-selects 'custom' if any below features enabled
    qk_norm: bool = False  # apply layer norm to q and k in attention
    scaled_cosine_attn: bool = False  # use scaled cosine attention
    scale_heads: bool = False  # learnable head-specific scale applied to attention logits
    scale_attn_inner: bool = False  # apply layer norm on attention context, before output projection
    scale_attn: bool = False  # apply layer norm after full attention block
    scale_fc: bool = False  # apply layer norm in MLP block

    timm_model_name: Optional[str] = None  # a valid model name overrides layers, width, patch_size
    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model
    timm_pool: str = 'avg'  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')
    timm_proj: str = 'linear'  # linear projection for timm model output ('linear', 'mlp', '')
    timm_proj_bias: bool = False  # enable bias final projection
    timm_drop: float = 0.  # head dropout
    timm_drop_path: Optional[float] = None  # backbone stochastic depth


@dataclass
class CLIPTextCfg:
    context_length: int = 77
    vocab_size: int = 49408
    hf_tokenizer_name: Optional[str] = None
    tokenizer_mode: Optional[str] = None
    tokenizer_kwargs: Optional[dict] = None

    width: int = 512
    heads: int = 8
    layers: int = 12
    mlp_ratio: float = 4.0
    ls_init_value: Optional[float] = None  # layer scale initial value
    embed_cls: bool = False
    pad_id: int = 0
    eos_id: int = 2  # only used for when pool_type == 'eos', must match tokenizer eos
    no_causal_mask: bool = False  # disable causal masking
    final_ln_after_pool: bool = False  # apply final LayerNorm after pooling
    pool_type: str = 'argmax'
    proj_bias: bool = False
    proj_type: str = 'linear'  # control final text projection, 'none' forces no projection
    output_tokens: bool = False
    act_kwargs: dict = None
    norm_kwargs: dict = None

    # Custom attention block settings
    block_type: Optional[str] = None  # attention block type ('default', 'custom'), auto-selects 'custom' if any custom features enabled
    qk_norm: bool = False  # apply layer norm to q and k in attention
    scaled_cosine_attn: bool = False  # use scaled cosine attention
    scale_heads: bool = False  # learnable head-specific scale applied to attention logits
    scale_attn_inner: bool = False  # apply layer norm on attention context, before output projection
    scale_attn: bool = False  # apply layer norm after full attention block
    scale_fc: bool = False  # apply layer norm in MLP block

    # HuggingFace specific text tower config
    hf_model_name: Optional[str] = None
    hf_model_pretrained: bool = True
    hf_proj_type: str = 'mlp'
    hf_pooler_type: str = 'mean_pooler'  # attentional pooling for HF models


def get_cast_dtype(precision: str):
    cast_dtype = None
    if precision == 'bf16':
        cast_dtype = torch.bfloat16
    elif precision == 'fp16':
        cast_dtype = torch.float16
    return cast_dtype


def get_input_dtype(precision: str):
    input_dtype = None
    if precision in ('bf16', 'pure_bf16'):
        input_dtype = torch.bfloat16
    elif precision in ('fp16', 'pure_fp16'):
        input_dtype = torch.float16
    return input_dtype


def _build_vision_tower(
        embed_dim: int,
        vision_cfg: CLIPVisionCfg,
        quick_gelu: bool = False,
        cast_dtype: Optional[torch.dtype] = None
):
    if isinstance(vision_cfg, dict):
        vision_cfg = CLIPVisionCfg(**vision_cfg)

    # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more
    # memory efficient in recent PyTorch releases (>= 1.10).
    # NOTE: timm models always use native GELU regardless of quick_gelu flag.
    act_layer = QuickGELU if quick_gelu else nn.GELU

    if vision_cfg.timm_model_name:
        visual = TimmModel(
            vision_cfg.timm_model_name,
            pretrained=vision_cfg.timm_model_pretrained,
            pool=vision_cfg.timm_pool,
            proj=vision_cfg.timm_proj,
            proj_bias=vision_cfg.timm_proj_bias,
            drop=vision_cfg.timm_drop,
            drop_path=vision_cfg.timm_drop_path,
            patch_drop=vision_cfg.patch_dropout if vision_cfg.patch_dropout > 0 else None,
            embed_dim=embed_dim,
            image_size=vision_cfg.image_size,
        )
    elif isinstance(vision_cfg.layers, (tuple, list)):
        vision_heads = vision_cfg.width * 32 // vision_cfg.head_width
        visual = ModifiedResNet(
            layers=vision_cfg.layers,
            output_dim=embed_dim,
            heads=vision_heads,
            image_size=vision_cfg.image_size,
            width=vision_cfg.width,
        )
    else:
        vision_heads = vision_cfg.width // vision_cfg.head_width
        norm_layer = LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm
        if vision_cfg.norm_kwargs:
            norm_layer = partial(norm_layer, **vision_cfg.norm_kwargs)
        if vision_cfg.act_kwargs is not None:
            act_layer = partial(act_layer, **vision_cfg.act_kwargs)

        visual = VisionTransformer(
            image_size=vision_cfg.image_size,
            patch_size=vision_cfg.patch_size,
            width=vision_cfg.width,
            layers=vision_cfg.layers,
            heads=vision_heads,
            mlp_ratio=vision_cfg.mlp_ratio,
            ls_init_value=vision_cfg.ls_init_value,
            patch_dropout=vision_cfg.patch_dropout,
            attentional_pool=vision_cfg.attentional_pool,
            attn_pooler_queries=vision_cfg.attn_pooler_queries,
            attn_pooler_heads=vision_cfg.attn_pooler_heads,
            pos_embed_type=vision_cfg.pos_embed_type,
            no_ln_pre=vision_cfg.no_ln_pre,
            final_ln_after_pool=vision_cfg.final_ln_after_pool,
            pool_type=vision_cfg.pool_type,
            output_tokens=vision_cfg.output_tokens,
            output_dim=embed_dim,
            act_layer=act_layer,
            norm_layer=norm_layer,
            block_type=vision_cfg.block_type,
            qk_norm=vision_cfg.qk_norm,
            scaled_cosine_attn=vision_cfg.scaled_cosine_attn,
            scale_heads=vision_cfg.scale_heads,
            scale_attn_inner=vision_cfg.scale_attn_inner,
            scale_attn=vision_cfg.scale_attn,
            scale_fc=vision_cfg.scale_fc,
        )

    return visual


def _build_text_tower(
        embed_dim: int,
        text_cfg: CLIPTextCfg,
        quick_gelu: bool = False,
        cast_dtype: Optional[torch.dtype] = None,
):
    if isinstance(text_cfg, dict):
        text_cfg = CLIPTextCfg(**text_cfg)

    if text_cfg.hf_model_name:
        text = HFTextEncoder(
            text_cfg.hf_model_name,
            output_dim=embed_dim,
            proj_type=text_cfg.hf_proj_type,
            pooler_type=text_cfg.hf_pooler_type,
            pretrained=text_cfg.hf_model_pretrained,
            output_tokens=text_cfg.output_tokens,
        )
    else:
        act_layer = QuickGELU if quick_gelu else nn.GELU
        norm_layer = LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm
        if text_cfg.norm_kwargs:
            norm_layer = partial(norm_layer, **text_cfg.norm_kwargs)
        if text_cfg.act_kwargs is not None:
            act_layer = partial(act_layer, **text_cfg.act_kwargs)

        text = TextTransformer(
            context_length=text_cfg.context_length,
            vocab_size=text_cfg.vocab_size,
            width=text_cfg.width,
            heads=text_cfg.heads,
            layers=text_cfg.layers,
            mlp_ratio=text_cfg.mlp_ratio,
            ls_init_value=text_cfg.ls_init_value,
            output_dim=embed_dim,
            embed_cls=text_cfg.embed_cls,
            no_causal_mask=text_cfg.no_causal_mask,
            pad_id=text_cfg.pad_id,
            eos_id=text_cfg.eos_id,
            pool_type=text_cfg.pool_type,
            proj_type=text_cfg.proj_type,
            proj_bias=text_cfg.proj_bias,
            output_tokens=text_cfg.output_tokens,
            act_layer=act_layer,
            norm_layer=norm_layer,
            block_type=text_cfg.block_type,
            qk_norm=text_cfg.qk_norm,
            scaled_cosine_attn=text_cfg.scaled_cosine_attn,
            scale_heads=text_cfg.scale_heads,
            scale_attn_inner=text_cfg.scale_attn_inner,
            scale_attn=text_cfg.scale_attn,
            scale_fc=text_cfg.scale_fc,
        )
    return text


class CLIP(nn.Module):
    output_dict: torch.jit.Final[bool]

    def __init__(
            self,
            embed_dim: int,
            vision_cfg: CLIPVisionCfg,
            text_cfg: CLIPTextCfg,
            quick_gelu: bool = False,
            init_logit_scale: float = np.log(1 / 0.07),
            init_logit_bias: Optional[float] = None,
            nonscalar_logit_scale: bool = False,
            cast_dtype: Optional[torch.dtype] = None,
            output_dict: bool = False,
    ):
        super().__init__()
        self.output_dict = output_dict

        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)

        text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)
        self.transformer = text.transformer
        self.context_length = text.context_length
        self.vocab_size = text.vocab_size
        self.token_embedding = text.token_embedding
        self.positional_embedding = text.positional_embedding
        self.ln_final = text.ln_final
        self.text_projection = text.text_projection
        self.text_pool_type = text.pool_type
        self.text_eos_id = text.eos_id
        self.register_buffer('attn_mask', text.attn_mask, persistent=False)

        lshape = [1] if nonscalar_logit_scale else []
        self.logit_scale = nn.Parameter(torch.ones(lshape) * init_logit_scale)
        if init_logit_bias is not None:
            self.logit_bias = nn.Parameter(torch.ones(lshape) * init_logit_bias)
        else:
            self.logit_bias = None

    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):
        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991
        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)

    def lock_text_tower(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):
        assert freeze_layer_norm, 'Unfreezing LayerNorm is not supported. LayerNorm treated like other weights.'
        lock_text_tower(self, unlocked_layers)

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable=True):
        self.visual.set_grad_checkpointing(enable)
        self.transformer.grad_checkpointing = enable

    @torch.jit.ignore
    def no_weight_decay(self):
        # for timm optimizers, 1d params like logit_scale, logit_bias, ln/bn scale, biases are excluded by default
        no_wd = {'positional_embedding'}
        if hasattr(self.visual, 'no_weight_decay'):
            for n in self.visual.no_weight_decay():
                no_wd.add('visual.' + n)
        return no_wd

    def encode_image(self, image, normalize: bool = False):
        features = self.visual(image)
        return F.normalize(features, dim=-1) if normalize else features

    def encode_text(self, text, normalize: bool = False):
        cast_dtype = self.transformer.get_cast_dtype()

        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]

        x = x + self.positional_embedding.to(cast_dtype)
        x = self.transformer(x, attn_mask=self.attn_mask)
        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]
        x = text_global_pool(x, text, self.text_pool_type, eos_token_id=getattr(self, "text_eos_id", None))
        if self.text_projection is not None:
            if isinstance(self.text_projection, nn.Linear):
                x = self.text_projection(x)
            else:
                x = x @ self.text_projection

        return F.normalize(x, dim=-1) if normalize else x

    def get_logits(self, image, text):
        image_features = self.encode_image(image, normalize=True)
        text_features = self.encode_text(text, normalize=True)
        image_logits = self.logit_scale.exp() * image_features @ text_features.T
        if self.logit_bias is not None:
            image_logits += self.logit_bias
        text_logits = image_logits.T
        return image_logits, text_logits

    def forward_intermediates(
            self,
            image: Optional[torch.Tensor] = None,
            text: Optional[torch.Tensor] = None,
            image_indices: Optional[Union[int, List[int]]] = None,
            text_indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
            normalize: bool = True,
            normalize_intermediates: bool = False,
            intermediates_only: bool = False,
            image_output_fmt: str = 'NCHW',
            image_output_extra_tokens: bool = False,
            text_output_fmt: str = 'NLC',
            text_output_extra_tokens: bool = False,
            output_logits: bool = False,
            output_logit_scale_bias: bool = False,
    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """ Forward features that returns intermediates.

        Args:
            image: Input image tensor
            text: Input text tensor
            image_indices: For image tower, Take last n blocks if int, all if None, select matching indices if sequence
            text_indices: Take last n blocks if int, all if None, select matching indices if sequence
            stop_early: Stop iterating over blocks when last desired intermediate hit
            normalize_intermediates: Apply final norm layer to all intermediates
            normalize: L2 Normalize final features
            intermediates_only: Only return intermediate features, do not return final features
            image_output_fmt: Shape of intermediate image feature outputs
            image_output_extra_tokens: Return both prefix and spatial intermediate tokens
            text_output_fmt: Shape of intermediate text feature outputs (ignored for this model)
            text_output_extra_tokens: Return both prefix and spatial intermediate tokens (ignored for this model)
            output_logits: Include logits in output
            output_logit_scale_bias: Include the logit scale bias in the output
        Returns:

        """
        output = {}
        if intermediates_only:
            # intermediates only disables final feature normalization, and include logits
            normalize = False
            output_logits = False
        if output_logits:
            assert image is not None and text is not None, 'Both image and text inputs are required to compute logits'

        if image is not None:
            image_output = self.visual.forward_intermediates(
                image,
                indices=image_indices,
                stop_early=stop_early,
                normalize_intermediates=normalize_intermediates,
                intermediates_only=intermediates_only,
                output_fmt=image_output_fmt,
                output_extra_tokens=image_output_extra_tokens,
            )
            if normalize and "image_features" in image_output:
                image_output["image_features"] = F.normalize(image_output["image_features"], dim=-1)
            output.update(image_output)

        if text is not None:
            cast_dtype = self.transformer.get_cast_dtype()
            x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]
            x = x + self.positional_embedding.to(cast_dtype)
            x, intermediates = self.transformer.forward_intermediates(
                x,
                attn_mask=self.attn_mask,
                indices=text_indices
            )
            if normalize_intermediates:
                intermediates = [self.ln_final(xi) for xi in intermediates]

            # NOTE this model doesn't support cls embed in text transformer, no need for extra intermediate tokens
            output["text_intermediates"] = intermediates

            if not intermediates_only:
                x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]
                x = text_global_pool(x, text, self.text_pool_type, eos_token_id=getattr(self, "text_eos_id", None))
                if self.text_projection is not None:
                    if isinstance(self.text_projection, nn.Linear):
                        x = self.text_projection(x)
                    else:
                        x = x @ self.text_projection
                if normalize:
                    x = F.normalize(x, dim=-1)
                output["text_features"] = x

        logit_scale_exp = self.logit_scale.exp() if output_logits or output_logit_scale_bias else None

        if output_logits:
            image_logits = logit_scale_exp * output["image_features"] @ output["text_features"].T
            if self.logit_bias is not None:
                image_logits += self.logit_bias
            text_logits = image_logits.T
            output["image_logits"] = image_logits
            output["text_logits"] = text_logits

        if output_logit_scale_bias:
            output["logit_scale"] = logit_scale_exp
            if self.logit_bias is not None:
                output['logit_bias'] = self.logit_bias

        return output

    def forward(
            self,
            image: Optional[torch.Tensor] = None,
            text: Optional[torch.Tensor] = None,
    ):
        image_features = self.encode_image(image, normalize=True) if image is not None else None
        text_features = self.encode_text(text, normalize=True) if text is not None else None

        if self.output_dict:
            out_dict = {
                "image_features": image_features,
                "text_features": text_features,
                "logit_scale": self.logit_scale.exp()
            }
            if self.logit_bias is not None:
                out_dict['logit_bias'] = self.logit_bias
            return out_dict

        if self.logit_bias is not None:
            return image_features, text_features, self.logit_scale.exp(), self.logit_bias
        return image_features, text_features, self.logit_scale.exp()


class CustomTextCLIP(nn.Module):
    output_dict: torch.jit.Final[bool]

    def __init__(
            self,
            embed_dim: int,
            vision_cfg: CLIPVisionCfg,
            text_cfg: CLIPTextCfg,
            quick_gelu: bool = False,
            init_logit_scale: float = np.log(1 / 0.07),
            init_logit_bias: Optional[float] = None,
            nonscalar_logit_scale: bool = False,
            cast_dtype: Optional[torch.dtype] = None,
            output_dict: bool = False,
    ):
        super().__init__()
        self.output_dict = output_dict
        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)
        self.text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)
        self.context_length = self.text.context_length
        self.vocab_size = self.text.vocab_size

        lshape = [1] if nonscalar_logit_scale else []
        self.logit_scale = nn.Parameter(torch.ones(lshape) * init_logit_scale)
        if init_logit_bias is not None:
            self.logit_bias = nn.Parameter(torch.ones(lshape) * init_logit_bias)
        else:
            self.logit_bias = None

    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):
        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991
        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)

    def lock_text_tower(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):
        self.text.lock(unlocked_layers, freeze_layer_norm)

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable=True):
        self.visual.set_grad_checkpointing(enable)
        self.text.set_grad_checkpointing(enable)

    @torch.jit.ignore
    def no_weight_decay(self):
        # for timm optimizers, 1d params like logit_scale, logit_bias, ln/bn scale, biases are excluded by default
        no_wd = set()
        if hasattr(self.visual, 'no_weight_decay'):
            for n in self.visual.no_weight_decay():
                no_wd.add('visual.' + n)
        if hasattr(self.text, 'no_weight_decay'):
            for n in self.text.no_weight_decay():
                no_wd.add('text.' + n)
        return no_wd

    def encode_image(self, image, normalize: bool = False):
        features = self.visual(image)
        return F.normalize(features, dim=-1) if normalize else features

    def encode_text(self, text, normalize: bool = False):
        features = self.text(text)
        return F.normalize(features, dim=-1) if normalize else features

    def get_logits(self, image, text):
        image_features = self.encode_image(image, normalize=True)
        text_features = self.encode_text(text, normalize=True)
        image_logits = self.logit_scale.exp() * image_features @ text_features.T
        if self.logit_bias is not None:
            image_logits += self.logit_bias
        text_logits = image_logits.T
        return image_logits, text_logits

    def forward_intermediates(
            self,
            image: Optional[torch.Tensor] = None,
            text: Optional[torch.Tensor] = None,
            image_indices: Optional[Union[int, List[int]]] = None,
            text_indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
            normalize: bool = True,
            normalize_intermediates: bool = False,
            intermediates_only: bool = False,
            image_output_fmt: str = 'NCHW',
            image_output_extra_tokens: bool = False,
            text_output_fmt: str = 'NLC',
            text_output_extra_tokens: bool = False,
            output_logits: bool = False,
            output_logit_scale_bias: bool = False,
    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """ Forward features that returns intermediates.

        Args:
            image: Input image tensor
            text: Input text tensor
            image_indices: For image tower, Take last n blocks if int, all if None, select matching indices if sequence
            text_indices: Take last n blocks if int, all if None, select matching indices if sequence
            stop_early: Stop iterating over blocks when last desired intermediate hit
            normalize: L2 Normalize final image and text features (if present)
            normalize_intermediates: Apply final encoder norm layer to all intermediates (if possible)
            intermediates_only: Only return intermediate features, do not return final features
            image_output_fmt: Shape of intermediate image feature outputs
            image_output_extra_tokens: Return both prefix and spatial intermediate tokens
            text_output_fmt: Shape of intermediate text feature outputs
            text_output_extra_tokens: Return both prefix and spatial intermediate tokens
            output_logits: Include logits in output
            output_logit_scale_bias: Include the logit scale bias in the output
        Returns:

        """
        output = {}
        if intermediates_only:
            # intermediates only disables final feature normalization, and include logits
            normalize = False
            output_logits = False
        if output_logits:
            assert image is not None and text is not None, 'Both image and text inputs are required to compute logits'

        if image is not None:
            image_output = self.visual.forward_intermediates(
                image,
                indices=image_indices,
                stop_early=stop_early,
                normalize_intermediates=normalize_intermediates,
                intermediates_only=intermediates_only,
                output_fmt=image_output_fmt,
                output_extra_tokens=image_output_extra_tokens,
            )
            if normalize and "image_features" in image_output:
                image_output["image_features"] = F.normalize(image_output["image_features"], dim=-1)
            output.update(image_output)

        if text is not None:
            text_output = self.text.forward_intermediates(
                text,
                indices=text_indices,
                stop_early=stop_early,
                normalize_intermediates=normalize_intermediates,
                intermediates_only=intermediates_only,
                output_fmt=text_output_fmt,
                output_extra_tokens=text_output_extra_tokens,
            )
            if normalize and "text_features" in text_output:
                text_output["text_features"] = F.normalize(text_output["text_features"], dim=-1)
            output.update(text_output)

        logit_scale_exp = self.logit_scale.exp() if output_logits or output_logit_scale_bias else None

        if output_logits:
            image_logits = logit_scale_exp * output["image_features"] @ output["text_features"].T
            if self.logit_bias is not None:
                image_logits += self.logit_bias
            text_logits = image_logits.T
            output["image_logits"] = image_logits
            output["text_logits"] = text_logits

        if output_logit_scale_bias:
            output["logit_scale"] = logit_scale_exp
            if self.logit_bias is not None:
                output['logit_bias'] = self.logit_bias

        return output

    def forward(
            self,
            image: Optional[torch.Tensor] = None,
            text: Optional[torch.Tensor] = None,
    ):
        image_features = self.encode_image(image, normalize=True) if image is not None else None
        text_features = self.encode_text(text, normalize=True) if text is not None else None

        if self.output_dict:
            out_dict = {
                "image_features": image_features,
                "text_features": text_features,
                "logit_scale": self.logit_scale.exp()
            }
            if self.logit_bias is not None:
                out_dict['logit_bias'] = self.logit_bias
            return out_dict

        if self.logit_bias is not None:
            return image_features, text_features, self.logit_scale.exp(), self.logit_bias
        return image_features, text_features, self.logit_scale.exp()


def convert_weights_to_lp(model: nn.Module, dtype=torch.float16):
    """Convert applicable model parameters to low-precision (bf16 or fp16)"""

    def _convert_weights(l):
        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            l.weight.data = l.weight.data.to(dtype)
            if l.bias is not None:
                l.bias.data = l.bias.data.to(dtype)

        if isinstance(l, (nn.MultiheadAttention, Attention)):
            for attr in [*[f"{s}_proj_weight" for s in ["in", "q", "k", "v"]], "in_proj_bias", "bias_k", "bias_v"]:
                tensor = getattr(l, attr, None)
                if tensor is not None:
                    tensor.data = tensor.data.to(dtype)

        if isinstance(l, (CLIP, TextTransformer)):
            # convert text nn.Parameter projections
            attr = getattr(l, "text_projection", None)
            if attr is not None:
                attr.data = attr.data.to(dtype)

        if isinstance(l, VisionTransformer):
            # convert vision nn.Parameter projections
            attr = getattr(l, "proj", None)
            if attr is not None:
                attr.data = attr.data.to(dtype)

    model.apply(_convert_weights)


convert_weights_to_fp16 = convert_weights_to_lp  # backwards compat


# used to maintain checkpoint compatibility
def convert_to_custom_text_state_dict(state_dict: dict):
    if 'text_projection' in state_dict:
        # old format state_dict, move text tower -> .text
        new_state_dict = {}
        for k, v in state_dict.items():
            if any(k.startswith(p) for p in (
                'text_projection',
                'positional_embedding',
                'token_embedding',
                'transformer',
                'ln_final',
            )):
                k = 'text.' + k
            new_state_dict[k] = v
        return new_state_dict
    return state_dict


def build_model_from_openai_state_dict(
        state_dict: dict,
        quick_gelu=True,
        cast_dtype=torch.float16,
):
    vit = "visual.proj" in state_dict

    if vit:
        vision_width = state_dict["visual.conv1.weight"].shape[0]
        vision_layers = len(
            [k for k in state_dict.keys() if k.startswith("visual.") and k.endswith(".attn.in_proj_weight")])
        vision_patch_size = state_dict["visual.conv1.weight"].shape[-1]
        grid_size = round((state_dict["visual.positional_embedding"].shape[0] - 1) ** 0.5)
        image_size = vision_patch_size * grid_size
    else:
        counts: list = [
            len(set(k.split(".")[2] for k in state_dict if k.startswith(f"visual.layer{b}"))) for b in [1, 2, 3, 4]]
        vision_layers = tuple(counts)
        vision_width = state_dict["visual.layer1.0.conv1.weight"].shape[0]
        output_width = round((state_dict["visual.attnpool.positional_embedding"].shape[0] - 1) ** 0.5)
        vision_patch_size = None
        assert output_width ** 2 + 1 == state_dict["visual.attnpool.positional_embedding"].shape[0]
        image_size = output_width * 32

    embed_dim = state_dict["text_projection"].shape[1]
    context_length = state_dict["positional_embedding"].shape[0]
    vocab_size = state_dict["token_embedding.weight"].shape[0]
    transformer_width = state_dict["ln_final.weight"].shape[0]
    transformer_heads = transformer_width // 64
    transformer_layers = len(set(k.split(".")[2] for k in state_dict if k.startswith(f"transformer.resblocks")))

    vision_cfg = CLIPVisionCfg(
        layers=vision_layers,
        width=vision_width,
        patch_size=vision_patch_size,
        image_size=image_size,
    )
    text_cfg = CLIPTextCfg(
        context_length=context_length,
        vocab_size=vocab_size,
        width=transformer_width,
        heads=transformer_heads,
        layers=transformer_layers,
    )
    model = CLIP(
        embed_dim,
        vision_cfg=vision_cfg,
        text_cfg=text_cfg,
        quick_gelu=quick_gelu,  # OpenAI models were trained with QuickGELU
        cast_dtype=cast_dtype,
    )

    for key in ["input_resolution", "context_length", "vocab_size"]:
        state_dict.pop(key, None)
    convert_weights_to_fp16(model)  # OpenAI state dicts are partially converted to float16
    model.load_state_dict(state_dict)
    return model.eval()


def trace_model(model, batch_size=256, device=torch.device('cpu')):
    model.eval()
    image_size = model.visual.image_size
    example_images = torch.ones((batch_size, 3, image_size, image_size), device=device)
    example_text = torch.zeros((batch_size, model.context_length), dtype=torch.int, device=device)
    model = torch.jit.trace_module(
        model,
        inputs=dict(
            forward=(example_images, example_text),
            encode_text=(example_text,),
            encode_image=(example_images,)
        ))
    model.visual.image_size = image_size
    return model


def resize_pos_embed(state_dict, model, interpolation: str = 'bicubic', antialias: bool = True):
    # Rescale the grid of position embeddings when loading from state_dict
    old_pos_embed = state_dict.get('visual.positional_embedding', None)
    if old_pos_embed is None or not hasattr(model.visual, 'grid_size'):
        return
    grid_size = to_2tuple(model.visual.grid_size)
    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)
    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens
    if new_seq_len == old_pos_embed.shape[0]:
        return

    if extra_tokens:
        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]
    else:
        pos_emb_tok, pos_emb_img = None, old_pos_embed
    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))

    logging.info('Resizing position embedding grid-size from %s to %s', old_grid_size, grid_size)
    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)
    pos_emb_img = F.interpolate(
        pos_emb_img,
        size=grid_size,
        mode=interpolation,
        antialias=antialias,
        align_corners=False,
    )
    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]
    if pos_emb_tok is not None:
        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)
    else:
        new_pos_embed = pos_emb_img
    state_dict['visual.positional_embedding'] = new_pos_embed


def resize_text_pos_embed(state_dict, model, interpolation: str = 'linear', antialias: bool = False):
    pos_embed_key = 'positional_embedding' if 'positional_embedding' in state_dict else 'text.positional_embedding'
    old_pos_embed = state_dict.get(pos_embed_key, None)
    if old_pos_embed is None:
        return
    # FIXME add support for text cls_token
    model_pos_embed = getattr(model, 'positional_embedding', None)
    if model_pos_embed is None:
        model_pos_embed = getattr(model.text, 'positional_embedding', None)

    old_num_pos = old_pos_embed.shape[0]
    old_width = old_pos_embed.shape[1]
    num_pos = model_pos_embed.shape[0]
    width = model_pos_embed.shape[1]
    assert old_width == width, 'text pos_embed width changed!'
    if old_num_pos == num_pos:
        return

    logging.info('Resizing text position embedding num_pos from %s to %s', old_num_pos, num_pos)
    old_pos_embed = old_pos_embed.reshape(1, old_num_pos, old_width).permute(0, 2, 1)
    old_pos_embed = F.interpolate(
        old_pos_embed,
        size=num_pos,
        mode=interpolation,
        antialias=antialias,
        align_corners=False,
    )
    old_pos_embed = old_pos_embed.permute(0, 2, 1)[0]
    new_pos_embed = old_pos_embed

    state_dict[pos_embed_key] = new_pos_embed


def get_model_preprocess_cfg(model):
    module = getattr(model, 'visual', model)
    preprocess_cfg = getattr(module, 'preprocess_cfg', {})
    if not preprocess_cfg:
        # use separate legacy attributes if preprocess_cfg dict not found
        size = getattr(module, 'image_size')
        if size is not None:
            preprocess_cfg['size'] = size
        mean = getattr(module, 'image_mean', None)
        if mean is not None:
            preprocess_cfg['mean'] = mean
        std = getattr(module, 'image_std', None)
        if std is not None:
            preprocess_cfg['std'] = std
    return preprocess_cfg


def set_model_preprocess_cfg(model, preprocess_cfg: Dict[str, Any]):
    module = getattr(model, 'visual', model)
    module.image_mean = preprocess_cfg['mean']  # legacy attribute, keeping for bwd compat
    module.image_std = preprocess_cfg['std']  # legacy attribute, keeping for bwd compat
    module.preprocess_cfg = copy.deepcopy(preprocess_cfg)  # new attr, package all pp cfg as dict


def get_model_tokenize_cfg(model):
    module = getattr(model, 'text', model)
    cfg = {}
    context_length = getattr(module, 'context_length', None)
    if context_length is not None:
        cfg['context_length'] = context_length
    vocab_size = getattr(module, 'vocab_size', None)
    if vocab_size is not None:
        cfg['vocab_size'] = vocab_size
    return cfg
===== src/open_clip/timm_model.py =====
""" timm model adapter

Wraps timm (https://github.com/rwightman/pytorch-image-models) models for use as a vision tower in CLIP model.
"""
import logging
from collections import OrderedDict
from typing import Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn

try:
    import timm
    from timm.layers import RotAttentionPool2d
    from timm.layers import AttentionPool2d as AbsAttentionPool2d
    from timm.layers import Mlp, to_2tuple
except ImportError:
    timm = None

from .utils import freeze_batch_norm_2d


class TimmModel(nn.Module):
    """ timm model adapter
    """

    def __init__(
            self,
            model_name: str,
            embed_dim: int,
            image_size: Union[int, Tuple[int, int]] = 224,
            pool: str = 'avg',
            proj: str = 'linear',
            proj_bias: bool = False,
            drop: float = 0.,
            drop_path: Optional[float] = None,
            patch_drop: Optional[float] = None,
            pretrained: bool = False,
    ):
        super().__init__()
        if timm is None:
            raise RuntimeError("Please install the latest timm (`pip install timm`) to use timm based models.")
        self.image_size = to_2tuple(image_size)

        # setup kwargs that may not be common across all models
        timm_kwargs = {}
        if drop_path is not None:
            timm_kwargs['drop_path_rate'] = drop_path
        if patch_drop is not None:
            timm_kwargs['patch_drop_rate'] = patch_drop

        custom_pool = pool in ('abs_attn', 'rot_attn')
        if proj:
            assert proj in ("linear", "mlp", "none")
        extra_proj = proj in ("linear", "mlp")
        if not extra_proj and not custom_pool:
            # use network classifier head as projection if no proj specified and no custom pooling used
            # if projection is explicitly set to "none" will be pass through from network trunk
            proj_dim = 0 if proj == 'none' else embed_dim
            self.trunk = timm.create_model(
                model_name,
                num_classes=proj_dim,
                global_pool=pool,
                pretrained=pretrained,
                **timm_kwargs,
            )
            prev_chs = embed_dim
        else:
            self.trunk = timm.create_model(
                model_name,
                pretrained=pretrained,
                **timm_kwargs,
            )
            feat_size = self.trunk.default_cfg.get('pool_size', None)
            feature_ndim = 1 if not feat_size else 2
            if custom_pool:
                assert feature_ndim == 2
                # if attn pooling used, remove both classifier and default pool
                self.trunk.reset_classifier(0, global_pool='')
            else:
                # reset global pool if pool config set, otherwise leave as network default
                reset_kwargs = dict(global_pool=pool) if pool else {}
                self.trunk.reset_classifier(0, **reset_kwargs)
            prev_chs = self.trunk.num_features

        head_layers = OrderedDict()

        # Add custom pooling to head
        if pool == 'abs_attn':
            head_layers['pool'] = AbsAttentionPool2d(prev_chs, feat_size=feat_size, out_features=embed_dim)
            prev_chs = embed_dim
        elif pool == 'rot_attn':
            head_layers['pool'] = RotAttentionPool2d(prev_chs, out_features=embed_dim)
            prev_chs = embed_dim

        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used
        if proj == 'linear':
            head_layers['drop'] = nn.Dropout(drop)
            head_layers['proj'] = nn.Linear(prev_chs, embed_dim, bias=proj_bias)
        elif proj == 'mlp':
            head_layers['mlp'] = Mlp(prev_chs, 2 * embed_dim, embed_dim, drop=(drop, 0), bias=(True, proj_bias))

        self.head = nn.Sequential(head_layers)

    def lock(self, unlocked_groups: int = 0, freeze_bn_stats: bool = False):
        """ lock modules
        Args:
            unlocked_groups (int): leave last n layer groups unlocked (default: 0)
        """
        if not unlocked_groups:
            # lock full model
            for param in self.trunk.parameters():
                param.requires_grad = False
            if freeze_bn_stats:
                freeze_batch_norm_2d(self.trunk)
        else:
            # NOTE: partial freeze requires latest timm (master) branch and is subject to change
            try:
                # FIXME import here until API stable and in an official release
                from timm.models.helpers import group_parameters, group_modules
            except ImportError:
                raise RuntimeError(
                    'Please install latest timm `pip install git+https://github.com/rwightman/pytorch-image-models`')
            matcher = self.trunk.group_matcher()
            gparams = group_parameters(self.trunk, matcher)
            max_layer_id = max(gparams.keys())
            max_layer_id = max_layer_id - unlocked_groups
            for group_idx in range(max_layer_id + 1):
                group = gparams[group_idx]
                for param in group:
                    self.trunk.get_parameter(param).requires_grad = False
            if freeze_bn_stats:
                gmodules = group_modules(self.trunk, matcher, reverse=True)
                gmodules = {k for k, v in gmodules.items() if v <= max_layer_id}
                freeze_batch_norm_2d(self.trunk, gmodules)

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable: bool = True):
        try:
            self.trunk.set_grad_checkpointing(enable)
        except Exception as e:
            logging.warning('grad checkpointing not supported for this timm image tower, continuing without...')

    def forward_intermediates(
            self,
            x: torch.Tensor,
            indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
            normalize_intermediates: bool = False,
            intermediates_only: bool = False,
            output_fmt: str = 'NCHW',
            output_extra_tokens: bool = False,
    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """ Forward features that returns intermediates.

        Args:
            x: Input image tensor
            indices: Take last n blocks if int, all if None, select matching indices if sequence
            stop_early: Stop iterating over blocks when last desired intermediate hit
            normalize_intermediates: Apply norm layer to all intermediates
            intermediates_only: Only return intermediate features
            output_fmt: Shape of intermediate feature outputs
            output_extra_tokens: Return both prefix and spatial intermediate tokens
        Returns:
        """
        extra_args = {}
        if output_extra_tokens:
            extra_args['return_prefix_tokens'] = True
        trunk_output = self.trunk.forward_intermediates(
                x,
                indices=indices,
                intermediates_only=intermediates_only,
                norm=normalize_intermediates,
                stop_early=stop_early,
                output_fmt=output_fmt,
                **extra_args,
            )

        return_dict = {}
        intermediates = trunk_output if intermediates_only else trunk_output[1]
        if output_extra_tokens and intermediates and isinstance(intermediates[0], tuple):
            intermediates_prefix = [xi[1] for xi in intermediates]
            intermediates = [xi[0] for xi in intermediates]
            return_dict['image_intermediates_prefix'] = intermediates_prefix

        return_dict['image_intermediates'] = intermediates
        if intermediates_only:
            return return_dict

        image_features = self.trunk.forward_head(trunk_output[0])  # run through timm pooling / projection
        image_features = self.head(image_features) # run through adapter pooling / projection
        return_dict['image_features'] = image_features
        return return_dict

    def set_input_size(self, image_size: Union[int, Tuple[int, int]]):
        """Set the input image size for the model after initialization.

        This method attempts to call set_input_size on the underlying timm model
        if it supports dynamic input size adjustment.

        Args:
            image_size: New image size as int (square) or tuple (h, w)
        """
        self.image_size = to_2tuple(image_size)

        # Check if the underlying timm model has set_input_size method
        if hasattr(self.trunk, 'set_input_size'):
            self.trunk.set_input_size(image_size)
        else:
            logging.info(f"timm model {self.trunk.__class__.__name__} does not have set_input_size method. Skipping.")

    def forward(self, x):
        x = self.trunk(x)
        x = self.head(x)
        return x

===== src/open_clip/zero_shot_classifier.py =====
from functools import partial
from itertools import islice
from typing import Callable, List, Optional, Sequence, Union

import torch
import torch.nn.functional as F


def batched(iterable, n):
    """Batch data into lists of length *n*. The last batch may be shorter.
    NOTE based on more-itertools impl, to be replaced by python 3.12 itertools.batched impl
    """
    it = iter(iterable)
    while True:
        batch = list(islice(it, n))
        if not batch:
            break
        yield batch


def build_zero_shot_classifier(
        model,
        tokenizer,
        classnames: Sequence[str],
        templates: Sequence[Union[Callable, str]],
        num_classes_per_batch: Optional[int] = 10,
        device: Union[str, torch.device] = 'cpu',
        use_tqdm: bool = False,
):
    """ Build zero-shot classifier weights by iterating over class names in batches
    Args:
        model: CLIP model instance
        tokenizer: CLIP tokenizer instance
        classnames: A sequence of class (label) names
        templates: A sequence of callables or format() friendly strings to produce templates per class name
        num_classes_per_batch: The number of classes to batch together in each forward, all if None
        device: Device to use.
        use_tqdm: Enable TQDM progress bar.
    """
    assert isinstance(templates, Sequence) and len(templates) > 0
    assert isinstance(classnames, Sequence) and len(classnames) > 0
    use_format = isinstance(templates[0], str)
    num_templates = len(templates)
    num_classes = len(classnames)
    if use_tqdm:
        import tqdm
        num_iter = 1 if num_classes_per_batch is None else ((num_classes - 1) // num_classes_per_batch + 1)
        iter_wrap = partial(tqdm.tqdm, total=num_iter, unit_scale=num_classes_per_batch)
    else:
        iter_wrap = iter

    def _process_batch(batch_classnames):
        num_batch_classes = len(batch_classnames)
        texts = [template.format(c) if use_format else template(c) for c in batch_classnames for template in templates]
        texts = tokenizer(texts).to(device)
        class_embeddings = model.encode_text(texts, normalize=True)
        class_embeddings = class_embeddings.reshape(num_batch_classes, num_templates, -1).mean(dim=1)
        class_embeddings = class_embeddings / class_embeddings.norm(dim=1, keepdim=True)
        class_embeddings = class_embeddings.T
        return class_embeddings

    with torch.no_grad():
        if num_classes_per_batch:
            batched_embeds = [_process_batch(batch) for batch in iter_wrap(batched(classnames, num_classes_per_batch))]
            zeroshot_weights = torch.cat(batched_embeds, dim=1)
        else:
            zeroshot_weights = _process_batch(classnames)
    return zeroshot_weights


def build_zero_shot_classifier_legacy(
        model,
        tokenizer,
        classnames: Sequence[str],
        templates: Sequence[Union[Callable, str]],
        device: Union[str, torch.device] = 'cpu',
        use_tqdm: bool = False,
):
    """ Build zero-shot classifier weights by iterating over class names 1 by 1
    Args:
        model: CLIP model instance
        tokenizer: CLIP tokenizer instance
        classnames: A sequence of class (label) names
        templates: A sequence of callables or format() friendly strings to produce templates per class name
        device: Device to use.
        use_tqdm: Enable TQDM progress bar.
    """
    assert isinstance(templates, Sequence) and len(templates) > 0
    assert isinstance(classnames, Sequence) and len(classnames) > 0
    if use_tqdm:
        import tqdm
        iter_wrap = tqdm.tqdm
    else:
        iter_wrap = iter

    use_format = isinstance(templates[0], str)

    with torch.no_grad():
        zeroshot_weights = []
        for classname in iter_wrap(classnames):
            texts = [template.format(classname) if use_format else template(classname) for template in templates]
            texts = tokenizer(texts).to(device)  # tokenize
            class_embeddings = model.encode_text(texts)
            class_embedding = F.normalize(class_embeddings, dim=-1).mean(dim=0)
            class_embedding /= class_embedding.norm()
            zeroshot_weights.append(class_embedding)
        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(device)

    return zeroshot_weights


===== src/open_clip/pretrained.py =====
import copy
import hashlib
import os
import urllib
import warnings
from functools import partial
from typing import Dict, Iterable, Optional, Union

from tqdm import tqdm


try:
    import safetensors.torch
    _has_safetensors = True
except ImportError:
    _has_safetensors = False


from .constants import (
    IMAGENET_MEAN,
    IMAGENET_STD,
    INCEPTION_MEAN,
    INCEPTION_STD,
    OPENAI_DATASET_MEAN,
    OPENAI_DATASET_STD,
    HF_WEIGHTS_NAME,
    HF_SAFE_WEIGHTS_NAME,
)
from .version import __version__

try:
    from huggingface_hub import hf_hub_download
    hf_hub_download = partial(hf_hub_download, library_name="open_clip", library_version=__version__)
    _has_hf_hub = True
except ImportError:
    hf_hub_download = None
    _has_hf_hub = False


def _pcfg(url='', hf_hub='', **kwargs):
    # OpenAI / OpenCLIP defaults
    return {
        'url': url,
        'hf_hub': hf_hub,
        'mean': OPENAI_DATASET_MEAN,
        'std': OPENAI_DATASET_STD,
        'interpolation': 'bicubic',
        'resize_mode': 'shortest',
        **kwargs,
    }


def _slpcfg(url='', hf_hub='', **kwargs):
    # SiGLIP defaults
    return {
        'url': url,
        'hf_hub': hf_hub,
        'mean': INCEPTION_MEAN,
        'std': INCEPTION_STD,
        'interpolation': 'bicubic',
        'resize_mode': 'squash',
        **kwargs,
    }


def _apcfg(url='', hf_hub='', **kwargs):
    # CLIPA defaults
    return {
        'url': url,
        'hf_hub': hf_hub,
        'mean': IMAGENET_MEAN,
        'std': IMAGENET_STD,
        'interpolation': 'bilinear',
        'resize_mode': 'squash',
        **kwargs,
    }


def _mccfg(url='', hf_hub='', **kwargs):
    # MobileCLIP
    return {
        'url': url,
        'hf_hub': hf_hub,
        'mean': (0., 0., 0.),
        'std': (1., 1., 1.),
        'interpolation': 'bilinear',
        'resize_mode': 'shortest',
        **kwargs,
    }


def _pecfg(url='', hf_hub='', **kwargs):
    # PE
    return {
        'url': url,
        'hf_hub': hf_hub,
        'mean': (0.5, 0.5, 0.5),
        'std': (0.5, 0.5, 0.5),
        'interpolation': 'bilinear',
        'resize_mode': 'squash',
        **kwargs,
    }


_RN50 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt",
        hf_hub="timm/resnet50_clip.openai/",
        quick_gelu=True,
    ),
    yfcc15m=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-yfcc15m-455df137.pt",
        hf_hub="timm/resnet50_clip.yfcc15m/",
        quick_gelu=True,
    ),
    cc12m=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-cc12m-f000538c.pt",
        hf_hub="timm/resnet50_clip.cc12m/",
        quick_gelu=True,
    ),
)

_RN101 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt",
        hf_hub="timm/resnet101_clip.openai/",
        quick_gelu=True,
    ),
    yfcc15m=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn101-quickgelu-yfcc15m-3e04b30e.pt",
        hf_hub="timm/resnet101_clip.yfcc15m/",
        quick_gelu=True,
    ),
)

_RN50x4 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt",
        hf_hub="timm/resnet50x4_clip.openai/",
        quick_gelu=True,
    ),
)

_RN50x16 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt",
        hf_hub="timm/resnet50x16_clip.openai/",
        quick_gelu=True,
    ),
)

_RN50x64 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt",
        hf_hub="timm/resnet50x64_clip.openai/",
        quick_gelu=True,
    ),
)

_VITB32 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt",
        hf_hub="timm/vit_base_patch32_clip_224.openai/",
        quick_gelu=True,
    ),
    # LAION 400M (quick gelu)
    laion400m_e31=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt",
        hf_hub="timm/vit_base_patch32_clip_224.laion400m_e31/",
        quick_gelu=True,
    ),
    laion400m_e32=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt",
        hf_hub="timm/vit_base_patch32_clip_224.laion400m_e32/",
        quick_gelu=True,
    ),
    # LAION 2B-en
    laion2b_e16=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-laion2b_e16-af8dbd0c.pth",
        hf_hub="timm/vit_base_patch32_clip_224.laion2b_e16/",
    ),
    laion2b_s34b_b79k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-laion2B-s34B-b79K/'),
    # DataComp-XL models
    datacomp_xl_s13b_b90k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K/'),
    # DataComp-M models
    datacomp_m_s128m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-DataComp.M-s128M-b4K/'),
    commonpool_m_clip_s128m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.M.clip-s128M-b4K/'),
    commonpool_m_laion_s128m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.M.laion-s128M-b4K/'),
    commonpool_m_image_s128m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.M.image-s128M-b4K/'),
    commonpool_m_text_s128m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.M.text-s128M-b4K/'),
    commonpool_m_basic_s128m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.M.basic-s128M-b4K/'),
    commonpool_m_s128m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.M-s128M-b4K/'),
    # DataComp-S models
    datacomp_s_s13m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-DataComp.S-s13M-b4K/'),
    commonpool_s_clip_s13m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.S.clip-s13M-b4K/'),
    commonpool_s_laion_s13m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.S.laion-s13M-b4K/'),
    commonpool_s_image_s13m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.S.image-s13M-b4K/'),
    commonpool_s_text_s13m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.S.text-s13M-b4K/'),
    commonpool_s_basic_s13m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.S.basic-s13M-b4K/'),
    commonpool_s_s13m_b4k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-CommonPool.S-s13M-b4K/'),
    # MetaClip models (NOTE quick-gelu activation used)
    metaclip_400m=_pcfg(
        url="https://dl.fbaipublicfiles.com/MMPT/metaclip/b32_400m.pt",
        hf_hub="timm/vit_base_patch32_clip_224.metaclip_400m/",
        quick_gelu=True,
    ),
    metaclip_fullcc=_pcfg(
        url="https://dl.fbaipublicfiles.com/MMPT/metaclip/b32_fullcc2.5b.pt",
        hf_hub="timm/vit_base_patch32_clip_224.metaclip_2pt5b/",
        quick_gelu=True,
    ),
)

_VITB32_256 = dict(
    datacomp_s34b_b86k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-256x256-DataComp-s34B-b86K/'),
)

_VITB16 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt",
        hf_hub="timm/vit_base_patch16_clip_224.openai/",
        quick_gelu=True,
    ),
    # LAION-400M
    laion400m_e31=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e31-00efa78f.pt",
        hf_hub="timm/vit_base_patch16_clip_224.laion400m_e31/",
    ),
    laion400m_e32=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e32-55e67d44.pt",
        hf_hub="timm/vit_base_patch16_clip_224.laion400m_e32/",
    ),
    # LAION-2B
    laion2b_s34b_b88k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-laion2B-s34B-b88K/'),
    # DataComp-XL models
    datacomp_xl_s13b_b90k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K/'),
    # DataComp-L models
    datacomp_l_s1b_b8k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K/'),
    commonpool_l_clip_s1b_b8k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-CommonPool.L.clip-s1B-b8K/'),
    commonpool_l_laion_s1b_b8k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-CommonPool.L.laion-s1B-b8K/'),
    commonpool_l_image_s1b_b8k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-CommonPool.L.image-s1B-b8K/'),
    commonpool_l_text_s1b_b8k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-CommonPool.L.text-s1B-b8K/'),
    commonpool_l_basic_s1b_b8k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-CommonPool.L.basic-s1B-b8K/'),
    commonpool_l_s1b_b8k=_pcfg(hf_hub='laion/CLIP-ViT-B-16-CommonPool.L-s1B-b8K/'),
    # DFN
    dfn2b=_pcfg(
        hf_hub='apple/DFN2B-CLIP-ViT-B-16/',
        quick_gelu=True,
    ),
    # MetaCLIP (these are quick-gelu)
    metaclip_400m=_pcfg(
        url="https://dl.fbaipublicfiles.com/MMPT/metaclip/b16_400m.pt",
        hf_hub="timm/vit_base_patch16_clip_224.metaclip_400m/",
        quick_gelu=True,
    ),
    metaclip_fullcc=_pcfg(
        url="https://dl.fbaipublicfiles.com/MMPT/metaclip/b16_fullcc2.5b.pt",
        hf_hub="timm/vit_base_patch16_clip_224.metaclip_2pt5b/",
        quick_gelu=True,
    ),
)

_VITB16_PLUS_240 = dict(
    laion400m_e31=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e31-8fb26589.pt",
        hf_hub="timm/vit_base_patch16_plus_clip_240.laion400m_e31/",
    ),
    laion400m_e32=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e32-699c4b84.pt",
        hf_hub="timm/vit_base_patch16_plus_clip_240.laion400m_e31/",
    ),
)

_VITL14 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt",
        hf_hub="timm/vit_large_patch14_clip_224.openai/",
        quick_gelu=True,
    ),
    # LAION-400M
    laion400m_e31=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e31-69988bb6.pt",
        hf_hub="timm/vit_large_patch14_clip_224.laion400m_e31/",
    ),
    laion400m_e32=_pcfg(
        url="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e32-3d133497.pt",
        hf_hub="timm/vit_large_patch14_clip_224.laion400m_e32/",
    ),
    # LAION-2B-en
    laion2b_s32b_b82k=_pcfg(
        hf_hub='laion/CLIP-ViT-L-14-laion2B-s32B-b82K/',
        mean=INCEPTION_MEAN, std=INCEPTION_STD),
    # DataComp-XL models
    datacomp_xl_s13b_b90k=_pcfg(hf_hub='laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/'),
    commonpool_xl_clip_s13b_b90k=_pcfg(hf_hub='laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K/'),
    commonpool_xl_laion_s13b_b90k=_pcfg(hf_hub='laion/CLIP-ViT-L-14-CommonPool.XL.laion-s13B-b90K/'),
    commonpool_xl_s13b_b90k=_pcfg(hf_hub='laion/CLIP-ViT-L-14-CommonPool.XL-s13B-b90K/'),
    # MetaCLIP
    metaclip_400m=_pcfg(
        url="https://dl.fbaipublicfiles.com/MMPT/metaclip/l14_400m.pt",
        hf_hub="timm/vit_large_patch14_clip_224.metaclip_400m/",
        quick_gelu=True,
    ),
    metaclip_fullcc=_pcfg(
        url="https://dl.fbaipublicfiles.com/MMPT/metaclip/l14_fullcc2.5b.pt",
        hf_hub="timm/vit_large_patch14_clip_224.metaclip_2pt5b/",
        quick_gelu=True,
    ),
    # DFN-2B (quick-gelu)
    dfn2b=_pcfg(
        hf_hub='apple/DFN2B-CLIP-ViT-L-14/',
        quick_gelu=True,
    ),
    # DFN-2B 39B SS
    dfn2b_s39b=_pcfg(
        hf_hub='apple/DFN2B-CLIP-ViT-L-14-39B/',
    ),
)

_VITL14_336 = dict(
    openai=_pcfg(
        url="https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt",
        hf_hub="timm/vit_large_patch14_clip_336.openai/",
        quick_gelu=True,
    ),
)

_VITH14 = dict(
    # LAION-2B-en
    laion2b_s32b_b79k=_pcfg(hf_hub='laion/CLIP-ViT-H-14-laion2B-s32B-b79K/'),
    # MetaCLIP (quick-gelu)
    metaclip_fullcc=_pcfg(
        url="https://dl.fbaipublicfiles.com/MMPT/metaclip/h14_fullcc2.5b.pt",
        hf_hub="timm/vit_huge_patch14_clip_224.metaclip_2pt5b/",
        quick_gelu=True,
    ),
    metaclip_altogether=_pcfg(
        url="https://dl.fbaipublicfiles.com/MMPT/metaclip/h14_v1.2_altogether.pt",
        hf_hub="timm/vit_huge_patch14_clip_224.metaclip_altogether/",
        # NOTE unlike other MetaCLIP models, this is not using QuickGELU, yay!
    ),
    # DFN-5B (quick-gelu)
    dfn5b=_pcfg(
        hf_hub='apple/DFN5B-CLIP-ViT-H-14/',
        quick_gelu=True,
        interpolation="bicubic",
        resize_mode="squash"
    ),
)

_VITH14_378 = dict(
    # DFN-5B (quick-gelu)
    dfn5b=_pcfg(
        hf_hub='apple/DFN5B-CLIP-ViT-H-14-378/',
        quick_gelu=True,
        interpolation="bicubic",
        resize_mode="squash"
    ),
)

_VITg14 = dict(
    laion2b_s12b_b42k=_pcfg(hf_hub='laion/CLIP-ViT-g-14-laion2B-s12B-b42K/'),
    laion2b_s34b_b88k=_pcfg(hf_hub='laion/CLIP-ViT-g-14-laion2B-s34B-b88K/'),
)

_VITbigG14 = dict(
    # LAION-2B-en
    laion2b_s39b_b160k=_pcfg(hf_hub='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k/'),
    # MetaCLIP (quick-gelu)
    metaclip_fullcc=_pcfg(
        url='https://dl.fbaipublicfiles.com/MMPT/metaclip/G14_fullcc2.5b.pt',
        hf_hub="timm/vit_gigantic_patch14_clip_224.metaclip_2pt5b/",
        quick_gelu=True,
    ),
)

_robertaViTB32 = dict(
    laion2b_s12b_b32k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k/'),
)

_xlmRobertaBaseViTB32 = dict(
    laion5b_s13b_b90k=_pcfg(hf_hub='laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k/'),
)

_xlmRobertaLargeFrozenViTH14 = dict(
    frozen_laion5b_s13b_b90k=_pcfg(hf_hub='laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/'),
)

_convnext_base = dict(
    laion400m_s13b_b51k=_pcfg(hf_hub='laion/CLIP-convnext_base-laion400M-s13B-b51K/'),
)

_convnext_base_w = dict(
    laion2b_s13b_b82k=_pcfg(hf_hub='laion/CLIP-convnext_base_w-laion2B-s13B-b82K/'),
    laion2b_s13b_b82k_augreg=_pcfg(hf_hub='laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg/'),
    laion_aesthetic_s13b_b82k=_pcfg(hf_hub='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K/'),
)

_convnext_base_w_320 = dict(
    laion_aesthetic_s13b_b82k=_pcfg(hf_hub='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K/'),
    laion_aesthetic_s13b_b82k_augreg=_pcfg(hf_hub='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg/'),
)

_convnext_large_d = dict(
    laion2b_s26b_b102k_augreg=_pcfg(hf_hub='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg/'),
)

_convnext_large_d_320 = dict(
    laion2b_s29b_b131k_ft=_pcfg(hf_hub='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft/'),
    laion2b_s29b_b131k_ft_soup=_pcfg(hf_hub='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup/'),
)

_convnext_xxlarge = dict(
    laion2b_s34b_b82k_augreg=_pcfg(hf_hub='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg/'),
    laion2b_s34b_b82k_augreg_rewind=_pcfg(hf_hub='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind/'),
    laion2b_s34b_b82k_augreg_soup=_pcfg(hf_hub='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup/'),
)

_coca_VITB32 = dict(
    laion2b_s13b_b90k=_pcfg(hf_hub='laion/CoCa-ViT-B-32-laion2B-s13B-b90k/'),
    mscoco_finetuned_laion2b_s13b_b90k=_pcfg(hf_hub='laion/mscoco_finetuned_CoCa-ViT-B-32-laion2B-s13B-b90k/')
)

_coca_VITL14 = dict(
    laion2b_s13b_b90k=_pcfg(hf_hub='laion/CoCa-ViT-L-14-laion2B-s13B-b90k/'),
    mscoco_finetuned_laion2b_s13b_b90k=_pcfg(hf_hub='laion/mscoco_finetuned_CoCa-ViT-L-14-laion2B-s13B-b90k/')
)


_PRETRAINED = {
    "RN50": _RN50,
    "RN101": _RN101,
    "RN50x4": _RN50x4,
    "RN50x16": _RN50x16,
    "RN50x64": _RN50x64,

    "ViT-B-32": _VITB32,
    "ViT-B-32-256": _VITB32_256,
    "ViT-B-16": _VITB16,
    "ViT-B-16-plus-240": _VITB16_PLUS_240,
    "ViT-L-14": _VITL14,
    "ViT-L-14-336": _VITL14_336,
    "ViT-H-14": _VITH14,
    "ViT-H-14-378": _VITH14_378,
    "ViT-g-14": _VITg14,
    "ViT-bigG-14": _VITbigG14,

    "roberta-ViT-B-32": _robertaViTB32,
    "xlm-roberta-base-ViT-B-32": _xlmRobertaBaseViTB32,
    "xlm-roberta-large-ViT-H-14": _xlmRobertaLargeFrozenViTH14,

    "convnext_base": _convnext_base,
    "convnext_base_w": _convnext_base_w,
    "convnext_base_w_320": _convnext_base_w_320,
    "convnext_large_d": _convnext_large_d,
    "convnext_large_d_320": _convnext_large_d_320,
    "convnext_xxlarge": _convnext_xxlarge,

    "coca_ViT-B-32": _coca_VITB32,
    "coca_ViT-L-14": _coca_VITL14,

    "EVA01-g-14": dict(
        # from QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt
        laion400m_s11b_b41k=_pcfg(hf_hub='timm/eva_giant_patch14_clip_224.laion400m_s11b_b41k/'),
    ),
    "EVA01-g-14-plus": dict(
        # from QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt
        merged2b_s11b_b114k=_pcfg(hf_hub='timm/eva_giant_patch14_plus_clip_224.merged2b_s11b_b114k/'),
    ),
    "EVA02-B-16": dict(
        # from QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt
        merged2b_s8b_b131k=_pcfg(hf_hub='timm/eva02_base_patch16_clip_224.merged2b_s8b_b131k/'),
    ),
    "EVA02-L-14": dict(
        # from QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt
        merged2b_s4b_b131k=_pcfg(hf_hub='timm/eva02_large_patch14_clip_224.merged2b_s4b_b131k/'),
    ),
    "EVA02-L-14-336": dict(
        # from QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt
        merged2b_s6b_b61k=_pcfg(hf_hub='timm/eva02_large_patch14_clip_336.merged2b_s6b_b61k/'),
    ),
    "EVA02-E-14": dict(
        # from QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt
        laion2b_s4b_b115k=_pcfg(hf_hub='timm/eva02_enormous_patch14_clip_224.laion2b_s4b_b115k/'),
    ),
    "EVA02-E-14-plus": dict(
        # from QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt
        laion2b_s9b_b144k=_pcfg(hf_hub='timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k/'),
    ),

    "ViT-B-16-SigLIP": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP/'),
    ),
    "ViT-B-16-SigLIP-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP-256/'),
    ),
    "ViT-B-16-SigLIP-i18n-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP-i18n-256/'),
    ),
    "ViT-B-16-SigLIP-384": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP-384/'),
    ),
    "ViT-B-16-SigLIP-512": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP-512/'),
    ),
    "ViT-L-16-SigLIP-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-L-16-SigLIP-256/'),
    ),
    "ViT-L-16-SigLIP-384": dict(
        webli=_slpcfg(hf_hub='timm/ViT-L-16-SigLIP-384/'),
    ),
    "ViT-SO400M-14-SigLIP": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-14-SigLIP/'),
    ),
    "ViT-SO400M-16-SigLIP-i18n-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-16-SigLIP-i18n-256/'),
    ),
    "ViT-SO400M-14-SigLIP-378": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-14-SigLIP-384/'),  # NOTE using 384 weights, but diff img_size used
    ),
    "ViT-SO400M-14-SigLIP-384": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-14-SigLIP-384/'),
    ),

    "ViT-B-32-SigLIP2-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-32-SigLIP2-256/'),
    ),
    "ViT-B-16-SigLIP2": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP2/'),
    ),
    "ViT-B-16-SigLIP2-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP2-256/'),
    ),
    "ViT-B-16-SigLIP2-384": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP2-384/'),
    ),
    "ViT-B-16-SigLIP2-512": dict(
        webli=_slpcfg(hf_hub='timm/ViT-B-16-SigLIP2-512/'),
    ),
    "ViT-L-16-SigLIP2-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-L-16-SigLIP2-256/'),
    ),
    "ViT-L-16-SigLIP2-384": dict(
        webli=_slpcfg(hf_hub='timm/ViT-L-16-SigLIP2-384/'),
    ),
    "ViT-L-16-SigLIP2-512": dict(
        webli=_slpcfg(hf_hub='timm/ViT-L-16-SigLIP2-512/'),
    ),
    "ViT-SO400M-14-SigLIP2": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-14-SigLIP2/'),
    ),
    "ViT-SO400M-14-SigLIP2-378": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-14-SigLIP2-378/'),
    ),
    "ViT-SO400M-16-SigLIP2-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-16-SigLIP2-256/'),
    ),
    "ViT-SO400M-16-SigLIP2-384": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-16-SigLIP2-384/'),
    ),
    "ViT-SO400M-16-SigLIP2-512": dict(
        webli=_slpcfg(hf_hub='timm/ViT-SO400M-16-SigLIP2-512/'),
    ),
    "ViT-gopt-16-SigLIP2-256": dict(
        webli=_slpcfg(hf_hub='timm/ViT-gopt-16-SigLIP2-256/'),
    ),
    "ViT-gopt-16-SigLIP2-384": dict(
        webli=_slpcfg(hf_hub='timm/ViT-gopt-16-SigLIP2-384/'),
    ),

    "ViT-L-14-CLIPA": dict(
        datacomp1b=_apcfg(hf_hub='UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B/'),
    ),
    "ViT-L-14-CLIPA-336": dict(
        datacomp1b=_apcfg(hf_hub='UCSC-VLAA/ViT-L-14-CLIPA-336-datacomp1B/'),
    ),
    "ViT-H-14-CLIPA": dict(
        datacomp1b=_apcfg(hf_hub='UCSC-VLAA/ViT-H-14-CLIPA-datacomp1B/'),
    ),
    "ViT-H-14-CLIPA-336": dict(
        laion2b=_apcfg(hf_hub='UCSC-VLAA/ViT-H-14-CLIPA-336-laion2B/'),
        datacomp1b=_apcfg(hf_hub='UCSC-VLAA/ViT-H-14-CLIPA-336-datacomp1B/'),
    ),
    "ViT-bigG-14-CLIPA": dict(
        datacomp1b=_apcfg(hf_hub='UCSC-VLAA/ViT-bigG-14-CLIPA-datacomp1B/'),
    ),
    "ViT-bigG-14-CLIPA-336": dict(
        datacomp1b=_apcfg(hf_hub='UCSC-VLAA/ViT-bigG-14-CLIPA-336-datacomp1B/'),
    ),

    "nllb-clip-base": dict(
        v1=_pcfg(hf_hub='visheratin/nllb-clip-base-oc/'),
    ),
    "nllb-clip-large": dict(
        v1=_pcfg(hf_hub='visheratin/nllb-clip-large-oc/'),
    ),

    "nllb-clip-base-siglip": dict(
        v1=_slpcfg(hf_hub='visheratin/nllb-clip-base-siglip/'),
        mrl=_slpcfg(hf_hub='visheratin/nllb-siglip-mrl-base/'),
    ),
    "nllb-clip-large-siglip": dict(
        v1=_slpcfg(hf_hub='visheratin/nllb-clip-large-siglip/'),
        mrl=_slpcfg(hf_hub='visheratin/nllb-siglip-mrl-large/'),
    ),

    "MobileCLIP-S1": dict(
        datacompdr=_mccfg(hf_hub='apple/MobileCLIP-S1-OpenCLIP/')),
    "MobileCLIP-S2": dict(
        datacompdr=_mccfg(hf_hub='apple/MobileCLIP-S2-OpenCLIP/')),
    "MobileCLIP-B": dict(
        datacompdr=_mccfg(hf_hub='apple/MobileCLIP-B-OpenCLIP/'),
        datacompdr_lt=_mccfg(hf_hub='apple/MobileCLIP-B-LT-OpenCLIP/'),
    ),

    "ViTamin-S": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-S/pytorch_model.bin'),
    ),
    "ViTamin-S-LTT": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-S-LTT/pytorch_model.bin'),
    ),
    "ViTamin-B": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-B/pytorch_model.bin'),
    ),
    "ViTamin-B-LTT": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-B-LTT/pytorch_model.bin'),
    ),
    "ViTamin-L": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-L-224px/pytorch_model.bin'),
    ),
    "ViTamin-L-256": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-L-256px/pytorch_model.bin'),
    ),
    "ViTamin-L-336": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-L-336px/pytorch_model.bin'),
    ),
    "ViTamin-L-384": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-L-384px/pytorch_model.bin'),
    ),
    "ViTamin-L2": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-L2-224px/pytorch_model.bin'),
    ),
    "ViTamin-L2-256": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-L2-256px/pytorch_model.bin'),
    ),
    "ViTamin-L2-336": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-L2-336px/pytorch_model.bin'),
    ),
    "ViTamin-L2-384": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-L2-384px/pytorch_model.bin'),
    ),
    "ViTamin-XL-256": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-XL-256px/pytorch_model.bin'),
    ),
    "ViTamin-XL-336": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-XL-336px/pytorch_model.bin'),
    ),
    "ViTamin-XL-384": dict(
        datacomp1b=_pcfg(hf_hub='jienengchen/ViTamin-XL-384px/pytorch_model.bin'),
    ),

    "PE-Core-T-16-384": dict(
        # original at facebook/PE-Core-T16-384/PE-Core-T16-384.pt
        meta=_pecfg(hf_hub='timm/PE-Core-T-16-384/'),
    ),
    "PE-Core-S-16-384": dict(
        # original at facebook/PE-Core-S16-384/PE-Core-S16-384.pt
        meta=_pecfg(hf_hub='timm/PE-Core-S-16-384/'),
    ),
    "PE-Core-B-16": dict(
        # original at facebook/PE-Core-B16-224/PE-Core-B16-224.pt
        meta=_pecfg(hf_hub='timm/PE-Core-B-16/'),
    ),
    "PE-Core-L-14-336": dict(
        # original at facebook/PE-Core-L14-336/PE-Core-L14-336.pt
        meta=_pecfg(hf_hub='timm/PE-Core-L-14-336/'),
    ),
    "PE-Core-bigG-14-448": dict(
        # original at facebook/PE-Core-G14-448/PE-Core-G14-448.pt
        meta=_pecfg(hf_hub='timm/PE-Core-bigG-14-448/'),
    ),

    # MetaCLIP 2
    "ViT-L-14-worldwide": dict(
        metaclip2_worldwide=_pcfg(
            hf_hub="timm/vit_large_patch14_clip_224.metaclip2_worldwide/",
            quick_gelu=True,
        ),
    ),
    "ViT-H-14-worldwide": dict(
        metaclip2_worldwide=_pcfg(
            hf_hub="timm/vit_huge_patch14_clip_224.metaclip2_worldwide/",
            quick_gelu=True,
        ),
    ),
    "ViT-H-14-worldwide-378": dict(
        metaclip2_worldwide=_pcfg(
            hf_hub="timm/vit_huge_patch14_clip_378.metaclip2_worldwide/",
            resize_mode="squash",
            # NOTE not quick-gelu
        ),
    ),
    "ViT-bigG-14-worldwide": dict(
        metaclip2_worldwide=_pcfg(hf_hub="timm/vit_gigantic_patch14_clip_224.metaclip2_worldwide/"),
        # NOTE not quick-gelu
    ),
    "ViT-bigG-14-worldwide-378": dict(
        metaclip2_worldwide=_pcfg(
            hf_hub="timm/vit_gigantic_patch14_clip_378.metaclip2_worldwide/",
            resize_mode="squash",
            # NOTE not quick-gelu
        ),

    ),

}

_PRETRAINED_quickgelu = {}
for k, v in _PRETRAINED.items():
    quick_gelu_tags = {}
    for tk, tv in v.items():
        if tv.get('quick_gelu', False):
            quick_gelu_tags[tk] = copy.deepcopy(tv)
    if quick_gelu_tags:
        _PRETRAINED_quickgelu[k + '-quickgelu'] = quick_gelu_tags
_PRETRAINED.update(_PRETRAINED_quickgelu)

def _clean_tag(tag: str):
    # normalize pretrained tags
    return tag.lower().replace('-', '_')


def list_pretrained(as_str: bool = False):
    """ returns list of pretrained models
    Returns a tuple (model_name, pretrain_tag) by default or 'name:tag' if as_str == True
    """
    return [':'.join([k, t]) if as_str else (k, t) for k in _PRETRAINED.keys() for t in _PRETRAINED[k].keys()]


def list_pretrained_models_by_tag(tag: str):
    """ return all models having the specified pretrain tag """
    models = []
    tag = _clean_tag(tag)
    for k in _PRETRAINED.keys():
        if tag in _PRETRAINED[k]:
            models.append(k)
    return models


def list_pretrained_tags_by_model(model: str):
    """ return all pretrain tags for the specified model architecture """
    tags = []
    if model in _PRETRAINED:
        tags.extend(_PRETRAINED[model].keys())
    return tags


def is_pretrained_cfg(model: str, tag: str):
    if model not in _PRETRAINED:
        return False
    return _clean_tag(tag) in _PRETRAINED[model]


def get_pretrained_cfg(model: str, tag: str):
    if model not in _PRETRAINED:
        return {}
    model_pretrained = _PRETRAINED[model]
    return model_pretrained.get(_clean_tag(tag), {})


def get_pretrained_url(model: str, tag: str):
    cfg = get_pretrained_cfg(model, _clean_tag(tag))
    return cfg.get('url', '')


def download_pretrained_from_url(
        url: str,
        cache_dir: Optional[str] = None,
):
    if not cache_dir:
        cache_dir = os.path.expanduser("~/.cache/clip")
    os.makedirs(cache_dir, exist_ok=True)
    filename = os.path.basename(url)

    if 'openaipublic' in url:
        expected_sha256 = url.split("/")[-2]
    elif 'mlfoundations' in url:
        expected_sha256 = os.path.splitext(filename)[0].split("-")[-1]
    else:
        expected_sha256 = ''

    download_target = os.path.join(cache_dir, filename)

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f"{download_target} exists and is not a regular file")

    if os.path.isfile(download_target):
        if expected_sha256:
            if hashlib.sha256(open(download_target, "rb").read()).hexdigest().startswith(expected_sha256):
                return download_target
            else:
                warnings.warn(f"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file")
        else:
            return download_target

    with urllib.request.urlopen(url) as source, open(download_target, "wb") as output:
        with tqdm(total=int(source.headers.get("Content-Length")), ncols=80, unit='iB', unit_scale=True) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                output.write(buffer)
                loop.update(len(buffer))

    if expected_sha256 and not hashlib.sha256(open(download_target, "rb").read()).hexdigest().startswith(expected_sha256):
        raise RuntimeError(f"Model has been downloaded but the SHA256 checksum does not not match")

    return download_target


def has_hf_hub(necessary=False):
    if not _has_hf_hub and necessary:
        # if no HF Hub module installed, and it is necessary to continue, raise error
        raise RuntimeError(
            'Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.')
    return _has_hf_hub


def _get_safe_alternatives(filename: str) -> Iterable[str]:
    """Returns potential safetensors alternatives for a given filename.

    Use case:
        When downloading a model from the Huggingface Hub, we first look if a .safetensors file exists and if yes, we use it.
    """
    if filename == HF_WEIGHTS_NAME:
        yield HF_SAFE_WEIGHTS_NAME

    if filename not in (HF_WEIGHTS_NAME,) and (filename.endswith(".bin") or filename.endswith(".pth")):
        yield filename[:-4] + ".safetensors"


def download_pretrained_from_hf(
        model_id: str,
        filename: Optional[str] = None,
        revision: Optional[str] = None,
        cache_dir: Optional[str] = None,
):
    has_hf_hub(True)

    filename = filename or HF_WEIGHTS_NAME

    # Look for .safetensors alternatives and load from it if it exists
    if _has_safetensors:
        for safe_filename in _get_safe_alternatives(filename):
            try:
                cached_file = hf_hub_download(
                    repo_id=model_id,
                    filename=safe_filename,
                    revision=revision,
                    cache_dir=cache_dir,
                )
                return cached_file
            except Exception:
                pass

    try:
        # Attempt to download the file
        cached_file = hf_hub_download(
            repo_id=model_id,
            filename=filename,
            revision=revision,
            cache_dir=cache_dir,
        )
        return cached_file  # Return the path to the downloaded file if successful
    except Exception as e:
        raise FileNotFoundError(f"Failed to download file ({filename}) for {model_id}. Last error: {e}")


def download_pretrained(
        cfg: Dict,
        prefer_hf_hub: bool = True,
        cache_dir: Optional[str] = None,
):
    target = ''
    if not cfg:
        return target

    if 'file' in cfg:
        return cfg['file']

    has_hub = has_hf_hub()
    download_url = cfg.get('url', '')
    download_hf_hub = cfg.get('hf_hub', '')
    if has_hub and prefer_hf_hub and download_hf_hub:
        # prefer to use HF hub, remove url info
        download_url = ''

    if download_url:
        target = download_pretrained_from_url(download_url, cache_dir=cache_dir)
    elif download_hf_hub:
        has_hf_hub(True)
        # we assume the hf_hub entries in pretrained config combine model_id + filename in
        # 'org/model_name/filename.pt' form. To specify just the model id w/o filename and
        # use 'open_clip_pytorch_model.bin' default, there must be a trailing slash 'org/model_name/'.
        model_id, filename = os.path.split(download_hf_hub)
        if filename:
            target = download_pretrained_from_hf(model_id, filename=filename, cache_dir=cache_dir)
        else:
            target = download_pretrained_from_hf(model_id, cache_dir=cache_dir)

    return target

===== src/open_clip/__init__.py =====
from .version import __version__

from .coca_model import CoCa
from .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD
from .factory import create_model, create_model_and_transforms, create_model_from_pretrained, get_tokenizer, create_loss
from .factory import list_models, add_model_config, get_model_config, load_checkpoint
from .loss import ClipLoss, DistillClipLoss, CoCaLoss
from .model import CLIP, CustomTextCLIP, CLIPTextCfg, CLIPVisionCfg, \
    convert_weights_to_lp, convert_weights_to_fp16, trace_model, get_cast_dtype, get_input_dtype, \
    get_model_tokenize_cfg, get_model_preprocess_cfg, set_model_preprocess_cfg
from .openai import load_openai_model, list_openai_models
from .pretrained import list_pretrained, list_pretrained_models_by_tag, list_pretrained_tags_by_model, \
    get_pretrained_url, download_pretrained_from_url, is_pretrained_cfg, get_pretrained_cfg, download_pretrained
from .push_to_hf_hub import push_pretrained_to_hf_hub, push_to_hf_hub
from .tokenizer import SimpleTokenizer, tokenize, decode
from .transform import image_transform, AugmentationCfg
from .zero_shot_classifier import build_zero_shot_classifier, build_zero_shot_classifier_legacy
from .zero_shot_metadata import OPENAI_IMAGENET_TEMPLATES, SIMPLE_IMAGENET_TEMPLATES, IMAGENET_CLASSNAMES

===== src/open_clip/utils.py =====
import collections.abc
from itertools import repeat
from typing import List, Optional, Tuple, Union

import torch
from torch import nn as nn
from torch import _assert
from torchvision.ops.misc import FrozenBatchNorm2d


def freeze_batch_norm_2d(module, module_match={}, name=''):
    """
    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is
    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and
    returned. Otherwise, the module is walked recursively and submodules are converted in place.

    Args:
        module (torch.nn.Module): Any PyTorch module.
        module_match (dict): Dictionary of full module names to freeze (all if empty)
        name (str): Full module name (prefix)

    Returns:
        torch.nn.Module: Resulting module

    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762
    """
    res = module
    is_match = True
    if module_match:
        is_match = name in module_match
    if is_match and isinstance(module, (nn.modules.batchnorm.BatchNorm2d, nn.modules.batchnorm.SyncBatchNorm)):
        res = FrozenBatchNorm2d(module.num_features)
        res.num_features = module.num_features
        res.affine = module.affine
        if module.affine:
            res.weight.data = module.weight.data.clone().detach()
            res.bias.data = module.bias.data.clone().detach()
        res.running_mean.data = module.running_mean.data
        res.running_var.data = module.running_var.data
        res.eps = module.eps
    else:
        for child_name, child in module.named_children():
            full_child_name = '.'.join([name, child_name]) if name else child_name
            new_child = freeze_batch_norm_2d(child, module_match, full_child_name)
            if new_child is not child:
                res.add_module(child_name, new_child)
    return res


# From PyTorch internals
def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))
    return parse


to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = lambda n, x: _ntuple(n)(x)

# Replaces all linear layers with linear_replacement
# TODO: add int8 support for other linear layers including attn and convnets
def replace_linear(model, linear_replacement, include_modules=['c_fc', 'c_proj'], copy_weights=True):
    for name, module in model.named_children():
        if len(list(module.children())) > 0:
            replace_linear(module, linear_replacement, include_modules, copy_weights)

        if isinstance(module, torch.nn.Linear) and name in include_modules:
            old_module = model._modules[name]
            model._modules[name] = linear_replacement(
                module.in_features,
                module.out_features,
                module.bias is not None,
            )
            if copy_weights:
                model._modules[name].weight.data.copy_(old_module.weight.data)
                if model._modules[name].bias is not None:
                    model._modules[name].bias.data.copy_(old_module.bias)

    return model

def convert_int8_model_to_inference_mode(model):
    for m in model.modules():
        if hasattr(m, 'prepare_for_eval'):
            int8_original_dtype = m.weight.dtype
            m.prepare_for_eval()
            m.int8_original_dtype = int8_original_dtype


def feature_take_indices(
        num_features: int,
        indices: Optional[Union[int, List[int]]] = None,
        as_set: bool = False,
) -> Tuple[List[int], int]:
    """ Determine the absolute feature indices to 'take' from.

    Note: This function can be called in forward() so must be torchscript compatible,
    which requires some incomplete typing and workaround hacks.

    Args:
        num_features: total number of features to select from
        indices: indices to select,
          None -> select all
          int -> select last n
          list/tuple of int -> return specified (-ve indices specify from end)
        as_set: return as a set

    Returns:
        List (or set) of absolute (from beginning) indices, Maximum index
    """
    if indices is None:
        indices = num_features  # all features if None

    if isinstance(indices, int):
        # convert int -> last n indices
        _assert(0 < indices <= num_features, f'last-n ({indices}) is out of range (1 to {num_features})')
        take_indices = [num_features - indices + i for i in range(indices)]
    else:
        take_indices: List[int] = []
        for i in indices:
            idx = num_features + i if i < 0 else i
            _assert(0 <= idx < num_features, f'feature index {idx} is out of range (0 to {num_features - 1})')
            take_indices.append(idx)

    if not torch.jit.is_scripting() and as_set:
        return set(take_indices), max(take_indices)

    return take_indices, max(take_indices)


def _out_indices_as_tuple(x: Union[int, Tuple[int, ...]]) -> Tuple[int, ...]:
    if isinstance(x, int):
        # if indices is an int, take last N features
        return tuple(range(-x, 0))
    return tuple(x)

===== src/open_clip/loss.py =====
from typing import Optional

import torch
import torch.nn as nn
from torch.nn import functional as F

try:
    import torch.distributed.nn
    from torch import distributed as dist

    has_distributed = True
except ImportError:
    has_distributed = False

try:
    import horovod.torch as hvd
except ImportError:
    hvd = None


def gather_features(
        image_features,
        text_features,
        local_loss=False,
        gather_with_grad=False,
        rank=0,
        world_size=1,
        use_horovod=False
):
    assert has_distributed, 'torch.distributed did not import correctly, please use a PyTorch version with support.'
    if use_horovod:
        assert hvd is not None, 'Please install horovod'
        if gather_with_grad:
            all_image_features = hvd.allgather(image_features)
            all_text_features = hvd.allgather(text_features)
        else:
            with torch.no_grad():
                all_image_features = hvd.allgather(image_features)
                all_text_features = hvd.allgather(text_features)
            if not local_loss:
                # ensure grads for local rank when all_* features don't have a gradient
                gathered_image_features = list(all_image_features.chunk(world_size, dim=0))
                gathered_text_features = list(all_text_features.chunk(world_size, dim=0))
                gathered_image_features[rank] = image_features
                gathered_text_features[rank] = text_features
                all_image_features = torch.cat(gathered_image_features, dim=0)
                all_text_features = torch.cat(gathered_text_features, dim=0)
    else:
        # We gather tensors from all gpus
        if gather_with_grad:
            all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features), dim=0)
            all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
        else:
            gathered_image_features = [torch.zeros_like(image_features) for _ in range(world_size)]
            gathered_text_features = [torch.zeros_like(text_features) for _ in range(world_size)]
            dist.all_gather(gathered_image_features, image_features)
            dist.all_gather(gathered_text_features, text_features)
            if not local_loss:
                # ensure grads for local rank when all_* features don't have a gradient
                gathered_image_features[rank] = image_features
                gathered_text_features[rank] = text_features
            all_image_features = torch.cat(gathered_image_features, dim=0)
            all_text_features = torch.cat(gathered_text_features, dim=0)

    return all_image_features, all_text_features


class ClipLoss(nn.Module):

    def __init__(
            self,
            local_loss=False,
            gather_with_grad=False,
            cache_labels=False,
            rank=0,
            world_size=1,
            use_horovod=False,
    ):
        super().__init__()
        self.local_loss = local_loss
        self.gather_with_grad = gather_with_grad
        self.cache_labels = cache_labels
        self.rank = rank
        self.world_size = world_size
        self.use_horovod = use_horovod

        # cache state
        self.prev_num_logits = 0
        self.labels = {}

    def get_ground_truth(self, device, num_logits) -> torch.Tensor:
        # calculated ground-truth and cache if enabled
        if self.prev_num_logits != num_logits or device not in self.labels:
            labels = torch.arange(num_logits, device=device, dtype=torch.long)
            if self.world_size > 1 and self.local_loss:
                labels = labels + num_logits * self.rank
            if self.cache_labels:
                self.labels[device] = labels
                self.prev_num_logits = num_logits
        else:
            labels = self.labels[device]
        return labels

    def get_logits(self, image_features, text_features, logit_scale, logit_bias=None):
        if self.world_size > 1:
            all_image_features, all_text_features = gather_features(
                image_features,
                text_features,
                local_loss=self.local_loss,
                gather_with_grad=self.gather_with_grad,
                rank=self.rank,
                world_size=self.world_size,
                use_horovod=self.use_horovod,
            )

            if self.local_loss:
                logits_per_image = logit_scale * image_features @ all_text_features.T
                logits_per_text = logit_scale * text_features @ all_image_features.T
            else:
                logits_per_image = logit_scale * all_image_features @ all_text_features.T
                logits_per_text = logits_per_image.T
        else:
            logits_per_image = logit_scale * image_features @ text_features.T
            logits_per_text = logit_scale * text_features @ image_features.T

        if logit_bias is not None:
            logits_per_image += logit_bias
            logits_per_text += logit_bias

        return logits_per_image, logits_per_text

    def forward(
            self,
            image_features,
            text_features,
            logit_scale,
            logit_bias=None,
            output_dict=False,
    ):
        device = image_features.device
        logits_per_image, logits_per_text = self.get_logits(
            image_features,
            text_features,
            logit_scale,
            logit_bias=logit_bias,
        )

        labels = self.get_ground_truth(device, logits_per_image.shape[0])

        total_loss = (
            F.cross_entropy(logits_per_image, labels) +
            F.cross_entropy(logits_per_text, labels)
        ) / 2

        return {"contrastive_loss": total_loss} if output_dict else total_loss


class CoCaLoss(ClipLoss):
    def __init__(
            self,
            caption_loss_weight,
            clip_loss_weight,
            pad_id=0,  # pad_token for open_clip custom tokenizer
            local_loss=False,
            gather_with_grad=False,
            cache_labels=False,
            rank=0,
            world_size=1,
            use_horovod=False,
    ):
        super().__init__(
            local_loss=local_loss,
            gather_with_grad=gather_with_grad,
            cache_labels=cache_labels,
            rank=rank,
            world_size=world_size,
            use_horovod=use_horovod
        )

        self.clip_loss_weight = clip_loss_weight
        self.caption_loss_weight = caption_loss_weight
        self.caption_loss = nn.CrossEntropyLoss(ignore_index=pad_id)

    def forward(self, image_features, text_features, logits, labels, logit_scale, output_dict=False):
        if self.clip_loss_weight:
            clip_loss = super().forward(image_features, text_features, logit_scale)
            clip_loss = self.clip_loss_weight * clip_loss
        else:
            clip_loss = torch.tensor(0, device=logits.device)

        caption_loss = self.caption_loss(
            logits.permute(0, 2, 1),
            labels,
        )
        caption_loss = caption_loss * self.caption_loss_weight

        if output_dict:
            return {"contrastive_loss": clip_loss, "caption_loss": caption_loss}

        return clip_loss, caption_loss


class DistillClipLoss(ClipLoss):

    def dist_loss(self, teacher_logits, student_logits):
        return -(teacher_logits.softmax(dim=1) * student_logits.log_softmax(dim=1)).sum(dim=1).mean(dim=0)

    def forward(
            self,
            image_features,
            text_features,
            logit_scale,
            dist_image_features,
            dist_text_features,
            dist_logit_scale,
            output_dict=False,
    ):
        logits_per_image, logits_per_text = \
            self.get_logits(image_features, text_features, logit_scale)

        dist_logits_per_image, dist_logits_per_text = \
            self.get_logits(dist_image_features, dist_text_features, dist_logit_scale)

        labels = self.get_ground_truth(image_features.device, logits_per_image.shape[0])

        contrastive_loss = (
            F.cross_entropy(logits_per_image, labels) +
            F.cross_entropy(logits_per_text, labels)
        ) / 2

        distill_loss = (
            self.dist_loss(dist_logits_per_image, logits_per_image) +
            self.dist_loss(dist_logits_per_text, logits_per_text)
        ) / 2

        if output_dict:
            return {"contrastive_loss": contrastive_loss, "distill_loss": distill_loss}

        return contrastive_loss, distill_loss


def neighbour_exchange(from_rank, to_rank, tensor, group=None):
    tensor_recv = torch.zeros_like(tensor)
    send_op = torch.distributed.P2POp(
        torch.distributed.isend,
        tensor,
        to_rank,
        group=group,
    )
    recv_op = torch.distributed.P2POp(
        torch.distributed.irecv,
        tensor_recv,
        from_rank,
        group=group,
    )
    reqs = torch.distributed.batch_isend_irecv([send_op, recv_op])
    for req in reqs:
        req.wait()
    return tensor_recv


def neighbour_exchange_bidir(left_rank, right_rank, tensor_to_left, tensor_to_right, group=None):
    tensor_from_left = torch.zeros_like(tensor_to_right)
    tensor_from_right = torch.zeros_like(tensor_to_left)
    send_op_left = torch.distributed.P2POp(
        torch.distributed.isend,
        tensor_to_left,
        left_rank,
        group=group,
    )
    send_op_right = torch.distributed.P2POp(
        torch.distributed.isend,
        tensor_to_right,
        right_rank,
        group=group,
    )
    recv_op_left = torch.distributed.P2POp(
        torch.distributed.irecv,
        tensor_from_left,
        left_rank,
        group=group,
    )
    recv_op_right = torch.distributed.P2POp(
        torch.distributed.irecv,
        tensor_from_right,
        right_rank,
        group=group,
    )
    reqs = torch.distributed.batch_isend_irecv([send_op_right, send_op_left, recv_op_right, recv_op_left])
    for req in reqs:
        req.wait()
    return tensor_from_right, tensor_from_left


class NeighbourExchange(torch.autograd.Function):
    @staticmethod
    def forward(ctx, from_rank, to_rank, group, tensor):
        ctx.group = group
        ctx.from_rank = from_rank
        ctx.to_rank = to_rank
        return neighbour_exchange(from_rank, to_rank, tensor, group=group)

    @staticmethod
    def backward(ctx, grad_output):
        return (None, None, None) + (NeighbourExchange.apply(ctx.to_rank, ctx.from_rank, ctx.group, grad_output),)


def neighbour_exchange_with_grad(from_rank, to_rank, tensor, group=None):
    return NeighbourExchange.apply(from_rank, to_rank, group, tensor)


class NeighbourExchangeBidir(torch.autograd.Function):
    @staticmethod
    def forward(ctx, left_rank, right_rank, group, tensor_to_left, tensor_to_right):
        ctx.group = group
        ctx.left_rank = left_rank
        ctx.right_rank = right_rank
        return neighbour_exchange_bidir(left_rank, right_rank, tensor_to_left, tensor_to_right, group=group)

    @staticmethod
    def backward(ctx, *grad_outputs):
        return (None, None, None) + \
            NeighbourExchangeBidir.apply(ctx.right_rank, ctx.left_rank, ctx.group, *grad_outputs)


def neighbour_exchange_bidir_with_grad(left_rank, right_rank, tensor_to_left, tensor_to_right, group=None):
    return NeighbourExchangeBidir.apply(left_rank, right_rank, group, tensor_to_left, tensor_to_right)


class SigLipLoss(nn.Module):
    """ Sigmoid Loss for Language Image Pre-Training (SigLIP) - https://arxiv.org/abs/2303.15343

    @article{zhai2023sigmoid,
      title={Sigmoid loss for language image pre-training},
      author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
      journal={arXiv preprint arXiv:2303.15343},
      year={2023}
    }
    """
    def __init__(
            self,
            cache_labels: bool = False,
            rank: int = 0,
            world_size: int = 1,
            dist_impl: Optional[str] = None,
    ):
        super().__init__()
        self.cache_labels = cache_labels
        self.rank = rank
        self.world_size = world_size
        self.dist_impl = dist_impl or 'bidir'  # default to bidir exchange for now, this will likely change
        assert self.dist_impl in ('bidir', 'shift', 'reduce', 'gather')

        # cache state FIXME cache not currently used, worthwhile?
        self.prev_num_logits = 0
        self.labels = {}

    def get_ground_truth(self, device, dtype, num_logits, negative_only=False) -> torch.Tensor:
        labels = -torch.ones((num_logits, num_logits), device=device, dtype=dtype)
        if not negative_only:
            labels = 2 * torch.eye(num_logits, device=device, dtype=dtype) + labels
        return labels

    def get_logits(self, image_features, text_features, logit_scale, logit_bias=None):
        logits = logit_scale * image_features @ text_features.T
        if logit_bias is not None:
            logits += logit_bias
        return logits

    def _loss(self, image_features, text_features, logit_scale, logit_bias=None, negative_only=False):
        logits = self.get_logits(image_features, text_features, logit_scale, logit_bias)
        labels = self.get_ground_truth(
            image_features.device,
            image_features.dtype,
            image_features.shape[0],
            negative_only=negative_only,
        )
        loss = -F.logsigmoid(labels * logits).sum() / image_features.shape[0]
        return loss

    def forward(self, image_features, text_features, logit_scale, logit_bias, output_dict=False):
        loss = self._loss(image_features, text_features, logit_scale, logit_bias)

        if self.world_size > 1:
            if self.dist_impl == 'bidir':
                right_rank = (self.rank + 1) % self.world_size
                left_rank = (self.rank - 1 + self.world_size) % self.world_size
                text_features_to_right = text_features_to_left = text_features
                num_bidir, remainder = divmod(self.world_size - 1, 2)
                for i in range(num_bidir):
                    text_features_recv = neighbour_exchange_bidir_with_grad(
                        left_rank,
                        right_rank,
                        text_features_to_left,
                        text_features_to_right,
                    )
                    for f in text_features_recv:
                        loss += self._loss(
                            image_features,
                            f,
                            logit_scale,
                            logit_bias,
                            negative_only=True,
                        )
                    text_features_to_left, text_features_to_right = text_features_recv

                if remainder:
                    text_features_recv = neighbour_exchange_with_grad(
                        left_rank,
                        right_rank,
                        text_features_to_right
                    )
                    loss += self._loss(
                        image_features,
                        text_features_recv,
                        logit_scale,
                        logit_bias,
                        negative_only=True,
                    )
            elif self.dist_impl == "shift":
                right_rank = (self.rank + 1) % self.world_size
                left_rank = (self.rank - 1 + self.world_size) % self.world_size
                text_features_to_right = text_features
                for i in range(self.world_size - 1):
                    text_features_from_left = neighbour_exchange_with_grad(
                        left_rank,
                        right_rank,
                        text_features_to_right,
                    )
                    loss += self._loss(
                        image_features,
                        text_features_from_left,
                        logit_scale,
                        logit_bias,
                        negative_only=True,
                    )
                    text_features_to_right = text_features_from_left
            elif self.dist_impl == "reduce":
                for i in range(self.world_size):
                    text_from_other = torch.distributed.nn.all_reduce(
                        text_features * (self.rank == i),
                        torch.distributed.ReduceOp.SUM,
                    )
                    loss += float(i != self.rank) * self._loss(
                        image_features,
                        text_from_other,
                        logit_scale,
                        logit_bias,
                        negative_only=True,
                    )
            elif self.dist_impl == "gather":
                all_text = torch.distributed.nn.all_gather(text_features)
                for i in range(self.world_size):
                    loss += float(i != self.rank) * self._loss(
                        image_features,
                        all_text[i],
                        logit_scale,
                        logit_bias,
                        negative_only=True,
                    )
            else:
                assert False

        return {"contrastive_loss": loss} if output_dict else loss

===== src/open_clip/constants.py =====
OPENAI_DATASET_MEAN = (0.48145466, 0.4578275, 0.40821073)
OPENAI_DATASET_STD = (0.26862954, 0.26130258, 0.27577711)
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)
INCEPTION_MEAN = (0.5, 0.5, 0.5)
INCEPTION_STD = (0.5, 0.5, 0.5)

# Default name for a weights file hosted on the Huggingface Hub.
HF_WEIGHTS_NAME = "open_clip_pytorch_model.bin"  # default pytorch pkl
HF_SAFE_WEIGHTS_NAME = "open_clip_model.safetensors"  # safetensors version
HF_CONFIG_NAME = 'open_clip_config.json'

===== src/open_clip/convert.py =====
""" Conversion functions for 3rd part state-dicts and non-torch native checkpoint formats.
"""
from typing import Union

import torch
import numpy as np

from .model import CLIP, CustomTextCLIP
from .transformer import TextTransformer, Transformer


@torch.no_grad()
def load_big_vision_weights(model: CustomTextCLIP, checkpoint_path: str):
    """ Load weights from .npz checkpoints for official Google big_vision image-text models

    Currently, the SigLIP source models are supported and a CustomTextCLIP destination model
    w/ timm image encoder.
    """
    from timm.layers import resample_patch_embed, resample_abs_pos_embed

    def _n2p(w, t=True, idx=None):
        if idx is not None:
            w = w[idx]
        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:
            w = w.flatten()
        if t:
            if w.ndim == 4:
                w = w.transpose([3, 2, 0, 1])
            elif w.ndim == 3:
                w = w.transpose([2, 0, 1])
            elif w.ndim == 2:
                w = w.transpose([1, 0])
        return torch.from_numpy(w)

    w = np.load(checkpoint_path)
    interpolation = 'bilinear'
    antialias = False

    def _convert_timm_img(module, prefix):
        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])
        if embed_conv_w.shape[-2:] != module.patch_embed.proj.weight.shape[-2:]:
            embed_conv_w = resample_patch_embed(
                embed_conv_w,
                module.patch_embed.proj.weight.shape[-2:],
                interpolation=interpolation,
                antialias=antialias,
                verbose=True,
            )
        module.patch_embed.proj.weight.copy_(embed_conv_w)
        module.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))

        if module.cls_token is not None:
            module.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))

        pos_embed_w = _n2p(w[f'{prefix}pos_embedding'], t=False)
        if pos_embed_w.shape != module.pos_embed.shape:
            assert False, f'{pos_embed_w.shape}, {module.pos_embed.shape}'
            num_prefix_tokens = 0 if getattr(module, 'no_embed_class', False) else getattr(module, 'num_prefix_tokens', 1)
            pos_embed_w = resample_abs_pos_embed(  # resize pos embedding when different size from pretrained weights
                pos_embed_w,
                new_size=module.patch_embed.grid_size,
                num_prefix_tokens=num_prefix_tokens,
                interpolation=interpolation,
                antialias=antialias,
                verbose=True,
            )
        module.pos_embed.copy_(pos_embed_w)

        mha_sub, b_sub, ln1_sub = (0, 0, 1)
        for i, block in enumerate(module.blocks.children()):
            if f'{prefix}Transformer/encoderblock/LayerNorm_0/scale' in w:
                block_prefix = f'{prefix}Transformer/encoderblock/'
                idx = i
            else:
                block_prefix = f'{prefix}Transformer/encoderblock_{i}/'
                idx = None
            mha_prefix = block_prefix + f'MultiHeadDotProductAttention_{mha_sub}/'
            block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale'], idx=idx))
            block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias'], idx=idx))
            block.attn.qkv.weight.copy_(torch.cat([
                _n2p(w[f'{mha_prefix}{n}/kernel'], t=False, idx=idx).flatten(1).T for n in ('query', 'key', 'value')]))
            block.attn.qkv.bias.copy_(torch.cat([
                _n2p(w[f'{mha_prefix}{n}/bias'], t=False, idx=idx).reshape(-1) for n in ('query', 'key', 'value')]))
            block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel'], idx=idx).flatten(1))
            block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias'], idx=idx))
            block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_{ln1_sub}/scale'], idx=idx))
            block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_{ln1_sub}/bias'], idx=idx))
            for r in range(2):
                getattr(block.mlp, f'fc{r + 1}').weight.copy_(
                    _n2p(w[f'{block_prefix}MlpBlock_{b_sub}/Dense_{r}/kernel'], idx=idx))
                getattr(block.mlp, f'fc{r + 1}').bias.copy_(
                    _n2p(w[f'{block_prefix}MlpBlock_{b_sub}/Dense_{r}/bias'], idx=idx))

        module.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))
        module.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))

        if module.attn_pool is not None:
            block_prefix = f'{prefix}MAPHead_0/'
            mha_prefix = block_prefix + f'MultiHeadDotProductAttention_0/'
            module.attn_pool.latent.copy_(_n2p(w[f'{block_prefix}probe'], t=False))
            module.attn_pool.q.weight.copy_(_n2p(w[f'{mha_prefix}query/kernel'], t=False).flatten(1).T)
            module.attn_pool.q.bias.copy_(_n2p(w[f'{mha_prefix}query/bias'], t=False).reshape(-1))
            module.attn_pool.kv.weight.copy_(torch.cat([
                _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('key', 'value')]))
            module.attn_pool.kv.bias.copy_(torch.cat([
                _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('key', 'value')]))
            module.attn_pool.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))
            module.attn_pool.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))
            module.attn_pool.norm.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))
            module.attn_pool.norm.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))
            for r in range(2):
                getattr(module.attn_pool.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_{r}/kernel']))
                getattr(module.attn_pool.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_{r}/bias']))

    def _convert_openclip_transformer(module: Transformer, prefix):
        for i, block in enumerate(module.resblocks.children()):
            if f'{prefix}encoderblock/LayerNorm_0/scale' in w:
                block_prefix = f'{prefix}encoderblock/'
                idx = i
            else:
                block_prefix = f'{prefix}encoderblock_{i}/'
                idx = None
            mha_prefix = block_prefix + f'MultiHeadDotProductAttention_0/'
            block.ln_1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale'], idx=idx))
            block.ln_1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias'], idx=idx))
            block.attn.in_proj_weight.copy_(torch.cat([
                _n2p(w[f'{mha_prefix}{n}/kernel'], t=False, idx=idx).flatten(1).T for n in ('query', 'key', 'value')]))
            block.attn.in_proj_bias.copy_(torch.cat([
                _n2p(w[f'{mha_prefix}{n}/bias'], t=False, idx=idx).reshape(-1) for n in ('query', 'key', 'value')]))
            block.attn.out_proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel'], idx=idx).flatten(1))
            block.attn.out_proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias'], idx=idx))
            block.ln_2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_1/scale'], idx=idx))
            block.ln_2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_1/bias'], idx=idx))
            block.mlp.c_fc.weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_0/kernel'], idx=idx))
            block.mlp.c_fc.bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_0/bias'], idx=idx))
            block.mlp.c_proj.weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_1/kernel'], idx=idx))
            block.mlp.c_proj.bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_1/bias'], idx=idx))

    def _convert_openclip_txt(module: TextTransformer, prefix):
        module.token_embedding.weight.copy_(_n2p(w[f'{prefix}Embed_0/embedding'], t=False))
        pos_embed_w = _n2p(w[f'{prefix}pos_embedding'], t=False).squeeze(0)
        module.positional_embedding.copy_(pos_embed_w)
        _convert_openclip_transformer(module.transformer, prefix=prefix + 'Encoder_0/')
        module.ln_final.weight.copy_(_n2p(w[f'{prefix}Encoder_0/encoder_norm/scale']))
        module.ln_final.bias.copy_(_n2p(w[f'{prefix}Encoder_0/encoder_norm/bias']))
        if module.text_projection is not None:
            module.text_projection.weight.copy_(_n2p(w[f'{prefix}head/kernel']))
            module.text_projection.bias.copy_(_n2p(w[f'{prefix}head/bias']))

    root_prefix = 'params/' if 'params/b' in w else ''
    _convert_timm_img(model.visual.trunk, f'{root_prefix}img/')
    _convert_openclip_txt(model.text, f'{root_prefix}txt/')
    model.logit_bias.copy_(_n2p(w[f'{root_prefix}b'])[0])
    model.logit_scale.copy_(_n2p(w[f'{root_prefix}t'])[0])


@torch.no_grad()
def convert_mobile_clip_state_dict(model: CustomTextCLIP, state_dict, fastvit = True):

    def _convert_timm_img(state_dict):
        if fastvit:
            from timm.models.fastvit import checkpoint_filter_fn
        else:
            from timm.models.vision_transformer_hybrid import checkpoint_filter_fn
        timm_state_dict = checkpoint_filter_fn(state_dict, model.visual.trunk)
        timm_state_dict = {'visual.trunk.' + k: v for k, v in timm_state_dict.items()}
        return timm_state_dict

    def _convert_openclip_txt(state_dict, prefix='text_encoder.'):
        text_dict = {}
        for k, v in state_dict.items():
            if not k.startswith(prefix):
                continue
            k = k.replace(prefix, '')
            k = k.replace('projection_layer', 'text_projection')
            k = k.replace('embedding_layer', 'token_embedding')
            if k.startswith('positional_embedding.pos_embed.pos_embed'):
                k = k.replace('positional_embedding.pos_embed.pos_embed', 'positional_embedding')
                v = v.squeeze()
            k = k.replace('final_layer_norm', 'ln_final')
            k = k.replace('pre_norm_mha.0', 'ln_1')
            k = k.replace('pre_norm_mha.1', 'attn')
            k = k.replace('pre_norm_ffn.0', 'ln_2')
            k = k.replace('pre_norm_ffn.1', 'mlp.c_fc')
            k = k.replace('pre_norm_ffn.4', 'mlp.c_proj')
            k = k.replace('qkv_proj.weight', 'in_proj_weight')
            k = k.replace('qkv_proj.bias', 'in_proj_bias')
            k = k.replace('transformer.', 'transformer.resblocks.')
            text_dict['text.' + k] = v
        return text_dict

    image_dict = _convert_timm_img(state_dict)
    text_dict = _convert_openclip_txt(state_dict)
    out_dict = {**image_dict, **text_dict}
    out_dict['logit_scale'] = state_dict['logit_scale']
    return out_dict


def convert_state_dict(model: Union[CustomTextCLIP, CLIP], state_dict):
    if 'image_encoder.model.patch_embed.0.rbr_conv.0.conv.weight' in state_dict:
        # Apple MobileCLIP s1 & s2 state_dicts (s0 and b not currently supported)
        state_dict = convert_mobile_clip_state_dict(model, state_dict)
    if 'image_encoder.model.patch_emb.0.block.conv.weight' in state_dict:
        # convert b model
        state_dict = convert_mobile_clip_state_dict(model, state_dict, fastvit=False)
    return state_dict

===== src/open_clip/openai.py =====
""" OpenAI pretrained model functions

Adapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.
"""

import os
import warnings
from typing import List, Optional, Union

import torch

from .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD
from .model import build_model_from_openai_state_dict, convert_weights_to_lp, get_cast_dtype
from .pretrained import get_pretrained_url, list_pretrained_models_by_tag, download_pretrained_from_url

__all__ = ["list_openai_models", "load_openai_model"]


def list_openai_models() -> List[str]:
    """Returns the names of available CLIP models"""
    return list_pretrained_models_by_tag('openai')


def load_openai_model(
        name: str,
        precision: Optional[str] = None,
        device: Optional[Union[str, torch.device]] = None,
        cache_dir: Optional[str] = None,
):
    """Load a CLIP model

    Parameters
    ----------
    name : str
        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict
    precision: str
        Model precision, if None defaults to 'fp32' if device == 'cpu' else 'fp16'.
    device : Union[str, torch.device]
        The device to put the loaded model
    cache_dir : Optional[str]
        The directory to cache the downloaded model weights

    Returns
    -------
    model : torch.nn.Module
        The CLIP model
    preprocess : Callable[[PIL.Image], torch.Tensor]
        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input
    """
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    if precision is None:
        precision = 'fp32' if device == 'cpu' else 'fp16'

    if get_pretrained_url(name, 'openai'):
        model_path = download_pretrained_from_url(get_pretrained_url(name, 'openai'), cache_dir=cache_dir)
    elif os.path.isfile(name):
        model_path = name
    else:
        raise RuntimeError(f"Model {name} not found; available models = {list_openai_models()}")

    try:
        # loading JIT archive
        model = torch.jit.load(model_path, map_location="cpu").eval()
        state_dict = None
    except RuntimeError:
        # loading saved state dict
        state_dict = torch.load(model_path, map_location="cpu")

    # Build a non-jit model from the OpenAI jitted model state dict
    cast_dtype = get_cast_dtype(precision)
    try:
        model = build_model_from_openai_state_dict(state_dict or model.state_dict(), cast_dtype=cast_dtype)
    except KeyError:
        sd = {k[7:]: v for k, v in state_dict["state_dict"].items()}
        model = build_model_from_openai_state_dict(sd, cast_dtype=cast_dtype)

    # model from OpenAI state dict is in manually cast fp16 mode, must be converted for AMP/fp32/bf16 use
    model = model.to(device)
    # FIXME support pure fp16/bf16 precision modes
    if precision != 'fp16':
        model.float()
        if precision == 'bf16':
            # for bf16, convert back to low-precision
            convert_weights_to_lp(model, dtype=torch.bfloat16)

    # add mean / std attributes for consistency with OpenCLIP models
    model.visual.image_mean = OPENAI_DATASET_MEAN
    model.visual.image_std = OPENAI_DATASET_STD
    return model

===== src/open_clip/version.py =====
__version__ = '3.1.0'

===== src/open_clip/zero_shot_metadata.py =====

OPENAI_IMAGENET_TEMPLATES = (
    lambda c: f'a bad photo of a {c}.',
    lambda c: f'a photo of many {c}.',
    lambda c: f'a sculpture of a {c}.',
    lambda c: f'a photo of the hard to see {c}.',
    lambda c: f'a low resolution photo of the {c}.',
    lambda c: f'a rendering of a {c}.',
    lambda c: f'graffiti of a {c}.',
    lambda c: f'a bad photo of the {c}.',
    lambda c: f'a cropped photo of the {c}.',
    lambda c: f'a tattoo of a {c}.',
    lambda c: f'the embroidered {c}.',
    lambda c: f'a photo of a hard to see {c}.',
    lambda c: f'a bright photo of a {c}.',
    lambda c: f'a photo of a clean {c}.',
    lambda c: f'a photo of a dirty {c}.',
    lambda c: f'a dark photo of the {c}.',
    lambda c: f'a drawing of a {c}.',
    lambda c: f'a photo of my {c}.',
    lambda c: f'the plastic {c}.',
    lambda c: f'a photo of the cool {c}.',
    lambda c: f'a close-up photo of a {c}.',
    lambda c: f'a black and white photo of the {c}.',
    lambda c: f'a painting of the {c}.',
    lambda c: f'a painting of a {c}.',
    lambda c: f'a pixelated photo of the {c}.',
    lambda c: f'a sculpture of the {c}.',
    lambda c: f'a bright photo of the {c}.',
    lambda c: f'a cropped photo of a {c}.',
    lambda c: f'a plastic {c}.',
    lambda c: f'a photo of the dirty {c}.',
    lambda c: f'a jpeg corrupted photo of a {c}.',
    lambda c: f'a blurry photo of the {c}.',
    lambda c: f'a photo of the {c}.',
    lambda c: f'a good photo of the {c}.',
    lambda c: f'a rendering of the {c}.',
    lambda c: f'a {c} in a video game.',
    lambda c: f'a photo of one {c}.',
    lambda c: f'a doodle of a {c}.',
    lambda c: f'a close-up photo of the {c}.',
    lambda c: f'a photo of a {c}.',
    lambda c: f'the origami {c}.',
    lambda c: f'the {c} in a video game.',
    lambda c: f'a sketch of a {c}.',
    lambda c: f'a doodle of the {c}.',
    lambda c: f'a origami {c}.',
    lambda c: f'a low resolution photo of a {c}.',
    lambda c: f'the toy {c}.',
    lambda c: f'a rendition of the {c}.',
    lambda c: f'a photo of the clean {c}.',
    lambda c: f'a photo of a large {c}.',
    lambda c: f'a rendition of a {c}.',
    lambda c: f'a photo of a nice {c}.',
    lambda c: f'a photo of a weird {c}.',
    lambda c: f'a blurry photo of a {c}.',
    lambda c: f'a cartoon {c}.',
    lambda c: f'art of a {c}.',
    lambda c: f'a sketch of the {c}.',
    lambda c: f'a embroidered {c}.',
    lambda c: f'a pixelated photo of a {c}.',
    lambda c: f'itap of the {c}.',
    lambda c: f'a jpeg corrupted photo of the {c}.',
    lambda c: f'a good photo of a {c}.',
    lambda c: f'a plushie {c}.',
    lambda c: f'a photo of the nice {c}.',
    lambda c: f'a photo of the small {c}.',
    lambda c: f'a photo of the weird {c}.',
    lambda c: f'the cartoon {c}.',
    lambda c: f'art of the {c}.',
    lambda c: f'a drawing of the {c}.',
    lambda c: f'a photo of the large {c}.',
    lambda c: f'a black and white photo of a {c}.',
    lambda c: f'the plushie {c}.',
    lambda c: f'a dark photo of a {c}.',
    lambda c: f'itap of a {c}.',
    lambda c: f'graffiti of the {c}.',
    lambda c: f'a toy {c}.',
    lambda c: f'itap of my {c}.',
    lambda c: f'a photo of a cool {c}.',
    lambda c: f'a photo of a small {c}.',
    lambda c: f'a tattoo of the {c}.',
)


# a much smaller subset of above prompts
# from https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb
SIMPLE_IMAGENET_TEMPLATES = (
    lambda c: f'itap of a {c}.',
    lambda c: f'a bad photo of the {c}.',
    lambda c: f'a origami {c}.',
    lambda c: f'a photo of the large {c}.',
    lambda c: f'a {c} in a video game.',
    lambda c: f'art of the {c}.',
    lambda c: f'a photo of the small {c}.',
)


IMAGENET_CLASSNAMES = (
    "tench", "goldfish", "great white shark", "tiger shark", "hammerhead shark", "electric ray",
    "stingray", "rooster", "hen", "ostrich", "brambling", "goldfinch", "house finch", "junco",
    "indigo bunting", "American robin", "bulbul", "jay", "magpie", "chickadee", "American dipper",
    "kite (bird of prey)", "bald eagle", "vulture", "great grey owl", "fire salamander",
    "smooth newt", "newt", "spotted salamander", "axolotl", "American bullfrog", "tree frog",
    "tailed frog", "loggerhead sea turtle", "leatherback sea turtle", "mud turtle", "terrapin",
    "box turtle", "banded gecko", "green iguana", "Carolina anole",
    "desert grassland whiptail lizard", "agama", "frilled-necked lizard", "alligator lizard",
    "Gila monster", "European green lizard", "chameleon", "Komodo dragon", "Nile crocodile",
    "American alligator", "triceratops", "worm snake", "ring-necked snake",
    "eastern hog-nosed snake", "smooth green snake", "kingsnake", "garter snake", "water snake",
    "vine snake", "night snake", "boa constrictor", "African rock python", "Indian cobra",
    "green mamba", "sea snake", "Saharan horned viper", "eastern diamondback rattlesnake",
    "sidewinder rattlesnake", "trilobite", "harvestman", "scorpion", "yellow garden spider",
    "barn spider", "European garden spider", "southern black widow", "tarantula", "wolf spider",
    "tick", "centipede", "black grouse", "ptarmigan", "ruffed grouse", "prairie grouse", "peafowl",
    "quail", "partridge", "african grey parrot", "macaw", "sulphur-crested cockatoo", "lorikeet",
    "coucal", "bee eater", "hornbill", "hummingbird", "jacamar", "toucan", "duck",
    "red-breasted merganser", "goose", "black swan", "tusker", "echidna", "platypus", "wallaby",
    "koala", "wombat", "jellyfish", "sea anemone", "brain coral", "flatworm", "nematode", "conch",
    "snail", "slug", "sea slug", "chiton", "chambered nautilus", "Dungeness crab", "rock crab",
    "fiddler crab", "red king crab", "American lobster", "spiny lobster", "crayfish", "hermit crab",
    "isopod", "white stork", "black stork", "spoonbill", "flamingo", "little blue heron",
    "great egret", "bittern bird", "crane bird", "limpkin", "common gallinule", "American coot",
    "bustard", "ruddy turnstone", "dunlin", "common redshank", "dowitcher", "oystercatcher",
    "pelican", "king penguin", "albatross", "grey whale", "killer whale", "dugong", "sea lion",
    "Chihuahua", "Japanese Chin", "Maltese", "Pekingese", "Shih Tzu", "King Charles Spaniel",
    "Papillon", "toy terrier", "Rhodesian Ridgeback", "Afghan Hound", "Basset Hound", "Beagle",
    "Bloodhound", "Bluetick Coonhound", "Black and Tan Coonhound", "Treeing Walker Coonhound",
    "English foxhound", "Redbone Coonhound", "borzoi", "Irish Wolfhound", "Italian Greyhound",
    "Whippet", "Ibizan Hound", "Norwegian Elkhound", "Otterhound", "Saluki", "Scottish Deerhound",
    "Weimaraner", "Staffordshire Bull Terrier", "American Staffordshire Terrier",
    "Bedlington Terrier", "Border Terrier", "Kerry Blue Terrier", "Irish Terrier",
    "Norfolk Terrier", "Norwich Terrier", "Yorkshire Terrier", "Wire Fox Terrier",
    "Lakeland Terrier", "Sealyham Terrier", "Airedale Terrier", "Cairn Terrier",
    "Australian Terrier", "Dandie Dinmont Terrier", "Boston Terrier", "Miniature Schnauzer",
    "Giant Schnauzer", "Standard Schnauzer", "Scottish Terrier", "Tibetan Terrier",
    "Australian Silky Terrier", "Soft-coated Wheaten Terrier", "West Highland White Terrier",
    "Lhasa Apso", "Flat-Coated Retriever", "Curly-coated Retriever", "Golden Retriever",
    "Labrador Retriever", "Chesapeake Bay Retriever", "German Shorthaired Pointer", "Vizsla",
    "English Setter", "Irish Setter", "Gordon Setter", "Brittany dog", "Clumber Spaniel",
    "English Springer Spaniel", "Welsh Springer Spaniel", "Cocker Spaniel", "Sussex Spaniel",
    "Irish Water Spaniel", "Kuvasz", "Schipperke", "Groenendael dog", "Malinois", "Briard",
    "Australian Kelpie", "Komondor", "Old English Sheepdog", "Shetland Sheepdog", "collie",
    "Border Collie", "Bouvier des Flandres dog", "Rottweiler", "German Shepherd Dog", "Dobermann",
    "Miniature Pinscher", "Greater Swiss Mountain Dog", "Bernese Mountain Dog",
    "Appenzeller Sennenhund", "Entlebucher Sennenhund", "Boxer", "Bullmastiff", "Tibetan Mastiff",
    "French Bulldog", "Great Dane", "St. Bernard", "husky", "Alaskan Malamute", "Siberian Husky",
    "Dalmatian", "Affenpinscher", "Basenji", "pug", "Leonberger", "Newfoundland dog",
    "Great Pyrenees dog", "Samoyed", "Pomeranian", "Chow Chow", "Keeshond", "brussels griffon",
    "Pembroke Welsh Corgi", "Cardigan Welsh Corgi", "Toy Poodle", "Miniature Poodle",
    "Standard Poodle", "Mexican hairless dog (xoloitzcuintli)", "grey wolf", "Alaskan tundra wolf",
    "red wolf or maned wolf", "coyote", "dingo", "dhole", "African wild dog", "hyena", "red fox",
    "kit fox", "Arctic fox", "grey fox", "tabby cat", "tiger cat", "Persian cat", "Siamese cat",
    "Egyptian Mau", "cougar", "lynx", "leopard", "snow leopard", "jaguar", "lion", "tiger",
    "cheetah", "brown bear", "American black bear", "polar bear", "sloth bear", "mongoose",
    "meerkat", "tiger beetle", "ladybug", "ground beetle", "longhorn beetle", "leaf beetle",
    "dung beetle", "rhinoceros beetle", "weevil", "fly", "bee", "ant", "grasshopper",
    "cricket insect", "stick insect", "cockroach", "praying mantis", "cicada", "leafhopper",
    "lacewing", "dragonfly", "damselfly", "red admiral butterfly", "ringlet butterfly",
    "monarch butterfly", "small white butterfly", "sulphur butterfly", "gossamer-winged butterfly",
    "starfish", "sea urchin", "sea cucumber", "cottontail rabbit", "hare", "Angora rabbit",
    "hamster", "porcupine", "fox squirrel", "marmot", "beaver", "guinea pig", "common sorrel horse",
    "zebra", "pig", "wild boar", "warthog", "hippopotamus", "ox", "water buffalo", "bison",
    "ram (adult male sheep)", "bighorn sheep", "Alpine ibex", "hartebeest", "impala (antelope)",
    "gazelle", "arabian camel", "llama", "weasel", "mink", "European polecat",
    "black-footed ferret", "otter", "skunk", "badger", "armadillo", "three-toed sloth", "orangutan",
    "gorilla", "chimpanzee", "gibbon", "siamang", "guenon", "patas monkey", "baboon", "macaque",
    "langur", "black-and-white colobus", "proboscis monkey", "marmoset", "white-headed capuchin",
    "howler monkey", "titi monkey", "Geoffroy's spider monkey", "common squirrel monkey",
    "ring-tailed lemur", "indri", "Asian elephant", "African bush elephant", "red panda",
    "giant panda", "snoek fish", "eel", "silver salmon", "rock beauty fish", "clownfish",
    "sturgeon", "gar fish", "lionfish", "pufferfish", "abacus", "abaya", "academic gown",
    "accordion", "acoustic guitar", "aircraft carrier", "airliner", "airship", "altar", "ambulance",
    "amphibious vehicle", "analog clock", "apiary", "apron", "trash can", "assault rifle",
    "backpack", "bakery", "balance beam", "balloon", "ballpoint pen", "Band-Aid", "banjo",
    "baluster / handrail", "barbell", "barber chair", "barbershop", "barn", "barometer", "barrel",
    "wheelbarrow", "baseball", "basketball", "bassinet", "bassoon", "swimming cap", "bath towel",
    "bathtub", "station wagon", "lighthouse", "beaker", "military hat (bearskin or shako)",
    "beer bottle", "beer glass", "bell tower", "baby bib", "tandem bicycle", "bikini",
    "ring binder", "binoculars", "birdhouse", "boathouse", "bobsleigh", "bolo tie", "poke bonnet",
    "bookcase", "bookstore", "bottle cap", "hunting bow", "bow tie", "brass memorial plaque", "bra",
    "breakwater", "breastplate", "broom", "bucket", "buckle", "bulletproof vest",
    "high-speed train", "butcher shop", "taxicab", "cauldron", "candle", "cannon", "canoe",
    "can opener", "cardigan", "car mirror", "carousel", "tool kit", "cardboard box / carton",
    "car wheel", "automated teller machine", "cassette", "cassette player", "castle", "catamaran",
    "CD player", "cello", "mobile phone", "chain", "chain-link fence", "chain mail", "chainsaw",
    "storage chest", "chiffonier", "bell or wind chime", "china cabinet", "Christmas stocking",
    "church", "movie theater", "cleaver", "cliff dwelling", "cloak", "clogs", "cocktail shaker",
    "coffee mug", "coffeemaker", "spiral or coil", "combination lock", "computer keyboard",
    "candy store", "container ship", "convertible", "corkscrew", "cornet", "cowboy boot",
    "cowboy hat", "cradle", "construction crane", "crash helmet", "crate", "infant bed",
    "Crock Pot", "croquet ball", "crutch", "cuirass", "dam", "desk", "desktop computer",
    "rotary dial telephone", "diaper", "digital clock", "digital watch", "dining table",
    "dishcloth", "dishwasher", "disc brake", "dock", "dog sled", "dome", "doormat", "drilling rig",
    "drum", "drumstick", "dumbbell", "Dutch oven", "electric fan", "electric guitar",
    "electric locomotive", "entertainment center", "envelope", "espresso machine", "face powder",
    "feather boa", "filing cabinet", "fireboat", "fire truck", "fire screen", "flagpole", "flute",
    "folding chair", "football helmet", "forklift", "fountain", "fountain pen", "four-poster bed",
    "freight car", "French horn", "frying pan", "fur coat", "garbage truck",
    "gas mask or respirator", "gas pump", "goblet", "go-kart", "golf ball", "golf cart", "gondola",
    "gong", "gown", "grand piano", "greenhouse", "radiator grille", "grocery store", "guillotine",
    "hair clip", "hair spray", "half-track", "hammer", "hamper", "hair dryer", "hand-held computer",
    "handkerchief", "hard disk drive", "harmonica", "harp", "combine harvester", "hatchet",
    "holster", "home theater", "honeycomb", "hook", "hoop skirt", "gymnastic horizontal bar",
    "horse-drawn vehicle", "hourglass", "iPod", "clothes iron", "carved pumpkin", "jeans", "jeep",
    "T-shirt", "jigsaw puzzle", "rickshaw", "joystick", "kimono", "knee pad", "knot", "lab coat",
    "ladle", "lampshade", "laptop computer", "lawn mower", "lens cap", "letter opener", "library",
    "lifeboat", "lighter", "limousine", "ocean liner", "lipstick", "slip-on shoe", "lotion",
    "music speaker", "loupe magnifying glass", "sawmill", "magnetic compass", "messenger bag",
    "mailbox", "tights", "one-piece bathing suit", "manhole cover", "maraca", "marimba", "mask",
    "matchstick", "maypole", "maze", "measuring cup", "medicine cabinet", "megalith", "microphone",
    "microwave oven", "military uniform", "milk can", "minibus", "miniskirt", "minivan", "missile",
    "mitten", "mixing bowl", "mobile home", "ford model t", "modem", "monastery", "monitor",
    "moped", "mortar and pestle", "graduation cap", "mosque", "mosquito net", "vespa",
    "mountain bike", "tent", "computer mouse", "mousetrap", "moving van", "muzzle", "metal nail",
    "neck brace", "necklace", "baby pacifier", "notebook computer", "obelisk", "oboe", "ocarina",
    "odometer", "oil filter", "pipe organ", "oscilloscope", "overskirt", "bullock cart",
    "oxygen mask", "product packet / packaging", "paddle", "paddle wheel", "padlock", "paintbrush",
    "pajamas", "palace", "pan flute", "paper towel", "parachute", "parallel bars", "park bench",
    "parking meter", "railroad car", "patio", "payphone", "pedestal", "pencil case",
    "pencil sharpener", "perfume", "Petri dish", "photocopier", "plectrum", "Pickelhaube",
    "picket fence", "pickup truck", "pier", "piggy bank", "pill bottle", "pillow", "ping-pong ball",
    "pinwheel", "pirate ship", "drink pitcher", "block plane", "planetarium", "plastic bag",
    "plate rack", "farm plow", "plunger", "Polaroid camera", "pole", "police van", "poncho",
    "pool table", "soda bottle", "plant pot", "potter's wheel", "power drill", "prayer rug",
    "printer", "prison", "missile", "projector", "hockey puck", "punching bag", "purse", "quill",
    "quilt", "race car", "racket", "radiator", "radio", "radio telescope", "rain barrel",
    "recreational vehicle", "fishing casting reel", "reflex camera", "refrigerator",
    "remote control", "restaurant", "revolver", "rifle", "rocking chair", "rotisserie", "eraser",
    "rugby ball", "ruler measuring stick", "sneaker", "safe", "safety pin", "salt shaker", "sandal",
    "sarong", "saxophone", "scabbard", "weighing scale", "school bus", "schooner", "scoreboard",
    "CRT monitor", "screw", "screwdriver", "seat belt", "sewing machine", "shield", "shoe store",
    "shoji screen / room divider", "shopping basket", "shopping cart", "shovel", "shower cap",
    "shower curtain", "ski", "balaclava ski mask", "sleeping bag", "slide rule", "sliding door",
    "slot machine", "snorkel", "snowmobile", "snowplow", "soap dispenser", "soccer ball", "sock",
    "solar thermal collector", "sombrero", "soup bowl", "keyboard space bar", "space heater",
    "space shuttle", "spatula", "motorboat", "spider web", "spindle", "sports car", "spotlight",
    "stage", "steam locomotive", "through arch bridge", "steel drum", "stethoscope", "scarf",
    "stone wall", "stopwatch", "stove", "strainer", "tram", "stretcher", "couch", "stupa",
    "submarine", "suit", "sundial", "sunglasses", "sunglasses", "sunscreen", "suspension bridge",
    "mop", "sweatshirt", "swim trunks / shorts", "swing", "electrical switch", "syringe",
    "table lamp", "tank", "tape player", "teapot", "teddy bear", "television", "tennis ball",
    "thatched roof", "front curtain", "thimble", "threshing machine", "throne", "tile roof",
    "toaster", "tobacco shop", "toilet seat", "torch", "totem pole", "tow truck", "toy store",
    "tractor", "semi-trailer truck", "tray", "trench coat", "tricycle", "trimaran", "tripod",
    "triumphal arch", "trolleybus", "trombone", "hot tub", "turnstile", "typewriter keyboard",
    "umbrella", "unicycle", "upright piano", "vacuum cleaner", "vase", "vaulted or arched ceiling",
    "velvet fabric", "vending machine", "vestment", "viaduct", "violin", "volleyball",
    "waffle iron", "wall clock", "wallet", "wardrobe", "military aircraft", "sink",
    "washing machine", "water bottle", "water jug", "water tower", "whiskey jug", "whistle",
    "hair wig", "window screen", "window shade", "Windsor tie", "wine bottle", "airplane wing",
    "wok", "wooden spoon", "wool", "split-rail fence", "shipwreck", "sailboat", "yurt", "website",
    "comic book", "crossword", "traffic or street sign", "traffic light", "dust jacket", "menu",
    "plate", "guacamole", "consomme", "hot pot", "trifle", "ice cream", "popsicle", "baguette",
    "bagel", "pretzel", "cheeseburger", "hot dog", "mashed potatoes", "cabbage", "broccoli",
    "cauliflower", "zucchini", "spaghetti squash", "acorn squash", "butternut squash", "cucumber",
    "artichoke", "bell pepper", "cardoon", "mushroom", "Granny Smith apple", "strawberry", "orange",
    "lemon", "fig", "pineapple", "banana", "jackfruit", "cherimoya (custard apple)", "pomegranate",
    "hay", "carbonara", "chocolate syrup", "dough", "meatloaf", "pizza", "pot pie", "burrito",
    "red wine", "espresso", "tea cup", "eggnog", "mountain", "bubble", "cliff", "coral reef",
    "geyser", "lakeshore", "promontory", "sandbar", "beach", "valley", "volcano", "baseball player",
    "bridegroom", "scuba diver", "rapeseed", "daisy", "yellow lady's slipper", "corn", "acorn",
    "rose hip", "horse chestnut seed", "coral fungus", "agaric", "gyromitra", "stinkhorn mushroom",
    "earth star fungus", "hen of the woods mushroom", "bolete", "corn cob", "toilet paper"
)


===== src/open_clip/transformer.py =====
from collections import OrderedDict
import math
from typing import Callable, Dict, List, Optional, Sequence, Tuple, Type, Union

import torch
from torch import nn
from torch.nn import functional as F
from torch.utils.checkpoint import checkpoint

from .utils import to_2tuple, feature_take_indices
from .pos_embed import get_2d_sincos_pos_embed


class LayerNormFp32(nn.LayerNorm):
    """Subclass torch's LayerNorm to handle fp16 (by casting to float32 and back)."""

    def forward(self, x: torch.Tensor):
        orig_type = x.dtype
        x = F.layer_norm(x.to(torch.float32), self.normalized_shape, self.weight, self.bias, self.eps)
        return x.to(orig_type)


class LayerNorm(nn.LayerNorm):
    """Subclass torch's LayerNorm (with cast back to input dtype)."""

    def forward(self, x: torch.Tensor):
        orig_type = x.dtype
        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        return x.to(orig_type)


class QuickGELU(nn.Module):
    # NOTE This is slower than nn.GELU or nn.SiLU and uses more GPU memory
    def forward(self, x: torch.Tensor):
        return x * torch.sigmoid(1.702 * x)


class LayerScale(nn.Module):
    def __init__(self, dim, init_values=1e-5, inplace=False):
        super().__init__()
        self.inplace = inplace
        self.gamma = nn.Parameter(init_values * torch.ones(dim))

    def forward(self, x):
        return x.mul_(self.gamma) if self.inplace else x * self.gamma


class PatchDropout(nn.Module):
    """
    https://arxiv.org/abs/2212.00794
    """

    def __init__(
            self,
            prob: float = 0.5,
            exclude_first_token: bool = True
    ):
        super().__init__()
        assert 0 <= prob < 1.
        self.prob = prob
        self.exclude_first_token = exclude_first_token  # exclude CLS token

    def forward(self, x):
        if not self.training or self.prob == 0.:
            return x

        if self.exclude_first_token:
            cls_tokens, x = x[:, :1], x[:, 1:]
        else:
            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])

        batch = x.size()[0]
        num_tokens = x.size()[1]

        batch_indices = torch.arange(batch)
        batch_indices = batch_indices[..., None]

        keep_prob = 1 - self.prob
        num_patches_keep = max(1, int(num_tokens * keep_prob))

        rand = torch.randn(batch, num_tokens)
        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices

        x = x[batch_indices, patch_indices_keep]

        if self.exclude_first_token:
            x = torch.cat((cls_tokens, x), dim=1)

        return x


class Attention(nn.Module):
    def __init__(
            self,
            dim: int,
            num_heads: int = 8,
            qkv_bias: bool = True,
            qk_norm: bool = False,
            scaled_cosine: bool = False,
            scale_heads: bool = False,
            inner_norm: bool = False,
            logit_scale_max: float = math.log(1. / 0.01),
            norm_layer: Type[nn.Module] = LayerNormFp32,
            attn_drop: float = 0.,
            proj_drop: float = 0.
    ):
        super().__init__()
        assert not (scaled_cosine and qk_norm), "Cannot activate both scaled cosine and QK normalization"
        self.scaled_cosine = scaled_cosine
        self.scale_heads = scale_heads
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.logit_scale_max = logit_scale_max
        self.use_fsdpa = hasattr(nn.functional, 'scaled_dot_product_attention')

        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original
        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)
        if qkv_bias:
            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))
        else:
            self.in_proj_bias = None

        # QK normalization (with LN) from https://arxiv.org/abs/2106.04560 and related to other QK Norm ideas
        if qk_norm:
            self.ln_q = norm_layer(self.head_dim)
            self.ln_k = norm_layer(self.head_dim)
        else:
            self.ln_q = nn.Identity()
            self.ln_k = nn.Identity()

        # Scaled cosine attention (from Swin Transformer V2, https://arxiv.org/abs/2111.09883)
        if self.scaled_cosine:
            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))
        else:
            self.logit_scale = None

        self.attn_drop = nn.Dropout(attn_drop)

        # Per-head attention logit scaling (from NormFormer, https://arxiv.org/abs/2110.09456)
        if self.scale_heads:
            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))
        else:
            self.head_scale = None

        # Normalization of attention logits, before final projection.
        # Origin likely Sub-LN in (Foundation Transformers, https://arxiv.org/abs/2210.06423)
        if inner_norm:
            self.ln_inner = norm_layer(dim)
        else:
            self.ln_inner = nn.Identity()

        self.out_proj = nn.Linear(dim, dim)
        self.out_drop = nn.Dropout(proj_drop)

    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):
        N, L, C = x.shape
        q, k, v = F.linear(x, self.in_proj_weight, self.in_proj_bias).chunk(3, dim=-1)
        q = q.reshape(N, L, self.num_heads, -1).transpose(1, 2)
        k = k.reshape(N, L, self.num_heads, -1).transpose(1, 2)
        v = v.reshape(N, L, self.num_heads, -1).transpose(1, 2)

        if attn_mask is not None:
            if attn_mask.ndim == 3:
                # this module works with (L, L), or (N, num_heads, L, L) masks
                attn_mask = attn_mask.reshape(N, self.num_heads, L, L)
            if attn_mask.dtype == torch.bool:
                new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)
                new_attn_mask.masked_fill_(attn_mask, float("-inf"))
                attn_mask = new_attn_mask
            else:
                attn_mask = attn_mask.to(dtype=q.dtype)

        if self.logit_scale is not None:
            attn = torch.bmm(
                F.normalize(q, dim=-1),
                F.normalize(k, dim=-1).transpose(-1, -2)
            )
            logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()
            attn = attn * logit_scale
            if attn_mask is not None:
                attn = attn + attn_mask
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = torch.bmm(attn, v)
        else:
            q = self.ln_q(q)
            k = self.ln_k(k)
            if self.use_fsdpa:
                x = F.scaled_dot_product_attention(
                    q, k, v,
                    attn_mask=attn_mask,
                    dropout_p=self.attn_drop.p if self.training else 0.,
                )
            else:
                q = q * self.scale
                attn = torch.bmm(q, k.transpose(-1, -2))
                if attn_mask is not None:
                    attn += attn_mask
                attn = attn.softmax(dim=-1)
                attn = self.attn_drop(attn)
                x = torch.bmm(attn, v)

        # N, num_heads, L, head_dim
        if self.head_scale is not None:
            x = x * self.head_scale
        x = x.transpose(1, 2).reshape(N, L, C)
        x = self.ln_inner(x)
        x = self.out_proj(x)
        x = self.out_drop(x)
        return x


class AttentionalPooler(nn.Module):
    def __init__(
            self,
            d_model: int,
            context_dim: int,
            n_head: int = 8,
            n_queries: int = 256,
            norm_layer: Callable = LayerNorm,
    ):
        super().__init__()
        self.query = nn.Parameter(torch.randn(n_queries, d_model))
        self.attn = nn.MultiheadAttention(d_model, n_head, kdim=context_dim, vdim=context_dim, batch_first=True)
        self.ln_q = norm_layer(d_model)
        self.ln_k = norm_layer(context_dim)

    def forward(self, x: torch.Tensor):
        N = x.shape[0]
        x = self.ln_k(x)
        q = self.ln_q(self.query)
        out = self.attn(q.unsqueeze(0).expand(N, -1, -1), x, x, need_weights=False)[0]
        return out


class ResidualAttentionBlock(nn.Module):
    def __init__(
            self,
            d_model: int,
            n_head: int,
            mlp_ratio: float = 4.0,
            ls_init_value: float = None,
            act_layer: Callable = nn.GELU,
            norm_layer: Callable = LayerNorm,
            is_cross_attention: bool = False,
            batch_first: bool = True,
    ):
        super().__init__()

        self.ln_1 = norm_layer(d_model)
        self.attn = nn.MultiheadAttention(d_model, n_head, batch_first=batch_first)
        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()
        if is_cross_attention:
            self.ln_1_kv = norm_layer(d_model)

        self.ln_2 = norm_layer(d_model)
        mlp_width = int(d_model * mlp_ratio)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, mlp_width)),
            ("gelu", act_layer()),
            ("c_proj", nn.Linear(mlp_width, d_model))
        ]))
        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()

    def get_weight_dtype(self) -> torch.dtype:
        if hasattr(self.mlp.c_fc, 'int8_original_dtype'):
            return self.mlp.c_fc.int8_original_dtype
        return self.mlp.c_fc.weight.dtype

    def attention(
            self,
            q_x: torch.Tensor,
            k_x: Optional[torch.Tensor] = None,
            v_x: Optional[torch.Tensor] = None,
            attn_mask: Optional[torch.Tensor] = None,
    ):
        k_x = k_x if k_x is not None else q_x
        v_x = v_x if v_x is not None else q_x

        attn_mask = attn_mask.to(q_x.dtype) if attn_mask is not None else None
        return self.attn(
            q_x, k_x, v_x,
            need_weights=False,
            attn_mask=attn_mask
        )[0]

    def forward(
            self,
            q_x: torch.Tensor,
            k_x: Optional[torch.Tensor] = None,
            v_x: Optional[torch.Tensor] = None,
            attn_mask: Optional[torch.Tensor] = None,
    ):
        k_x = self.ln_1_kv(k_x) if hasattr(self, "ln_1_kv") and k_x is not None else None
        v_x = self.ln_1_kv(v_x) if hasattr(self, "ln_1_kv") and v_x is not None else None
        x = q_x + self.ls_1(self.attention(q_x=self.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask))
        x = x + self.ls_2(self.mlp(self.ln_2(x)))
        return x


class CustomResidualAttentionBlock(nn.Module):
    def __init__(
            self,
            d_model: int,
            n_head: int,
            mlp_ratio: float = 4.0,
            ls_init_value: float = None,
            act_layer: Type[nn.Module] = nn.GELU,
            norm_layer: Type[nn.Module] = LayerNorm,
            qk_norm: bool = False,
            scale_cosine_attn: bool = False,
            scale_heads: bool = False,
            scale_attn_inner: bool = False,
            scale_attn: bool = False,
            scale_fc: bool = False,
            batch_first: bool = True,
    ):
        super().__init__()
        assert batch_first, 'batch_first must be True for CustomResidualAttentionBlock'

        self.ln_1 = norm_layer(d_model)
        self.attn = Attention(
            d_model,
            n_head,
            qk_norm=qk_norm,
            scaled_cosine=scale_cosine_attn,
            scale_heads=scale_heads,
            inner_norm=scale_attn_inner,
            norm_layer=norm_layer,
        )
        self.ln_attn = norm_layer(d_model) if scale_attn else nn.Identity()
        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()

        self.ln_2 = norm_layer(d_model)
        mlp_width = int(d_model * mlp_ratio)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, mlp_width)),
            ("gelu", act_layer()),
            ('ln', norm_layer(mlp_width) if scale_fc else nn.Identity()),  # from NormFormer / Foundation Transformers
            ("c_proj", nn.Linear(mlp_width, d_model))
        ]))
        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()

    def get_weight_dtype(self) -> torch.dtype:
        if hasattr(self.mlp.c_fc, 'int8_original_dtype'):
            return self.mlp.c_fc.int8_original_dtype
        return self.mlp.c_fc.weight.dtype

    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
        x = x + self.ls_1(self.ln_attn(self.attn(self.ln_1(x), attn_mask=attn_mask)))
        x = x + self.ls_2(self.mlp(self.ln_2(x)))
        return x


class CustomTransformer(nn.Module):
    """ A custom transformer that can use different block types. """
    def __init__(
            self,
            width: int,
            layers: int,
            heads: int,
            mlp_ratio: float = 4.0,
            ls_init_value: float = None,
            act_layer: Type[nn.Module] = nn.GELU,
            norm_layer: Type[nn.Module] = LayerNorm,
            batch_first: bool = True,
            block_types: Union[str, List[str]] = 'CustomResidualAttentionBlock',
    ):
        super().__init__()
        self.width = width
        self.layers = layers
        self.batch_first = batch_first  # run transformer stack in batch first (N, L, D)
        self.grad_checkpointing = False

        if isinstance(block_types, str):
            block_types = [block_types] * layers
        assert len(block_types) == layers

        def _create_block(bt: str):
            if bt == 'CustomResidualAttentionBlock':
                return CustomResidualAttentionBlock(
                    width,
                    heads,
                    mlp_ratio=mlp_ratio,
                    ls_init_value=ls_init_value,
                    act_layer=act_layer,
                    norm_layer=norm_layer,
                    batch_first=batch_first,
                )
            else:
                assert False

        self.resblocks = nn.ModuleList([
            _create_block(bt)
            for bt in block_types
        ])

    def get_cast_dtype(self) -> torch.dtype:
        return self.resblocks[0].get_weight_dtype()

    def forward_intermediates(
            self,
            x: torch.Tensor,
            attn_mask: Optional[torch.Tensor] = None,
            indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
    ):
        take_indices, max_index = feature_take_indices(len(self.resblocks), indices)

        if not self.batch_first:
            x = x.transpose(0, 1).contiguous()  # NLD -> LND

        intermediates = []
        if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
            blocks = self.resblocks
        else:
            blocks = self.resblocks[:max_index + 1]
        for i, blk in enumerate(blocks):
            if self.grad_checkpointing and not torch.jit.is_scripting():
                x = checkpoint(blk, x, None, None, attn_mask, use_reentrant=False)
            else:
                x = blk(x, attn_mask=attn_mask)

            if i in take_indices:
                intermediates.append(x.transpose(0, 1) if not self.batch_first else x)

        if not self.batch_first:
            x = x.transpose(0, 1)  # LND -> NLD

        return x, intermediates

    def prune_intermediate_layers(self, indices: Union[int, List[int]] = 1):
        """ Prune layers not required for specified intermediates.
        """
        take_indices, max_index = feature_take_indices(len(self.resblocks), indices)
        self.resblocks = self.resblocks[:max_index + 1]  # truncate blocks
        return take_indices

    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
        if not self.batch_first:
            x = x.transpose(0, 1)  # NLD -> LND

        for r in self.resblocks:
            if self.grad_checkpointing and not torch.jit.is_scripting():
                # TODO: handle kwargs https://github.com/pytorch/pytorch/issues/79887#issuecomment-1161758372
                x = checkpoint(r, x, None, None, attn_mask, use_reentrant=False)
            else:
                x = r(x, attn_mask=attn_mask)

        if not self.batch_first:
            x = x.transpose(0, 1)  # NLD -> LND
        return x


class Transformer(nn.Module):
    def __init__(
            self,
            width: int,
            layers: int,
            heads: int,
            mlp_ratio: float = 4.0,
            ls_init_value: float = None,
            act_layer: Type[nn.Module] = nn.GELU,
            norm_layer: Type[nn.Module] = LayerNorm,
            batch_first: bool = True,
            block_type: Optional[str] = None,
            qk_norm: bool = False,
            scaled_cosine_attn: bool = False,
            scale_heads: bool = False,
            scale_attn_inner: bool = False,
            scale_attn: bool = False,
            scale_fc: bool = False,
    ):
        super().__init__()
        self.width = width
        self.layers = layers
        self.batch_first = batch_first
        self.grad_checkpointing = False

        # Auto-select custom block if any custom features are enabled
        if block_type is None:
            if any([qk_norm, scaled_cosine_attn, scale_heads, scale_attn_inner, scale_attn, scale_fc]):
                block_type = 'custom'
            else:
                block_type = 'default'

        if block_type == 'custom':
            self.resblocks = nn.ModuleList([
                CustomResidualAttentionBlock(
                    width,
                    heads,
                    mlp_ratio,
                    ls_init_value=ls_init_value,
                    act_layer=act_layer,
                    norm_layer=norm_layer,
                    qk_norm=qk_norm,
                    scale_cosine_attn=scaled_cosine_attn,
                    scale_heads=scale_heads,
                    scale_attn_inner=scale_attn_inner,
                    scale_attn=scale_attn,
                    scale_fc=scale_fc,
                    batch_first=batch_first,
                )
                for _ in range(layers)
            ])
        else:
            self.resblocks = nn.ModuleList([
                ResidualAttentionBlock(
                    width,
                    heads,
                    mlp_ratio,
                    ls_init_value=ls_init_value,
                    act_layer=act_layer,
                    norm_layer=norm_layer,
                    batch_first=batch_first,
                )
                for _ in range(layers)
            ])

    def get_cast_dtype(self) -> torch.dtype:
        return self.resblocks[0].get_weight_dtype()

    def forward_intermediates(
            self,
            x: torch.Tensor,
            attn_mask: Optional[torch.Tensor] = None,
            indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
    ):
        take_indices, max_index = feature_take_indices(len(self.resblocks), indices)

        if not self.batch_first:
            x = x.transpose(0, 1).contiguous()    # NLD -> LND

        intermediates = []
        if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
            blocks = self.resblocks
        else:
            blocks = self.resblocks[:max_index + 1]
        for i, blk in enumerate(blocks):
            if self.grad_checkpointing and not torch.jit.is_scripting():
                x = checkpoint(blk, x, None, None, attn_mask, use_reentrant=False)
            else:
                x = blk(x, attn_mask=attn_mask)

            if i in take_indices:
                intermediates.append(x.transpose(0, 1) if not self.batch_first else x)

        if not self.batch_first:
            x = x.transpose(0, 1)    # LND -> NLD

        return x, intermediates

    def prune_intermediate_layers(self, indices: Union[int, List[int]] = 1):
        """ Prune layers not required for specified intermediates.
        """
        take_indices, max_index = feature_take_indices(len(self.resblocks), indices)
        self.resblocks = self.resblocks[:max_index + 1]  # truncate blocks
        return take_indices

    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
        if not self.batch_first:
            x = x.transpose(0, 1).contiguous()    # NLD -> LND

        for r in self.resblocks:
            if self.grad_checkpointing and not torch.jit.is_scripting():
                # TODO: handle kwargs https://github.com/pytorch/pytorch/issues/79887#issuecomment-1161758372
                x = checkpoint(r, x, None, None, attn_mask, use_reentrant=False)
            else:
                x = r(x, attn_mask=attn_mask)

        if not self.batch_first:
            x = x.transpose(0, 1)    # LND -> NLD
        return x


def _expand_token(token, batch_size: int):
    return token.view(1, 1, -1).expand(batch_size, -1, -1)


class VisionTransformer(nn.Module):
    output_tokens: torch.jit.Final[bool]

    def __init__(
            self,
            image_size: int,
            patch_size: int,
            width: int,
            layers: int,
            heads: int,
            mlp_ratio: float,
            ls_init_value: float = None,
            attentional_pool: bool = False,
            attn_pooler_queries: int = 256,
            attn_pooler_heads: int = 8,
            output_dim: int = 512,
            patch_dropout: float = 0.,
            no_ln_pre: bool = False,
            pos_embed_type: str = 'learnable',
            pool_type: str = 'tok',
            final_ln_after_pool: bool = False,
            act_layer: Callable = nn.GELU,
            norm_layer: Callable = LayerNorm,
            output_tokens: bool = False,
            block_type: Optional[str] = None,
            qk_norm: bool = False,
            scaled_cosine_attn: bool = False,
            scale_heads: bool = False,
            scale_attn_inner: bool = False,
            scale_attn: bool = False,
            scale_fc: bool = False,
    ):
        super().__init__()
        assert pool_type in ('tok', 'avg', 'none')
        self.output_tokens = output_tokens
        image_height, image_width = self.image_size = to_2tuple(image_size)
        patch_height, patch_width = self.patch_size = to_2tuple(patch_size)
        self.grid_size = (image_height // patch_height, image_width // patch_width)
        self.final_ln_after_pool = final_ln_after_pool  # currently ignored w/ attn pool enabled
        self.output_dim = output_dim

        self.conv1 = nn.Conv2d(
            in_channels=3,
            out_channels=width,
            kernel_size=patch_size,
            stride=patch_size,
            bias=False,
        )

        # class embeddings and positional embeddings
        scale = width ** -0.5
        self.class_embedding = nn.Parameter(scale * torch.randn(width))
        if pos_embed_type == 'learnable':
            self.positional_embedding = nn.Parameter(
                scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, width))
        elif pos_embed_type == 'sin_cos_2d':
            # fixed sin-cos embedding
            assert self.grid_size[0] == self.grid_size[1],\
                'currently sin cos 2d pos embedding only supports square input'
            self.positional_embedding = nn.Parameter(
                torch.zeros(self.grid_size[0] * self.grid_size[1] + 1, width), requires_grad=False)
            pos_embed_type = get_2d_sincos_pos_embed(width, self.grid_size[0], cls_token=True)
            self.positional_embedding.data.copy_(torch.from_numpy(pos_embed_type).float())
        else:
            raise ValueError

        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn
        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0. else nn.Identity()

        self.ln_pre = nn.Identity() if no_ln_pre else norm_layer(width)
        self.transformer = Transformer(
            width,
            layers,
            heads,
            mlp_ratio,
            ls_init_value=ls_init_value,
            act_layer=act_layer,
            norm_layer=norm_layer,
            block_type=block_type,
            qk_norm=qk_norm,
            scaled_cosine_attn=scaled_cosine_attn,
            scale_heads=scale_heads,
            scale_attn_inner=scale_attn_inner,
            scale_attn=scale_attn,
            scale_fc=scale_fc,
        )

        if attentional_pool:
            if isinstance(attentional_pool, str):
                self.attn_pool_type = attentional_pool
                self.pool_type = 'none'
                if attentional_pool in ('parallel', 'cascade'):
                    self.attn_pool = AttentionalPooler(
                        output_dim,
                        width,
                        n_head=attn_pooler_heads,
                        n_queries=attn_pooler_queries,
                    )
                    self.attn_pool_contrastive = AttentionalPooler(
                        output_dim,
                        width,
                        n_head=attn_pooler_heads,
                        n_queries=1,
                    )
                else:
                    assert False
            else:
                self.attn_pool_type = ''
                self.pool_type = pool_type
                self.attn_pool = AttentionalPooler(
                    output_dim,
                    width,
                    n_head=attn_pooler_heads,
                    n_queries=attn_pooler_queries,
                )
                self.attn_pool_contrastive = None
            pool_dim = output_dim
        else:
            self.attn_pool = None
            pool_dim = width
            self.pool_type = pool_type

        self.ln_post = norm_layer(pool_dim)
        self.proj = nn.Parameter(scale * torch.randn(pool_dim, output_dim))

        self.init_parameters()

    def lock(self, unlocked_groups: int = 0, freeze_bn_stats: bool = False):
        for param in self.parameters():
            param.requires_grad = False

        if unlocked_groups != 0:
            groups = [
                [
                    self.conv1,
                    self.class_embedding,
                    self.positional_embedding,
                    self.ln_pre,
                ],
                *self.transformer.resblocks[:-1],
                [
                    self.transformer.resblocks[-1],
                    self.ln_post,
                ],
                self.proj,
            ]

            def _unlock(x):
                if isinstance(x, Sequence):
                    for g in x:
                        _unlock(g)
                else:
                    if isinstance(x, torch.nn.Parameter):
                        x.requires_grad = True
                    else:
                        for p in x.parameters():
                            p.requires_grad = True

            _unlock(groups[-unlocked_groups:])

    def init_parameters(self):
        # FIXME OpenAI CLIP did not define an init for the VisualTransformer
        # TODO experiment if default PyTorch init, below, or alternate init is best.

        # nn.init.normal_(self.class_embedding, std=self.scale)
        # nn.init.normal_(self.positional_embedding, std=self.scale)
        #
        # proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)
        # attn_std = self.transformer.width ** -0.5
        # fc_std = (2 * self.transformer.width) ** -0.5
        # for block in self.transformer.resblocks:
        #     nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
        #     nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
        #     nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
        #     nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)
        #
        # if self.text_projection is not None:
        #     nn.init.normal_(self.text_projection, std=self.scale)
        pass

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable: bool = True):
        self.transformer.grad_checkpointing = enable

    @torch.jit.ignore
    def no_weight_decay(self):
        # for timm optimizers, 1d params like logit_scale, logit_bias, ln/bn scale, biases are excluded by default
        no_wd = {'positional_embedding', 'class_embedding'}
        return no_wd

    def _global_pool(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        if self.pool_type == 'avg':
            pooled, tokens = x[:, 1:].mean(dim=1), x[:, 1:]
        elif self.pool_type == 'tok':
            pooled, tokens = x[:, 0], x[:, 1:]
        else:
            pooled = tokens = x

        return pooled, tokens

    def _embeds(self, x:torch.Tensor) -> torch.Tensor:
        x = self.conv1(x)  # shape = [*, dim, grid, grid]
        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]
        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]

        # class embeddings and positional embeddings
        x = torch.cat([_expand_token(self.class_embedding, x.shape[0]).to(x.dtype), x], dim=1)
        # shape = [*, grid ** 2 + 1, width]
        x = x + self.positional_embedding.to(x.dtype)

        # patch dropout (if active)
        x = self.patch_dropout(x)

        # apply norm before transformer
        x = self.ln_pre(x)
        return x

    def _pool(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        if self.attn_pool is not None:
            if self.attn_pool_contrastive is not None:
                # This is untested, WIP pooling that should match paper
                x = self.ln_post(x)  # TBD LN first or separate one after each pool?
                tokens = self.attn_pool(x)
                if self.attn_pool_type == 'parallel':
                    pooled = self.attn_pool_contrastive(x)
                else:
                    assert self.attn_pool_type == 'cascade'
                    pooled = self.attn_pool_contrastive(tokens)
            else:
                # this is the original OpenCLIP CoCa setup, does not match paper
                x = self.attn_pool(x)
                x = self.ln_post(x)
                pooled, tokens = self._global_pool(x)
        elif self.final_ln_after_pool:
            pooled, tokens = self._global_pool(x)
            pooled = self.ln_post(pooled)
        else:
            x = self.ln_post(x)
            pooled, tokens = self._global_pool(x)

        return pooled, tokens

    def forward_intermediates(
            self,
            x: torch.Tensor,
            indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
            normalize_intermediates: bool = False,
            intermediates_only: bool = False,
            output_fmt: str = 'NCHW',
            output_extra_tokens: bool = False,
    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """ Forward features that returns intermediates.

        Args:
            x: Input image tensor
            indices: Take last n blocks if int, all if None, select matching indices if sequence
            stop_early: Stop iterating over blocks when last desired intermediate hit
            intermediates_only: Only return intermediate features
            normalize_intermediates: Apply final norm layer to all intermediates
            output_fmt: Shape of intermediate feature outputs
            output_extra_tokens: Return both extra prefix class tokens
        Returns:

        """
        assert output_fmt in ('NCHW', 'NLC'), 'Output format must be one of NCHW or NLC.'
        reshape = output_fmt == 'NCHW'

        # forward pass
        B, _, height, width = x.shape
        x = self._embeds(x)
        x, intermediates = self.transformer.forward_intermediates(
            x,
            indices=indices,
            stop_early=stop_early,
        )

        # process intermediates
        if normalize_intermediates:
            # apply final norm to all intermediates
            intermediates = [self.ln_post(xi) for xi in intermediates]
        num_prefix_tokens = 1  # one class token that's always there (as of now)
        if num_prefix_tokens:
            # split prefix (e.g. class, distill) and spatial feature tokens
            prefix_tokens = [y[:, 0:num_prefix_tokens] for y in intermediates]
            intermediates = [y[:, num_prefix_tokens:] for y in intermediates]
        else:
            prefix_tokens = None
        if reshape:
            # reshape to BCHW output format
            H, W = height // self.patch_size[0], width // self.patch_size[1]
            intermediates = [y.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous() for y in intermediates]

        output = {'image_intermediates': intermediates}
        if prefix_tokens is not None and output_extra_tokens:
            output['image_intermediates_prefix'] = prefix_tokens

        if intermediates_only:
            return output

        pooled, _ = self._pool(x)

        if self.proj is not None:
            pooled = pooled @ self.proj

        output['image_features'] = pooled

        return output

    def prune_intermediate_layers(
            self,
            indices: Union[int, List[int]] = 1,
            prune_norm: bool = False,
            prune_head: bool = True,
    ):
        """ Prune layers not required for specified intermediates.
        """
        take_indices = self.transformer.prune_intermediate_layers(indices)
        if prune_norm:
            self.ln_post = nn.Identity()
        if prune_head:
            self.proj = None
        return take_indices

    def forward(self, x: torch.Tensor):
        x = self._embeds(x)
        x = self.transformer(x)
        pooled, tokens = self._pool(x)

        if self.proj is not None:
            pooled = pooled @ self.proj

        if self.output_tokens:
            return pooled, tokens
        
        return pooled


def text_global_pool(
        x: torch.Tensor,
        text: Optional[torch.Tensor] = None,
        pool_type: str = 'argmax',
        eos_token_id: Optional[int] = None,
) -> torch.Tensor:
    if pool_type == 'first':
        pooled = x[:, 0]
    elif pool_type == 'last':
        pooled = x[:, -1]
    elif pool_type == 'argmax':
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        assert text is not None
        pooled = x[torch.arange(x.shape[0], device=x.device), text.argmax(dim=-1)]
    elif pool_type == 'eos':
        # take features from tokenizer specific eos
        assert text is not None
        assert eos_token_id is not None
        idx = (text == eos_token_id).int().argmax(dim=-1)
        pooled = x[torch.arange(x.shape[0], device=x.device), idx]
    else:
        pooled = x

    return pooled


class TextTransformer(nn.Module):
    output_tokens: torch.jit.Final[bool]

    def __init__(
            self,
            context_length: int = 77,
            vocab_size: int = 49408,
            width: int = 512,
            heads: int = 8,
            layers: int = 12,
            mlp_ratio: float = 4.0,
            ls_init_value: float = None,
            output_dim: Optional[int] = 512,
            embed_cls: bool = False,
            no_causal_mask: bool = False,
            use_pad_mask: bool = False,
            correct_cls_mask: bool = False,
            pad_id: int = 0,
            eos_id: int = 2,
            pool_type: str = 'argmax',
            proj_type: str = 'linear',
            proj_bias: bool = False,
            act_layer: Type[nn.Module] = nn.GELU,
            norm_layer: Type[nn.Module] = LayerNorm,
            output_tokens: bool = False,
            block_type: Optional[str] = None,
            qk_norm: bool = False,
            scaled_cosine_attn: bool = False,
            scale_heads: bool = False,
            scale_attn_inner: bool = False,
            scale_attn: bool = False,
            scale_fc: bool = False,
    ):
        super().__init__()
        assert pool_type in ('first', 'last', 'argmax', 'eos', 'none')
        self.output_tokens = output_tokens
        self.num_pos = self.context_length = context_length
        self.vocab_size = vocab_size
        self.width = width
        self.output_dim = output_dim
        self.heads = heads
        self.pad_id = pad_id
        self.eos_id = eos_id
        self.pool_type = pool_type
        self.use_pad_mask = use_pad_mask and no_causal_mask  # only use in bi‚Äëdir mode
        self.correct_cls_mask = correct_cls_mask  # use the correct cls mask for CoCa (original is wrong)

        self.token_embedding = nn.Embedding(vocab_size, width)
        if embed_cls:
            self.cls_emb = nn.Parameter(torch.empty(width))
            self.num_pos += 1
        else:
            self.cls_emb = None
        self.positional_embedding = nn.Parameter(torch.empty(self.num_pos, width))
        self.transformer = Transformer(
            width=width,
            layers=layers,
            heads=heads,
            mlp_ratio=mlp_ratio,
            ls_init_value=ls_init_value,
            act_layer=act_layer,
            norm_layer=norm_layer,
            block_type=block_type,
            qk_norm=qk_norm,
            scaled_cosine_attn=scaled_cosine_attn,
            scale_heads=scale_heads,
            scale_attn_inner=scale_attn_inner,
            scale_attn=scale_attn,
            scale_fc=scale_fc,
        )
        self.ln_final = norm_layer(width)

        if no_causal_mask:
            self.attn_mask = None  # bi‚Äëdirectional
        else:
            self.register_buffer('attn_mask', self.build_causal_mask(), persistent=False)

        if proj_type == 'none' or not output_dim:
            self.text_projection = None
        else:
            if proj_bias:
                self.text_projection = nn.Linear(width, output_dim)
            else:
                self.text_projection = nn.Parameter(torch.empty(width, output_dim))

        self.init_parameters()

    def init_parameters(self):
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.positional_embedding, std=0.01)
        if self.cls_emb is not None:
            nn.init.normal_(self.cls_emb, std=0.01)

        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)
        attn_std = self.transformer.width ** -0.5
        fc_std = (2 * self.transformer.width) ** -0.5
        for block in self.transformer.resblocks:
            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)

        if self.text_projection is not None:
            if isinstance(self.text_projection, nn.Linear):
                nn.init.normal_(self.text_projection.weight, std=self.transformer.width ** -0.5)
                if self.text_projection.bias is not None:
                    nn.init.zeros_(self.text_projection.bias)
            else:
                nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable=True):
        self.transformer.grad_checkpointing = enable

    def lock(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):
        """
        Lock the text transformer layers, optionally leaving some layers unlocked.

        Args:
            unlocked_layers: Number of layers to leave unlocked (from the end).
            freeze_layer_norm: LayerNorm freeze (only for API compatibility, not functional)
        """
        assert freeze_layer_norm, 'Unfreezing LayerNorm is not supported. LayerNorm treated like other weights.'
        lock_text_tower(self, unlocked_layers)

    @torch.jit.ignore
    def no_weight_decay(self):
        # for timm optimizers, 1d params like logit_scale, logit_bias, ln/bn scale, biases are excluded by default
        no_wd = {'positional_embedding'}
        if self.cls_emb is not None:
            no_wd.add('cls_emb')
        return no_wd

    def build_causal_mask(self):
        # lazily create causal attention mask, with full attention between the tokens
        # pytorch uses additive attention mask; fill with -inf
        mask = torch.empty(self.num_pos, self.num_pos)
        mask.fill_(float("-inf"))
        mask.triu_(1)  # zero out the lower diagonal
        return mask

    def _build_additive_mask(
        self,
        text: torch.Tensor,  # [B, L] ‚Äì original text ids without CLS yet
        seq_len: int,  # L (+1 if CLS added)
        dtype: torch.dtype,
    ) -> torch.Tensor:
        """
        Returns an additive (-inf) mask of shape [B*heads, seq_len, seq_len] that
        simultaneously masks padding tokens and (optionally) the CLS token.
        """
        valid = text != self.pad_id  # [B, L] (True = keep)

        if self.cls_emb is not None:
            cls_valid = valid.new_ones(valid.size(0), 1) # [B, 1]
            # cls mask pos at end if correct or front for incorrect legacy mode in existing CoCa weights
            valid = torch.cat([valid, cls_valid] if self.correct_cls_mask else [cls_valid, valid], 1)

        # broadcast over query dimension
        key_mask = valid.unsqueeze(1).expand(-1, seq_len, -1)  # [B, Q, K]
        additive = torch.zeros_like(key_mask, dtype=dtype)
        additive.masked_fill_(~key_mask, float("-inf"))
        additive = additive.repeat_interleave(self.heads, 0)  # [B*H, Q, K]
        return additive

    def _embeds(self, text) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        cast_dtype = self.transformer.get_cast_dtype()
        B, seq_len = text.shape

        x = self.token_embedding(text).to(cast_dtype)

        # Optional class token (always appended ala CoCa)
        if self.cls_emb is not None:
            x = torch.cat([x, _expand_token(self.cls_emb, x.size(0))], 1)
            seq_len += 1

        attn_mask = self.attn_mask  # Base causal mask (if any)

        # Class + padding additive mask
        if self.use_pad_mask or self.cls_emb is not None:
            add_mask  = self._build_additive_mask(text, seq_len, x.dtype)
            if attn_mask is not None:
                # Slice the causal mask to match current sequence length
                attn_mask = attn_mask[:seq_len, :seq_len].unsqueeze(0) + add_mask
            else:
                attn_mask = add_mask

        x = x + self.positional_embedding[:seq_len].to(cast_dtype)
        return x, attn_mask

    def forward_intermediates(
            self,
            text: torch.Tensor,
            indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
            normalize_intermediates: bool = False,
            intermediates_only: bool = False,
            output_fmt: str = 'NCHW',
            output_extra_tokens: bool = False,
    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """ Forward features that returns intermediates.

        Args:
            text: Input text ids
            indices: Take last n blocks if int, all if None, select matching indices if sequence
            stop_early: Stop iterating over blocks when last desired intermediate hit
            normalize_intermediates: Apply norm layer to all intermediates
            intermediates_only: Only return intermediate features
            output_fmt: Shape of intermediate feature outputs
            output_extra_tokens: Return both prefix and intermediate tokens
        Returns:

        """
        assert output_fmt in ('NLC',), 'Output format must be NLC.'
        # forward pass
        x, attn_mask = self._embeds(text)
        x, intermediates = self.transformer.forward_intermediates(
            x,
            attn_mask=attn_mask,
            indices=indices,
            stop_early=stop_early,
        )

        # process intermediates
        if normalize_intermediates:
            # apply final norm to all intermediates
            intermediates = [self.ln_final(xi) for xi in intermediates]

        output = {}

        if self.cls_emb is not None:
            seq_intermediates = [xi[:, :-1] for xi in intermediates]  # separate concat'd class token from sequence
            if output_extra_tokens:
                # return suffix class tokens separately
                cls_intermediates = [xi[:, -1:] for xi in intermediates]
                output['text_intermediates_suffix'] = cls_intermediates
            intermediates = seq_intermediates
        output['text_intermediates'] = intermediates

        if intermediates_only:
            return output

        if self.cls_emb is not None:
            # presence of appended cls embed (CoCa) overrides pool_type, always take last token
            pooled = text_global_pool(x, pool_type='last')
            pooled = self.ln_final(pooled)  # final LN applied after pooling in this case
        else:
            x = self.ln_final(x)
            pooled = text_global_pool(x, text, pool_type=self.pool_type, eos_token_id=getattr(self, "eos_id", None))

        if self.text_projection is not None:
            if isinstance(self.text_projection, nn.Linear):
                pooled = self.text_projection(pooled)
            else:
                pooled = pooled @ self.text_projection

        output['text_features'] = pooled

        return output

    def prune_intermediate_layers(
            self,
            indices: Union[int, List[int]] = 1,
            prune_norm: bool = False,
            prune_head: bool = True,
    ):
        """ Prune layers not required for specified intermediates.
        """
        take_indices = self.transformer.prune_intermediate_layers(indices)
        if prune_norm:
            self.ln_final = nn.Identity()
        if prune_head:
            self.text_projection = None
        return take_indices

    def forward(self, text):
        x, attn_mask = self._embeds(text)

        x = self.transformer(x, attn_mask=attn_mask)

        # x.shape = [batch_size, n_ctx, transformer.width]
        if self.cls_emb is not None:
            # presence of appended cls embed (CoCa) overrides pool_type, always take last token
            pooled = text_global_pool(x, pool_type='last')
            pooled = self.ln_final(pooled)  # final LN applied after pooling in this case
            tokens = x[:, :-1]
        else:
            x = self.ln_final(x)
            pooled = text_global_pool(x, text, pool_type=self.pool_type, eos_token_id=getattr(self, "eos_id", None))
            tokens = x

        if self.text_projection is not None:
            if isinstance(self.text_projection, nn.Linear):
                pooled = self.text_projection(pooled)
            else:
                pooled = pooled @ self.text_projection

        if self.output_tokens:
            return pooled, tokens

        return pooled


class MultimodalTransformer(Transformer):
    def __init__(
            self,
            width: int,
            layers: int,
            heads: int,
            context_length: int = 77,
            mlp_ratio: float = 4.0,
            ls_init_value: float = None,
            act_layer: Type[nn.Module] = nn.GELU,
            norm_layer: Type[nn.Module] = LayerNorm,
            output_dim: int = 512,
            batch_first: bool = True,
    ):
        super().__init__(
            width=width,
            layers=layers,
            heads=heads,
            mlp_ratio=mlp_ratio,
            ls_init_value=ls_init_value,
            act_layer=act_layer,
            norm_layer=norm_layer,
            batch_first=batch_first,
        )
        self.context_length = context_length
        self.cross_attn = nn.ModuleList([
            ResidualAttentionBlock(
                width,
                heads,
                mlp_ratio,
                ls_init_value=ls_init_value,
                act_layer=act_layer,
                norm_layer=norm_layer,
                is_cross_attention=True,
                batch_first=batch_first,
            )
            for _ in range(layers)
        ])

        self.register_buffer('attn_mask', self.build_attention_mask(), persistent=False)

        self.ln_final = norm_layer(width)
        self.text_projection = nn.Parameter(torch.empty(width, output_dim))

    def init_parameters(self):
        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)
        attn_std = self.transformer.width ** -0.5
        fc_std = (2 * self.transformer.width) ** -0.5
        for block in self.transformer.resblocks:
            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)
        for block in self.transformer.cross_attn:
            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)

        if self.text_projection is not None:
            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)

    def build_attention_mask(self):
        # lazily create causal attention mask, with full attention between the tokens
        # pytorch uses additive attention mask; fill with -inf
        mask = torch.empty(self.context_length, self.context_length)
        mask.fill_(float("-inf"))
        mask.triu_(1)  # zero out the lower diagonal
        return mask

    def forward_intermediates(
            self,
            x: torch.Tensor,
            attn_mask: Optional[torch.Tensor] = None,
            indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
    ):
        assert False, "Not currently implemented for MultimodalTransformer w/ xattn"

    def forward(self, image_embs, text_embs):
        seq_len = text_embs.shape[1]
        if not self.batch_first:
            image_embs = image_embs.permute(1, 0, 2)  # NLD -> LND
            text_embs = text_embs.permute(1, 0, 2)  # NLD -> LND

        for resblock, cross_attn in zip(self.resblocks, self.cross_attn):
            if self.grad_checkpointing and not torch.jit.is_scripting():
                # TODO: handle kwargs https://github.com/pytorch/pytorch/issues/79887#issuecomment-1161758372
                text_embs = checkpoint(
                    resblock, text_embs, None, None, self.attn_mask[:seq_len, :seq_len], use_reentrant=False)
                text_embs = checkpoint(
                    cross_attn, text_embs, image_embs, image_embs, None, use_reentrant=False)
            else:
                text_embs = resblock(text_embs, attn_mask=self.attn_mask[:seq_len, :seq_len])
                text_embs = cross_attn(text_embs, k_x=image_embs, v_x=image_embs)

        if not self.batch_first:
            text_embs = text_embs.permute(1, 0, 2)  # LND -> NLD

        out = self.ln_final(text_embs)
        if self.text_projection is not None:
            out = out @ self.text_projection

        return out

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable=True):
        self.grad_checkpointing = enable


def lock_text_tower(
    model: nn.Module,
    unlocked_layers: int = 0,
):
    """
    Lock text tower layers for CLIP models.

    Works with both model architectures:
    - CustomTextCLIP where text components are in self.text
    - Standard CLIP where text components are unpacked as attributes

    Args:
        model: The CLIP model or TextTransformer module
        unlocked_layers: Number of layers to leave unlocked (from the end)
    """
    # Determine where to look for text components
    if hasattr(model, 'text'):
        # CustomTextCLIP or already a TextTransformer with nested structure
        text_module = model.text
    else:
        # Standard CLIP or direct TextTransformer
        text_module = model

    # Collect text components
    text_params = {}
    text_params['token_embedding'] = getattr(text_module, 'token_embedding', None)
    text_params['positional_embedding'] = getattr(text_module, 'positional_embedding', None)
    text_params['cls_emb'] = getattr(text_module, 'cls_emb', None)
    text_params['transformer'] = getattr(text_module, 'transformer', None)
    text_params['ln_final'] = getattr(text_module, 'ln_final', None)
    text_params['text_projection'] = getattr(text_module, 'text_projection', None)

    # Filter out None values
    text_params = {k: v for k, v in text_params.items() if v is not None}

    # Freeze all text parameters first
    for module in text_params.values():
        if isinstance(module, nn.Parameter):
            module.requires_grad = False
        elif isinstance(module, nn.Module):
            for param in module.parameters():
                param.requires_grad = False

    if unlocked_layers == 0:
        return

    # Check if we have transformer blocks to work with
    transformer = text_params['transformer']
    if not transformer or not hasattr(transformer, 'resblocks'):
        return

    total_layers = len(transformer.resblocks)
    if total_layers == 0:
        return

    # Build groups for selective unlocking
    groups = []

    # Group 1: Embeddings
    embedding_group = []
    for key in ['token_embedding', 'positional_embedding', 'cls_emb']:
        if key in text_params:
            embedding_group.append(text_params[key])
    if embedding_group:
        groups.append(embedding_group)

    # Group 2-N: Individual transformer blocks (except last)
    if total_layers > 1:
        for block in transformer.resblocks[:-1]:
            groups.append([block])

    # Combine last transformer block + final ln as the penultimate group
    last_block = [transformer.resblocks[-1]]
    if 'ln_final' in text_params:
        last_block.append(text_params['ln_final'])
    groups.append(last_block)

    # The final group is the projection only
    if 'text_projection' in text_params:
        groups.append([text_params['text_projection']])

    # Helper function to unlock parameters
    def _unlock(module):
        if isinstance(module, Sequence):
            for m in module:
                _unlock(m)
        elif isinstance(module, nn.Parameter):
            module.requires_grad = True
        elif isinstance(module, nn.Module):
            for name, param in module.named_parameters():
                param.requires_grad = True

    # Unlock the specified number of layer groups from the end
    num_groups_to_unlock = min(unlocked_layers, len(groups))
    for group in groups[-num_groups_to_unlock:]:
        _unlock(group)

===== src/open_clip/pos_embed.py =====
# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# Position embedding utils
# --------------------------------------------------------

import numpy as np

import torch

# --------------------------------------------------------
# 2D sine-cosine position embedding
# References:
# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py
# MoCo v3: https://github.com/facebookresearch/moco-v3
# --------------------------------------------------------
def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: int of the grid height and width
    return:
    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)  # here w goes first
    grid = np.stack(grid, axis=0)

    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 2 == 0

    # use half of dimensions to encode grid_h
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)

    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)
    return emb


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=float)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb


# --------------------------------------------------------
# Interpolate position embeddings for high-resolution
# References:
# DeiT: https://github.com/facebookresearch/deit
# --------------------------------------------------------
def interpolate_pos_embed(model, checkpoint_model):
    if 'pos_embed' in checkpoint_model:
        pos_embed_checkpoint = checkpoint_model['pos_embed']
        embedding_size = pos_embed_checkpoint.shape[-1]
        num_patches = model.patch_embed.num_patches
        num_extra_tokens = model.pos_embed.shape[-2] - num_patches
        # height (== width) for the checkpoint position embedding
        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
        # height (== width) for the new position embedding
        new_size = int(num_patches ** 0.5)
        # class_token and dist_token are kept unchanged
        if orig_size != new_size:
            print("Position interpolate from %dx%d to %dx%d" % (orig_size, orig_size, new_size, new_size))
            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
            # only the position tokens are interpolated
            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)
            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
            checkpoint_model['pos_embed'] = new_pos_embed

===== src/open_clip/modified_resnet.py =====
from collections import OrderedDict
from typing import Dict, List, Optional, Union

import torch
from torch import nn
from torch.nn import functional as F

from .utils import freeze_batch_norm_2d, feature_take_indices


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1):
        super().__init__()

        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1
        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.act1 = nn.ReLU(inplace=True)

        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.act2 = nn.ReLU(inplace=True)

        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()

        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.act3 = nn.ReLU(inplace=True)

        self.downsample = None
        self.stride = stride

        if stride > 1 or inplanes != planes * Bottleneck.expansion:
            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1
            self.downsample = nn.Sequential(OrderedDict([
                ("-1", nn.AvgPool2d(stride)),
                ("0", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),
                ("1", nn.BatchNorm2d(planes * self.expansion))
            ]))

    def forward(self, x: torch.Tensor):
        identity = x

        out = self.act1(self.bn1(self.conv1(x)))
        out = self.act2(self.bn2(self.conv2(out)))
        out = self.avgpool(out)
        out = self.bn3(self.conv3(out))

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.act3(out)
        return out


class AttentionPool2d(nn.Module):
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
        x, _ = F.multi_head_attention_forward(
            query=x, key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0.,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )

        return x[0]


class ModifiedResNet(nn.Module):
    """
    A ResNet class that is similar to torchvision's but contains the following changes:
    - There are now 3 "stem" convolutions as opposed to 1, with an average pool instead of a max pool.
    - Performs antialiasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1
    - The final pooling layer is a QKV attention instead of an average pool
    """

    def __init__(
            self,
            layers: List[int],
            output_dim: int,
            heads: int,
            image_size: int = 224,
            width: int = 64,
    ):
        super().__init__()
        self.output_dim = output_dim
        self.image_size = image_size

        # the 3-layer stem
        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(width // 2)
        self.act1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(width // 2)
        self.act2 = nn.ReLU(inplace=True)
        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(width)
        self.act3 = nn.ReLU(inplace=True)
        self.avgpool = nn.AvgPool2d(2)

        # residual layers
        self._inplanes = width  # this is a *mutable* variable used during construction
        self.layer1 = self._make_layer(width, layers[0])
        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)
        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)
        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)

        embed_dim = width * 32  # the ResNet feature dimension
        self.attnpool = AttentionPool2d(image_size // 32, embed_dim, heads, output_dim)

        self.init_parameters()

    def _make_layer(self, planes, blocks, stride=1):
        layers = [Bottleneck(self._inplanes, planes, stride)]

        self._inplanes = planes * Bottleneck.expansion
        for _ in range(1, blocks):
            layers.append(Bottleneck(self._inplanes, planes))

        return nn.Sequential(*layers)

    def init_parameters(self):
        if self.attnpool is not None:
            std = self.attnpool.c_proj.in_features ** -0.5
            nn.init.normal_(self.attnpool.q_proj.weight, std=std)
            nn.init.normal_(self.attnpool.k_proj.weight, std=std)
            nn.init.normal_(self.attnpool.v_proj.weight, std=std)
            nn.init.normal_(self.attnpool.c_proj.weight, std=std)

        for resnet_block in [self.layer1, self.layer2, self.layer3, self.layer4]:
            for name, param in resnet_block.named_parameters():
                if name.endswith("bn3.weight"):
                    nn.init.zeros_(param)

    def lock(self, unlocked_groups=0, freeze_bn_stats=False):
        assert unlocked_groups == 0, 'partial locking not currently supported for this model'
        for param in self.parameters():
            param.requires_grad = False
        if freeze_bn_stats:
            freeze_batch_norm_2d(self)

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable=True):
        # FIXME support for non-transformer
        pass

    def stem(self, x):
        x = self.act1(self.bn1(self.conv1(x)))
        x = self.act2(self.bn2(self.conv2(x)))
        x = self.act3(self.bn3(self.conv3(x)))
        x = self.avgpool(x)
        return x

    def forward_intermediates(
            self,
            x: torch.Tensor,
            indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
            normalize_intermediates: bool = False,
            intermediates_only: bool = False,
            output_fmt: str = 'NCHW',
            output_extra_tokens: bool = False,
    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """ Forward features that returns intermediates.

        Args:
            x: Input image tensor
            indices: Take last n blocks if int, all if None, select matching indices if sequence
            stop_early: Stop iterating over blocks when last desired intermediate hit
            normalize_intermediates: Apply final norm layer to all intermediates
            intermediates_only: Only return intermediate features
            output_fmt: Shape of intermediate feature outputs
            output_extra_tokens: Return both extra class, eot tokens
        Returns:

        """
        assert output_fmt in ('NCHW',), 'Output format must be == NCHW.'
        # NOTE normalize_intermediates and return_extra_tokens don't apply
        take_indices, max_index = feature_take_indices(5, indices)

        output = {}
        intermediates = []
        blocks = [self.stem, self.layer1, self.layer2, self.layer3, self.layer4]
        if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
            blocks = blocks[:max_index + 1]
        for i, blk in enumerate(blocks):
            x = blk(x)
            if i in take_indices:
                intermediates.append(x)

        output['image_intermediates'] = intermediates

        if intermediates_only:
            return output

        x = self.attnpool(x)
        output['image_features'] = x

        return output

    def forward(self, x):
        x = self.stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.attnpool(x)

        return x

===== src/open_clip/transform.py =====
# src/open_clip/transform.py

import numbers
import random
import warnings
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Optional, Sequence, Tuple, Union

import torch
import torchvision.transforms.functional as F
from omegaconf import DictConfig, OmegaConf
from torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, \
    CenterCrop, ColorJitter, Grayscale

from .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD
from .utils import to_2tuple

@dataclass
class PreprocessCfg:
    size: Union[int, Tuple[int, int]] = 224
    mode: str = 'RGB'
    mean: Tuple[float, ...] = OPENAI_DATASET_MEAN
    std: Tuple[float, ...] = OPENAI_DATASET_STD
    interpolation: str = 'bicubic'
    resize_mode: str = 'shortest'
    fill_color: int = 0

    def __post_init__(self):
        assert self.mode in ('RGB',)

    @property
    def num_channels(self):
        return 3

    @property
    def input_size(self):
        return (self.num_channels,) + to_2tuple(self.size)

_PREPROCESS_KEYS = set(asdict(PreprocessCfg()).keys())

def merge_preprocess_dict(
        base: Union[PreprocessCfg, Dict],
        overlay: Dict,
):
    if isinstance(base, PreprocessCfg):
        base_clean = asdict(base)
    else:
        base_clean = {k: v for k, v in base.items() if k in _PREPROCESS_KEYS}
    if overlay:
        overlay_clean = {k: v for k, v in overlay.items() if k in _PREPROCESS_KEYS and v is not None}
        base_clean.update(overlay_clean)
    return base_clean

def merge_preprocess_kwargs(base: PreprocessCfg, **kwargs):
    return merge_preprocess_dict(base, kwargs)

@dataclass
class AugmentationCfg:
    scale: Tuple[float, float] = (0.9, 1.0)
    ratio: Optional[Tuple[float, float]] = None
    color_jitter: Optional[Union[float, Tuple[float, float, float], Tuple[float, float, float, float]]] = None
    re_prob: Optional[float] = None
    re_count: Optional[int] = None
    use_timm: bool = False
    color_jitter_prob: Optional[float] = None
    gray_scale_prob: Optional[float] = None

def _setup_size(size, error_msg):
    if isinstance(size, numbers.Number):
        return int(size), int(size)
    if isinstance(size, Sequence) and len(size) == 1:
        return size[0], size[0]
    if len(size) != 2:
        raise ValueError(error_msg)
    return size

class ResizeKeepRatio:
    def __init__(self, size, longest=0., interpolation=InterpolationMode.BICUBIC):
        if isinstance(size, (list, tuple)):
            self.size = tuple(size)
        else:
            self.size = (size, size)
        self.interpolation = interpolation
        self.longest = float(longest)
    
    def __call__(self, img):
        source_size = img.size[::-1]
        h, w = source_size
        target_h, target_w = self.size
        ratio_h, ratio_w = h / target_h, w / target_w
        ratio = max(ratio_h, ratio_w) * self.longest + min(ratio_h, ratio_w) * (1. - self.longest)
        size = [round(x / ratio) for x in source_size]
        return F.resize(img, size, self.interpolation)

def center_crop_or_pad(img: torch.Tensor, output_size: List[int], fill=0) -> torch.Tensor:
    if isinstance(output_size, numbers.Number):
        output_size = (int(output_size), int(output_size))
    elif isinstance(output_size, (tuple, list)) and len(output_size) == 1:
        output_size = (output_size[0], output_size[0])
    
    _, image_height, image_width = F.get_dimensions(img)
    crop_height, crop_width = output_size

    if crop_width > image_width or crop_height > image_height:
        padding_ltrb = [
            (crop_width - image_width) // 2 if crop_width > image_width else 0,
            (crop_height - image_height) // 2 if crop_height > image_height else 0,
            (crop_width - image_width + 1) // 2 if crop_width > image_width else 0,
            (crop_height - image_height + 1) // 2 if crop_height > image_height else 0,
        ]
        img = F.pad(img, padding_ltrb, fill=fill)
        _, image_height, image_width = F.get_dimensions(img)
        if crop_width == image_width and crop_height == image_height:
            return img

    crop_top = int(round((image_height - crop_height) / 2.0))
    crop_left = int(round((image_width - crop_width) / 2.0))
    return F.crop(img, crop_top, crop_left, crop_height, crop_width)

class CenterCropOrPad(torch.nn.Module):
    def __init__(self, size, fill=0):
        super().__init__()
        self.size = _setup_size(size, error_msg="Please provide only two dimensions (h, w) for size.")
        self.fill = fill

    def forward(self, img):
        return center_crop_or_pad(img, self.size, fill=self.fill)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(size={self.size})"

def _convert_to_rgb(image):
    return image.convert('RGB')

def image_transform(
        image_size: Union[int, Tuple[int, int]],
        is_train: bool,
        mean: Optional[Tuple[float, ...]] = None,
        std: Optional[Tuple[float, ...]] = None,
        resize_mode: Optional[str] = None,
        interpolation: Optional[str] = None,
        fill_color: int = 0,
        aug_cfg: Optional[Union[Dict[str, Any], AugmentationCfg, DictConfig]] = None,
):
    mean = mean or OPENAI_DATASET_MEAN
    if not isinstance(mean, (list, tuple)):
        mean = (mean,) * 3

    std = std or OPENAI_DATASET_STD
    if not isinstance(std, (list, tuple)):
        std = (std,) * 3

    interpolation = interpolation or 'bicubic'
    interpolation_mode = InterpolationMode.BILINEAR if interpolation == 'bilinear' else InterpolationMode.BICUBIC

    resize_mode = resize_mode or 'shortest'
    assert resize_mode in ('shortest', 'longest', 'squash')

    normalize = Normalize(mean=mean, std=std)

    if is_train:
        # CodeGuardian: FINAL FIX. This block is now robust to different config types.
        aug_cfg_dict = {}
        if isinstance(aug_cfg, AugmentationCfg):
            aug_cfg_dict = {k: v for k, v in asdict(aug_cfg).items() if v is not None}
        elif isinstance(aug_cfg, (dict, DictConfig)):
            aug_cfg_dict = OmegaConf.to_container(aug_cfg, resolve=True)

        use_timm = aug_cfg_dict.pop('use_timm', False) if aug_cfg_dict else False

        if use_timm:
            from timm.data import create_transform
            if isinstance(image_size, (tuple, list)):
                input_size = (3,) + tuple(image_size[-2:])
            else:
                input_size = (3, image_size, image_size)

            # Ensure necessary defaults for timm are set if not provided
            aug_cfg_dict.setdefault('color_jitter', None)
            aug_cfg_dict.setdefault('re_prob', 0.)
            
            # Note: timm's default hflip is 0.5, let's stick to it if not specified
            train_transform = create_transform(
                input_size=input_size,
                is_training=True,
                mean=mean,
                std=std,
                interpolation=interpolation,
                **aug_cfg_dict,
            )
        else:
            # Fallback to torchvision transforms
            aug_cfg_obj = AugmentationCfg(**aug_cfg_dict)
            
            # CRITICAL FIX: Ensure ratio has a default value if not provided
            ratio = aug_cfg_obj.ratio or (3. / 4., 4. / 3.)

            transforms = [
                RandomResizedCrop(
                    image_size,
                    scale=aug_cfg_obj.scale,
                    ratio=ratio,
                    interpolation=interpolation_mode,
                ),
                _convert_to_rgb,
                ToTensor(),
                normalize,
            ]
            train_transform = Compose(transforms)
        
        return train_transform
    else:
        # Validation transform logic (unchanged)
        if resize_mode == 'longest':
            transforms = [ResizeKeepRatio(image_size, interpolation=interpolation_mode, longest=1), CenterCropOrPad(image_size, fill=fill_color)]
        elif resize_mode == 'squash':
            if isinstance(image_size, int):
                image_size = (image_size, image_size)
            transforms = [Resize(image_size, interpolation=interpolation_mode)]
        else:
            assert resize_mode == 'shortest'
            if not isinstance(image_size, (tuple, list)):
                image_size = (image_size, image_size)
            if image_size[0] == image_size[1]:
                transforms = [Resize(image_size[0], interpolation=interpolation_mode)]
            else:
                transforms = [ResizeKeepRatio(image_size)]
            transforms += [CenterCrop(image_size)]

        transforms.extend([_convert_to_rgb, ToTensor(), normalize])
        return Compose(transforms)


def image_transform_v2(
        cfg: PreprocessCfg,
        is_train: bool,
        aug_cfg: Optional[Union[Dict[str, Any], AugmentationCfg]] = None,
):
    return image_transform(
        image_size=cfg.size,
        is_train=is_train,
        mean=cfg.mean,
        std=cfg.std,
        interpolation=cfg.interpolation,
        resize_mode=cfg.resize_mode,
        fill_color=cfg.fill_color,
        aug_cfg=aug_cfg,
    )
===== src/open_clip/factory.py =====
import json
import logging
import os
import re
import warnings
from copy import deepcopy
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, Union

import torch

from .convert import convert_state_dict
from .model import CLIP, CustomTextCLIP, convert_weights_to_lp, convert_to_custom_text_state_dict,\
    resize_pos_embed, get_cast_dtype, resize_text_pos_embed, set_model_preprocess_cfg
from .coca_model import CoCa
from .loss import ClipLoss, DistillClipLoss, CoCaLoss, SigLipLoss
from .pretrained import is_pretrained_cfg, get_pretrained_cfg, download_pretrained,\
    list_pretrained_tags_by_model, download_pretrained_from_hf
from .transform import image_transform_v2, AugmentationCfg, PreprocessCfg, merge_preprocess_dict, merge_preprocess_kwargs
from .tokenizer import HFTokenizer, SimpleTokenizer, SigLipTokenizer, DEFAULT_CONTEXT_LENGTH

HF_HUB_PREFIX = 'hf-hub:'
_MODEL_CONFIG_PATHS = [Path(__file__).parent / f"model_configs/"]
_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs


def _natural_key(string_):
    return [int(s) if s.isdigit() else s for s in re.split(r'(\d+)', string_.lower())]


def _rescan_model_configs():
    global _MODEL_CONFIGS

    config_ext = ('.json',)
    config_files = []
    for config_path in _MODEL_CONFIG_PATHS:
        if config_path.is_file() and config_path.suffix in config_ext:
            config_files.append(config_path)
        elif config_path.is_dir():
            for ext in config_ext:
                config_files.extend(config_path.glob(f'*{ext}'))

    for cf in config_files:
        with open(cf, 'r') as f:
            model_cfg = json.load(f)
            if all(a in model_cfg for a in ('embed_dim', 'vision_cfg', 'text_cfg')):
                _MODEL_CONFIGS[cf.stem] = model_cfg

    _MODEL_CONFIGS = {k: v for k, v in sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0]))}


_rescan_model_configs()  # initial populate of model config registry


def list_models():
    """ enumerate available model architectures based on config files """
    return list(_MODEL_CONFIGS.keys())


def add_model_config(path):
    """ add model config path or file and update registry """
    if not isinstance(path, Path):
        path = Path(path)
    _MODEL_CONFIG_PATHS.append(path)
    _rescan_model_configs()


# Define Schema Prefixes as constants
HF_HUB_PREFIX = 'hf-hub:'
LOCAL_DIR_PREFIX = 'local-dir:'

def parse_model_name(model_name: str) -> Tuple[Optional[str], str]:
    """
    Parses a model name string to identify a schema and the remaining identifier.

    Args:
        model_name: The model name string (e.g., 'ViT-B-32',
                    'hf-hub:org/repo', 'local-dir:/path/to/dir',
                    'local-dir:./relative/path').

    Returns:
        A tuple (schema, identifier):
          - schema (Optional[str]): 'hf-hub', 'local-dir', or None if no schema detected.
          - identifier (str): The part after the schema prefix, or the original
                              string if no schema was present. For 'local-dir',
                              this is the raw path string provided.
    Raises:
        ValueError: If a schema prefix is present but the identifier part is empty.
    """
    # Check for local directory schema first
    if model_name.startswith(LOCAL_DIR_PREFIX):
        # Extract the identifier (path) after the prefix
        identifier = model_name[len(LOCAL_DIR_PREFIX):]
        # Validate that the identifier (path) is not empty
        if not identifier:
            raise ValueError("Empty path specified after 'local-dir:' schema.")
        # Return the schema and the raw path identifier
        # Note: We don't resolve or fully validate the path here,
        #       that's left to the calling function (e.g., using os.path.isdir)
        return 'local-dir', identifier

    # Check for Hugging Face Hub schema
    elif model_name.startswith(HF_HUB_PREFIX):
        # Extract the identifier (HF Hub ID) after the prefix
        identifier = model_name[len(HF_HUB_PREFIX):]
        # Validate that the identifier is not empty
        if not identifier:
            raise ValueError("Empty identifier specified after 'hf-hub:' schema.")
        # Return the schema and the HF Hub ID
        return 'hf-hub', identifier

    # If neither schema prefix is found
    else:
        # No schema detected, return None for schema and the original string as identifier
        return None, model_name


def _get_hf_config(
        model_id: str,
        cache_dir: Optional[str] = None,
):
    """ Fetch model config from HuggingFace Hub.
    """
    config_path = download_pretrained_from_hf(
        model_id,
        filename='open_clip_config.json',
        cache_dir=cache_dir,
    )
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config


def get_model_config(model_name):
    """ Fetch model config from schema specified location or local library configs.
    """
    loc, model_id = parse_model_name(model_name)
    if loc == 'local-dir':
        local_path = Path(model_id) / 'open_clip_config.json'
        with open(local_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config.get('model_cfg', config)
    elif loc == 'hf-hub':
        config = _get_hf_config(model_id)
        return config.get('model_cfg', config)
    elif model_name in _MODEL_CONFIGS:
        return deepcopy(_MODEL_CONFIGS[model_name])
    else:
        return None


def load_state_dict(
        checkpoint_path: str,
        device='cpu',
        weights_only=True,
):
    # Check if safetensors or not and load weights accordingly
    if str(checkpoint_path).endswith(".safetensors"):
        from safetensors.torch import load_file
        checkpoint = load_file(checkpoint_path, device=device)
    else:
        try:
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=weights_only)
        except TypeError:
            checkpoint = torch.load(checkpoint_path, map_location=device)

    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    elif isinstance(checkpoint, torch.jit.ScriptModule):
        state_dict = checkpoint.state_dict()
        for key in ["input_resolution", "context_length", "vocab_size"]:
            state_dict.pop(key, None)
    else:
        state_dict = checkpoint
    if next(iter(state_dict.items()))[0].startswith('module'):
        state_dict = {k[7:]: v for k, v in state_dict.items()}
    return state_dict


def load_checkpoint(
        model: Union[CLIP, CustomTextCLIP],
        checkpoint_path: str,
        strict: bool = True,
        weights_only: bool = True,
        device='cpu',
):
    if Path(checkpoint_path).suffix in ('.npz', '.npy'):
        # Separate path loading numpy big_vision (SigLIP) weights
        from open_clip.convert import load_big_vision_weights
        load_big_vision_weights(model, checkpoint_path)
        return {}

    state_dict = load_state_dict(checkpoint_path, device=device, weights_only=weights_only)

    # Detect & convert 3rd party state_dicts -> open_clip
    state_dict = convert_state_dict(model, state_dict)

    # Detect old format and make compatible with new format
    if 'positional_embedding' in state_dict and not hasattr(model, 'positional_embedding'):
        state_dict = convert_to_custom_text_state_dict(state_dict)

    # correct if logit_scale differs in being scaler vs 1d param
    if 'logit_scale' in state_dict and model.logit_scale.ndim != state_dict['logit_scale'].ndim:
        state_dict['logit_scale'] = state_dict['logit_scale'].reshape(model.logit_scale.shape)

    # correct if logit_bias differs in being scaler vs 1d param
    if 'logit_bias' in state_dict and model.logit_bias.ndim != state_dict['logit_bias'].ndim:
        state_dict['logit_bias'] = state_dict['logit_bias'].reshape(model.logit_bias.shape)

    # If loading a non-SigLIP model for SigLIP training. See https://github.com/mlfoundations/open_clip/issues/712
    if 'logit_bias' not in state_dict and model.logit_bias is not None:
        state_dict["logit_bias"] = torch.zeros_like(state_dict["logit_scale"])

    # Certain text transformers no longer expect position_ids after transformers==4.31
    position_id_key = 'text.transformer.embeddings.position_ids'
    if position_id_key in state_dict and not hasattr(model, position_id_key):
        del state_dict[position_id_key]

    resize_pos_embed(state_dict, model)
    resize_text_pos_embed(state_dict, model)

    # Finally, load the massaged state_dict into model
    incompatible_keys = model.load_state_dict(state_dict, strict=strict)
    return incompatible_keys


def _find_checkpoint_in_dir(dir_path: Path) -> Optional[str]:
    checkpoints = list(dir_path.glob('*.safetensors')) + list(dir_path.glob('*.bin')) + list(dir_path.glob('*.pth'))
    if not checkpoints:
        return None
    checkpoints.sort()
    checkpoints.sort(key=lambda x: x.suffix == '.safetensors', reverse=True)
    preferred_order = [
        "open_clip_model.safetensors", "open_clip_pytorch_model.safetensors",
        "open_clip_pytorch_model.bin",  "open_clip_pytorch_model.pth",
        "model.safetensors", "pytorch_model.bin", "pytorch_model.pth", "model.pth"
    ]
    preferred_checkpoints = [c for c in checkpoints if c.name in preferred_order]
    if preferred_checkpoints:
        preferred_checkpoints.sort(key=lambda x: preferred_order.index(x.name))
        chosen = preferred_checkpoints[0]
        logging.info(f"Found preferred checkpoint file: {chosen.name} in {dir_path}")
        return str(chosen)
    chosen = checkpoints[0]
    logging.warning(
        f"Multiple checkpoints found in {dir_path}: {[c.name for c in checkpoints]}. Using '{chosen.name}'.")
    return str(chosen)


def create_model(
        model_name: str, # Can contain schemas 'hf-hub:' or 'local-dir:'
        pretrained: Optional[str] = None, # Used ONLY if model_name has NO schema
        load_weights: bool = True,
        precision: str = 'fp32',
        device: Union[str, torch.device] = 'cpu',
        jit: bool = False,
        force_quick_gelu: bool = False,
        force_custom_text: bool = False,
        force_patch_dropout: Optional[float] = None,
        force_image_size: Optional[Union[int, Tuple[int, int]]] = None,
        force_preprocess_cfg: Optional[Dict[str, Any]] = None,
        force_context_length: Optional[int] = None,
        pretrained_image: bool = False, # Load default base image weights (at creation, if no CLIP weights)
        pretrained_text: bool = True,  # Load default base text weights (at creation, if no CLIP weights) - NEW
        pretrained_image_path: Optional[str] = None, # Load specific image weights from file (after creation)
        pretrained_text_path: Optional[str] = None, # Load specific text weights from file (after creation)
        cache_dir: Optional[str] = None,
        output_dict: Optional[bool] = None,
        require_pretrained: bool = False,
        weights_only: bool = True,
        **model_kwargs,
) -> torch.nn.Module:
    """
    Creates and configures a contrastive vision-language model.

    `model_name` specifies architecture/config source:
      - 'ViT-B-32': Built-in model name. `pretrained` specifies CLIP weights source (tag or file path).
      - 'hf-hub:org/repo': Loads config/weights from HF Hub. `pretrained` is IGNORED.
      - 'local-dir:/path/to/folder': Loads config/weights from local dir. `pretrained` is IGNORED.

    Base tower weights loading controlled by `pretrained_image` and `pretrained_text` flags,
    only effective if no full CLIP checkpoint (`pretrained` or schema source) is loaded.

    Tower-specific weights can be loaded *after* creation via `pretrained_image_path`
    and `pretrained_text_path`.

    Args:
        model_name: Model identifier, potentially with schema ('hf-hub:', 'local-dir:').
        pretrained: Source for CLIP weights (tag or file path) ONLY if model_name has no schema.
        load_weights: Load the resolved pretrained weights if True, otherwise random init or tower overrides only.
        precision: Model precision ('fp32', 'fp16', 'bf16', ...).
        device: Device ('cpu', 'cuda', ...).
        jit: If True, JIT compile the model.
        force_quick_gelu: Force use of QuickGELU activation in model config.
        force_custom_text: Force use of custom text encoder architecture.
        force_patch_dropout: Override patch dropout value in model config.
        force_image_size: Override image size in model config.
        force_preprocess_cfg: Dict to override specific FINAL preprocessing parameters.
        force_context_length: Override context length in model config.
        pretrained_image: Load default base weights for image tower at creation if no CLIP weights loaded.
        pretrained_text: Load default base weights for text tower at creation if no CLIP weights loaded (default: True).
        pretrained_image_path: Path to load weights specifically into image tower after creation.
        pretrained_text_path: Path to load weights specifically into text tower after creation.
        cache_dir: Cache directory for downloads.
        output_dict: If True and model supports it, return dict output.
        require_pretrained: Raise error if no `pretrained` CLIP weights loaded when required.
        weights_only: Use weights_only=True for torch.load (safer).
        **model_kwargs: Additional keyword arguments for model constructor (highest override priority).

    Returns:
        The created model instance.
    """
    schema, identifier = parse_model_name(model_name)
    if 'pretrained_hf' in model_kwargs:
        # for backwards compat, override pretrained_text
        pretrained_text = model_kwargs.pop('pretrained_hf')
    if isinstance(device, str):
        device = torch.device(device)

    model_cfg = None
    preprocess_cfg = asdict(PreprocessCfg())  # Populate with defaults
    checkpoint_path = None # Final path for full CLIP weights
    pretrained_cfg_for_tag = None # Store tag config if pretrained is a tag and schema is None

    logging.info(f"Parsing model identifier. Schema: {schema}, Identifier: {identifier}")
    if schema and pretrained:
        logging.warning(f"Ignoring `pretrained='{pretrained}'` because `model_name` has '{schema}' schema.")
        pretrained = None  # Nullify pretrained as it's ignored

    # Handle schemas first - these ignore the `pretrained` argument
    if schema == 'local-dir':
        # Handle local directory schema
        local_path = Path(identifier)
        if not local_path.is_dir():
            raise FileNotFoundError(f"Directory specified via 'local-dir:' schema not found: {local_path}")

        local_config_path = local_path / 'open_clip_config.json'
        logging.info(f"Attempting to load config from local dir: {local_config_path}")
        if local_config_path.is_file():
            try:
                # Try loading and parsing the JSON config
                with open(local_config_path, 'r', encoding='utf-8') as f:
                    local_json_config = json.load(f)
                # Check if the required 'model_cfg' key is present
                if 'model_cfg' in local_json_config:
                    # Load model config and merge preprocess config
                    model_cfg = local_json_config['model_cfg']
                    preprocess_cfg = merge_preprocess_dict(preprocess_cfg, local_json_config.get('preprocess_cfg', {}))
                    logging.info(f"Loaded model config and preprocess from: {local_config_path}")
                    # Look for weights checkpoint in the same directory
                    checkpoint_path = _find_checkpoint_in_dir(local_path)
                    if checkpoint_path:
                        logging.info(f"Found CLIP weights in local folder: {checkpoint_path}")
                    else:
                        logging.warning(f"Local config loaded, but no CLIP weights found in {local_path}")
                else:
                    # Config file exists but lacks the necessary key
                    raise ValueError(f"Local config {local_config_path} missing 'model_cfg'.")
            except Exception as e:
                # Handle JSON parsing errors or other exceptions during config load
                raise ValueError(f"Could not load valid config from specified 'local-dir:{identifier}': {e}") from e
        else:
            # Directory exists but the config file is missing
            raise FileNotFoundError(f"'local-dir:' specified, but config file missing: {local_config_path}")

    elif schema == 'hf-hub':
        # Handle Hugging Face Hub schema
        model_id = identifier
        logging.info(f"Attempting to load config from HF Hub: {model_id}")
        try:
            # Fetch configuration from Hugging Face Hub
            hf_config = _get_hf_config(model_id, cache_dir=cache_dir)
            if 'model_cfg' not in hf_config:
                raise RuntimeError(f"'model_cfg' not found in config from {model_id}")
            # Load model config and merge preprocess config
            model_cfg = hf_config['model_cfg']
            preprocess_cfg = merge_preprocess_dict(preprocess_cfg, hf_config.get('preprocess_cfg', {}))
            logging.info(f"Loaded model config from HF Hub: {model_id}")
            # Attempt find default weights file from the Hub repo
            try:
                checkpoint_path = download_pretrained_from_hf(model_id, cache_dir=cache_dir)
                logging.info(f"Found default weights file on HF Hub: {checkpoint_path}")
            except Exception as e_weights:
                # Log warning if weights download fails, but proceed (might only need config)
                logging.warning(f"Could not find/download default weights on HF Hub for {model_id}: {e_weights}")
        except Exception as e_config:
            # Handle errors during config fetching from HF Hub
            raise RuntimeError(f"Failed initial config/weights load from HF Hub {model_id}: {e_config}") from e_config

    # No Schema Prefix - Use built-in name + pretrained arg (tag or file)
    elif schema is None:
        # Handle model names without schema prefix
        # Use identifier (original model_name) and clean it for lookup
        model_name_cleaned = identifier.replace('/', '-')

        # Get base config from built-in name using the cleaned identifier
        model_cfg = get_model_config(model_name_cleaned)
        if model_cfg is None:
            # Raise error if no matching built-in config found
            raise RuntimeError(
                f"Model config for '{model_name_cleaned}' not found in built-ins. Available: {list_models()}")
        logging.info(f"Loaded built-in {model_name_cleaned} model config.")

        # Determine checkpoint path and update preprocess_cfg based on `pretrained` arg (tag or file)
        if pretrained:
            # Check if `pretrained` is a known tag
            pretrained_cfg_for_tag = get_pretrained_cfg(model_name_cleaned, pretrained)
            if pretrained_cfg_for_tag:
                try:
                    # Download weights associated with the tag
                    checkpoint_path = download_pretrained(pretrained_cfg_for_tag, cache_dir=cache_dir)
                    preprocess_cfg = merge_preprocess_dict(preprocess_cfg, pretrained_cfg_for_tag)
                    # QuickGELU compatibility check will happen in after force overrides
                except Exception as e:
                    logging.error(f"Failed to download weights for tag '{pretrained}': {e}")
                    raise RuntimeError(f"Failed to download weights for tag '{pretrained}': {e}")
            elif os.path.isfile(pretrained):
                # Handle pretrained file path
                logging.info(f"`pretrained` specifies file path: {pretrained}")
                checkpoint_path = pretrained
            else:
                logging.error(
                    f"Pretrained tag or path ({pretrained}) for '{model_name_cleaned}' not found. "
                    f"Available tags: {list_pretrained_tags_by_model(model_name_cleaned)}"
                )
                raise RuntimeError(f"Pretrained value '{pretrained}' is not a known tag or valid file path")

    # Apply model config overrides
    if model_cfg is None:
        raise RuntimeError("Model configuration could not be determined after Stage 1.")
    text_cfg = model_cfg['text_cfg']
    vision_cfg = model_cfg['vision_cfg']
    if force_quick_gelu:
        model_cfg["quick_gelu"] = True
    if force_patch_dropout is not None:
        vision_cfg["patch_dropout"] = force_patch_dropout
    if force_image_size is not None:
        vision_cfg["image_size"] = force_image_size
    if force_context_length is not None:
        text_cfg["context_length"] = force_context_length

    # Check compatibility (e.g., QuickGELU warning for tags)
    if schema is None and pretrained_cfg_for_tag:
        # Only perform check if config came from built-in and weights from a tag
        model_quick_gelu = model_cfg.get('quick_gelu', False) # Check the potentially overridden value
        tag_quick_gelu = pretrained_cfg_for_tag.get('quick_gelu', False)
        if tag_quick_gelu != model_quick_gelu:
            # Warn if the final model config's GELU setting mismatches the tag's training setting
             warnings.warn(
                 f"QuickGELU mismatch between final model config (quick_gelu={model_quick_gelu}) "
                 f"and pretrained tag '{pretrained}' (quick_gelu={tag_quick_gelu}).",
                 UserWarning
             )

    # Decide whether to use the checkpoint path based on load_weights
    if checkpoint_path is not None:
        if not load_weights:
            logging.info(
                f"Potential checkpoint path '{checkpoint_path}' found, but skipping assignment due to load_weights=False.")
            checkpoint_path = None
    else:
        logging.info("No potential checkpoint path found from config source or pretrained arg.")

    # Set default base weight loading flags for image and text towers
    # Only load base pretrained weights if other weights will not be loaded into respective towers
    enable_default_image_weights = pretrained_image and pretrained_image_path is None and checkpoint_path is None
    enable_default_text_weights = pretrained_text and pretrained_text_path is None and checkpoint_path is None
    is_timm_model = 'timm_model_name' in model_cfg.get("vision_cfg", {})
    is_hf_text_model = 'hf_model_name' in model_cfg.get('text_cfg', {})
    if is_timm_model:
        vision_cfg['timm_model_pretrained'] = enable_default_image_weights
    else:
        enable_default_image_weights = False  # for accurate logging
    if is_hf_text_model:
        text_cfg['hf_model_pretrained'] = enable_default_text_weights
    else:
        enable_default_text_weights = False  # for accurate logging

    # Determine model class (CLIP, CustomTextCLIP, CoCa)
    custom_text = model_cfg.pop('custom_text', False) or force_custom_text or is_hf_text_model
    if custom_text:
        # Use CustomTextCLIP (or CoCa if multimodal_cfg is present)
        if "multimodal_cfg" in model_cfg:
            model_class = CoCa
        else:
            model_class = CustomTextCLIP
    else:
        # Default to standard CLIP
        model_class = CLIP

    # Apply final **kwargs overrides (highest priority) to a copy of model_cfg
    final_model_cfg = deepcopy(model_cfg)
    final_model_cfg.update(model_kwargs)

    # Get casting dtype based on precision argument
    cast_dtype = get_cast_dtype(precision)

    # Instantiate the model
    logging.info(f"Instantiating model architecture: {model_class.__name__}")
    model = model_class(**final_model_cfg, cast_dtype=cast_dtype)
    _set_model_device_and_precision(model, device, precision, is_timm_model)

    # Load Full Pretrained CLIP Weights (if path exists)
    pretrained_loaded = False
    if checkpoint_path:
        logging.info(f'Loading full pretrained weights from: {checkpoint_path}')
        # Use the load_checkpoint helper which handles state dict loading, conversions, etc.
        # Use strict=True by default for full model loading to catch mismatches.
        load_checkpoint(
            model,
            checkpoint_path,
            strict=True,
            weights_only=weights_only,
            device='cpu' # Load to CPU first
        )
        pretrained_loaded = True

    # Load tower-specific weights (image and text), after the full CLIP checkpoint, potentially overwriting parts.
    pretrained_image_loaded = False # Track if specific image weights loaded
    if pretrained_image_path:
        if os.path.isfile(pretrained_image_path):
            logging.info(f"Attempting to load image tower weights from: {pretrained_image_path}")
            try:
                # Load the state dict from the file
                image_state_dict = load_state_dict(
                    pretrained_image_path,
                    device='cpu',
                    weights_only=weights_only
                )
                # Check if model has the 'visual' attribute
                if hasattr(model, 'visual'):
                    # Load into the visual tower, use strict=False for flexibility
                    incompatible_keys = model.visual.load_state_dict(image_state_dict, strict=False)
                    logging.info(
                        f"Loaded image tower weights from {pretrained_image_path}. Incompatible keys: {incompatible_keys}")
                    pretrained_image_loaded = True # Mark specific image weights as loaded
                else:
                    # Model structure doesn't match expectation
                    logging.warning(
                        f"Model does not have a 'visual' attribute, cannot load image tower weights from {pretrained_image_path}")
            except Exception as e:
                # Handle errors during image tower weight loading
                logging.error(f"Error loading image tower weights from {pretrained_image_path}: {e}")
        else:
            # Path provided is not a valid file
            logging.warning(f"Invalid file path specified for pretrained_image_path: {pretrained_image_path}")

    pretrained_text_loaded = False # Track if specific text weights loaded
    if pretrained_text_path:
        if os.path.isfile(pretrained_text_path):
            logging.info(f"Attempting to load text tower weights from: {pretrained_text_path}")
            try:
                # Load the state dict from the file
                text_state_dict = load_state_dict(
                    pretrained_text_path,
                    device='cpu',
                    weights_only=weights_only
                )
                # Safely get the text attribute (usually 'text', but could be different)
                text_module = getattr(model, 'text', model)
                if text_module is not None:
                    # Load into the text tower, use strict=False for flexibility
                    incompatible_keys = text_module.load_state_dict(text_state_dict, strict=False)
                    logging.info(f"Loaded text tower weights from {pretrained_text_path}. Incompatible keys: {incompatible_keys}")
                    pretrained_text_loaded = True # Mark specific text weights as loaded
                else:
                    # Model structure doesn't match expectation
                    logging.warning(f"Model does not have a standard 'text' attribute, cannot load text tower weights from {pretrained_text_path}")
            except Exception as e:
                # Handle errors during text tower weight loading
                logging.error(f"Error loading text tower weights from {pretrained_text_path}: {e}")
        else:
            # Path provided is not a valid file
            logging.warning(f"Invalid file path specified for pretrained_text_path: {pretrained_text_path}")

    partially_loaded = enable_default_text_weights or enable_default_image_weights \
        or pretrained_image_loaded or pretrained_text_loaded
    if require_pretrained and not pretrained_loaded:
         # If CLIP weights were required but failed to load, raise an error.
         # Loading tower-specific weights does not satisfy `require_pretrained`.
         raise RuntimeError(
             f"Required pretrained weights (`model_name='{model_name}', pretrained='{pretrained}'`) could not be loaded. "
         )
    elif not pretrained_loaded and partially_loaded:
         # Some tower weights loaded
         logging.warning(f"Model {model_name} initialized partially.")
    elif not pretrained_loaded and not partially_loaded:
         # Absolutely no weights were loaded from any source
         logging.warning(f"No pretrained weights loaded for model '{model_name}'. Model initialized randomly.")

    if output_dict and hasattr(model, "output_dict"):
        # Enable dictionary output if model supports it
        model.output_dict = True

    # If force_image_size was specified and we have a timm model, call set_input_size after loading weights
    if force_image_size is not None and is_timm_model and hasattr(model.visual, 'set_input_size'):
        logging.info(f"Calling set_input_size({force_image_size}) on timm vision model.")
        model.visual.set_input_size(force_image_size)

    if jit:
        logging.info("Attempting JIT scripting...")
        try:
            model = torch.jit.script(model)
            logging.info("JIT scripting successful.")
        except Exception as e:
            logging.warning(f"JIT scripting failed: {e}. Returning non-JIT model.")

    # Prepare and set final preprocessing configuration on the model
    final_preprocess_cfg = deepcopy(preprocess_cfg) # Start with config determined earlier
    # Ensure image_size in preprocess config matches the actual model's visual component size, if possible
    visual_module = getattr(model, 'visual', None)
    if visual_module is not None and hasattr(visual_module, 'image_size'):
        # Update preprocess size from the instantiated visual module
         final_preprocess_cfg['size'] = visual_module.image_size
    # Apply force_preprocess_cfg overrides (highest priority for preprocessing)
    final_preprocess_cfg = merge_preprocess_dict(final_preprocess_cfg, force_preprocess_cfg or {})

    # Attach the final config to the model
    set_model_preprocess_cfg(model, final_preprocess_cfg)
    logging.info(f"Final image preprocessing configuration set: {final_preprocess_cfg}")

    # Log completion and return the configured model
    logging.info(f"Model {model_name} creation process complete.")
    return model


def get_tokenizer(
        model_name: str = '',
        context_length: Optional[int] = None,
        cache_dir: Optional[str] = None,
        **kwargs, # Additional tokenizer kwargs passed to constructor
):
    """
    Gets the appropriate tokenizer based on the model identifier schema or name.

    `model_name` can specify source via schema:
      - 'ViT-B-32': Looks up built-in config to determine tokenizer type.
      - 'hf-hub:org/repo': Loads config from HF Hub to determine tokenizer type.
      - 'local-dir:/path/to/folder': Loads config from local dir to determine tokenizer type.
    """
    schema, identifier = parse_model_name(model_name)

    config = {} # Stores the loaded model_cfg relevant section (usually text_cfg)
    local_dir_path = None # Store path if schema is local-dir to resolve relative paths
    hf_fallback_id = None

    # Determine Configuration Source based on Schema
    logging.info(f"Parsing tokenizer identifier. Schema: {schema}, Identifier: {identifier}")

    if schema == 'local-dir':
        # Handle local directory schema
        local_dir_path = Path(identifier) # Store the path for later use
        if not local_dir_path.is_dir():
            raise FileNotFoundError(f"Directory specified via 'local-dir:' schema not found at {local_dir_path}")
        local_config_path = local_dir_path / 'open_clip_config.json'
        logging.info(f"Attempting to load config from local-dir: {local_config_path}")
        if local_config_path.is_file():
            try:
                # Load and parse the JSON config
                with open(local_config_path, 'r', encoding='utf-8') as f:
                    local_json_config = json.load(f)
                if 'model_cfg' in local_json_config:
                    config = local_json_config['model_cfg']
                else:
                    raise ValueError(f"Local config {local_config_path} missing 'model_cfg'.")
            except Exception as e:
                raise ValueError(f"Could not load valid config for 'local-dir:{identifier}' ({e}).") from e
        else:
             raise FileNotFoundError(f"'local-dir:' specified, but config file missing: {local_config_path}")

    elif schema == 'hf-hub':
        # Handle Hugging Face Hub schema
        model_id = identifier
        logging.info(f"Attempting to load config from hf-hub:{model_id}")
        config_err = ''
        try:
            # Fetch config from HF Hub
            hf_config = _get_hf_config(model_id, cache_dir=cache_dir)
            config = hf_config.get('model_cfg', None)
            if not config:
                config_err = 'model_cfg key not found'
        except Exception as e:
            config_err = str(e)
        if not config:
            hf_fallback_id = model_id
            config = {}
            logging.warning(
                f"Could not load config from hf-hub:{model_id} ({config_err})."
                f"Falling back to using model_id for tokenizer.")

    elif schema is None and identifier:
        # Try built-in config lookup using the identifier (original model_name)
        logging.info(f"Attempting to load config from built-in: {identifier}")
        config = get_model_config(identifier)

    # Check if config determination failed completely (should only be possible if initial schema parsing failed badly)
    if config is None:
        logging.warning(f"Model configuration not found, returning default SimpleTokenizer.")
        return SimpleTokenizer(context_length=context_length or DEFAULT_CONTEXT_LENGTH, **kwargs)

    # Safely access text_cfg even if config is {} (from non-builtin name case)
    text_config = config.get('text_cfg', {})

    # Resolve context length: argument > config > default
    if context_length is None:
        # Use context_length from text_cfg if available, otherwise default
        context_length = text_config.get('context_length', DEFAULT_CONTEXT_LENGTH)

    # Merge tokenizer kwargs: function kwargs override config kwargs
    tokenizer_kwargs = text_config.get('tokenizer_kwargs', {}) # Start with config kwargs
    tokenizer_kwargs.update(kwargs) # Apply caller kwargs, overriding config ones

    # Get the specified HF tokenizer name from config, if any
    hf_tokenizer_name = text_config.get('hf_tokenizer_name', '')
    if not hf_tokenizer_name and hf_fallback_id:
        hf_tokenizer_name = hf_fallback_id

    if hf_tokenizer_name:
        # If 'hf_tokenizer_name' key exists in text_cfg (even if empty string): Use HFTokenizer.
        if schema == 'local-dir':
            # If config came from local-dir, ALWAYS use the local dir path for HFTokenizer.
            # This assumes the tokenizer files are inside that directory.
            tokenizer_source = local_dir_path
        else:
            tokenizer_source = hf_tokenizer_name
        tokenizer_mode = text_config.get('tokenizer_mode', None)

        logging.info(f"Using HFTokenizer with source: '{tokenizer_source}', mode: '{tokenizer_mode}'")
        tokenizer = HFTokenizer(
            tokenizer_source,
            context_length=context_length,
            cache_dir=cache_dir,
            tokenizer_mode=tokenizer_mode,
            **tokenizer_kwargs,
        )

    elif schema is None and 'siglip' in identifier.lower():
        # Check for SigLIP naming convention ONLY if no schema was present AND no hf_tokenizer_name found
        # Avoids misinterpreting 'local-dir:/path/with/siglip/in/name'
        tn_variant = 'gemma' if 'siglip2' in identifier.lower() else 'mc4' if 'i18n' in identifier.lower() else 'c4-en'
        logging.info(f"Using SigLipTokenizer variant: {tn_variant}")
        tokenizer = SigLipTokenizer(
            tn_variant,
            context_length=context_length,
        )
    else:
        # Default to SimpleTokenizer if no HF specified and not SigLIP name match
        logging.info("Using default SimpleTokenizer.")
        tokenizer = SimpleTokenizer(
            context_length=context_length,
            **tokenizer_kwargs,
        )

    return tokenizer


def _set_model_device_and_precision(
        model: torch.nn.Module,
        device: torch.device,
        precision: str,
        is_timm_model: bool = False
):
    if precision in ("fp16", "bf16"):
        dtype = torch.float16 if 'fp16' in precision else torch.bfloat16
        # manual mixed precision that matches original OpenAI behaviour
        if is_timm_model:
            from .transformer import LayerNormFp32
            # FIXME this is a bit janky, create timm based model in low-precision and
            # then cast only LayerNormFp32 instances back to float32 so they don't break.
            # Why? The convert_weights_to_lp fn only works with native models.
            model.to(device=device, dtype=dtype)

            def _convert_ln(m):
                if isinstance(m, LayerNormFp32):
                    m.weight.data = m.weight.data.to(torch.float32)
                    m.bias.data = m.bias.data.to(torch.float32)

            model.apply(_convert_ln)
        else:
            model.to(device=device)
            convert_weights_to_lp(model, dtype=dtype)
    elif precision in ("pure_fp16", "pure_bf16"):
        dtype = torch.float16 if 'fp16' in precision else torch.bfloat16
        model.to(device=device, dtype=dtype)
    else:
        model.to(device=device)


def create_loss(args):
    if args.distill:
        return DistillClipLoss(
            local_loss=args.local_loss,
            gather_with_grad=args.gather_with_grad,
            cache_labels=True,
            rank=args.rank,
            world_size=args.world_size,
            use_horovod=args.horovod,
        )
    elif "coca" in args.model.lower():
        return CoCaLoss(
            caption_loss_weight=args.coca_caption_loss_weight,
            clip_loss_weight=args.coca_contrastive_loss_weight,
            local_loss=args.local_loss,
            gather_with_grad=args.gather_with_grad,
            cache_labels=True,
            rank=args.rank,
            world_size=args.world_size,
            use_horovod=args.horovod,
        )
    elif args.siglip:
        assert not args.horovod, "Horovod not currently supported for SigLip"
        return SigLipLoss(
            rank=args.rank,
            world_size=args.world_size,
            dist_impl=args.loss_dist_impl,  # siglip has multiple distributed implementations to choose from
        )

    return ClipLoss(
        local_loss=args.local_loss,
        gather_with_grad=args.gather_with_grad,
        cache_labels=True,
        rank=args.rank,
        world_size=args.world_size,
        use_horovod=args.horovod,
    )


def create_model_and_transforms(
        model_name: str,
        pretrained: Optional[str] = None,
        load_weights: bool = True,
        precision: str = 'fp32',
        device: Union[str, torch.device] = 'cpu',
        jit: bool = False,
        force_quick_gelu: bool = False,
        force_custom_text: bool = False,
        force_patch_dropout: Optional[float] = None,
        force_image_size: Optional[Union[int, Tuple[int, int]]] = None,
        force_context_length: Optional[int] = None,
        image_mean: Optional[Tuple[float, ...]] = None,
        image_std: Optional[Tuple[float, ...]] = None,
        image_interpolation: Optional[str] = None,
        image_resize_mode: Optional[str] = None,  # only effective for inference
        aug_cfg: Optional[Union[Dict[str, Any], AugmentationCfg]] = None,
        pretrained_image: bool = False,
        pretrained_text: bool = True,
        pretrained_image_path: Optional[str] = None,
        pretrained_text_path: Optional[str] = None,
        cache_dir: Optional[str] = None,
        output_dict: Optional[bool] = None,
        weights_only: bool = True,
        **model_kwargs,
):
    """
    Creates a contrastive vision-language model along with preprocessing transforms for training and validation.

    This function combines model creation with the generation of appropriate image preprocessing pipelines,
    making it convenient for training workflows where both model and transforms are needed.

    `model_name` specifies architecture/config source:
      - 'ViT-B-32': Built-in model name. `pretrained` specifies CLIP weights source (tag or file path).
      - 'hf-hub:org/repo': Loads config/weights from HF Hub. `pretrained` is IGNORED.
      - 'local-dir:/path/to/folder': Loads config/weights from local dir. `pretrained` is IGNORED.

    The preprocessing transforms are automatically configured based on the model's requirements,
    with separate pipelines for training (with augmentation) and validation (without augmentation).

    Args:
        model_name: Model identifier, potentially with schema ('hf-hub:', 'local-dir:').
        pretrained: Source for CLIP weights (tag or file path) ONLY if model_name has no schema.
        load_weights: Load the resolved pretrained weights if True, otherwise random init or tower overrides only.
        precision: Model precision ('fp32', 'fp16', 'bf16', ...).
        device: Device ('cpu', 'cuda', ...).
        jit: If True, JIT compile the model.
        force_quick_gelu: Force use of QuickGELU activation in model config.
        force_custom_text: Force use of custom text encoder architecture.
        force_patch_dropout: Override patch dropout value in model config.
        force_image_size: Override image size in model config.
        force_context_length: Override context length in model config.
        image_mean: Override default image normalization mean values (per channel).
        image_std: Override default image normalization std values (per channel).
        image_interpolation: Override default interpolation method for image resizing.
        image_resize_mode: Override resize mode for inference preprocessing ('squash', 'longest', 'shortest').
        aug_cfg: Augmentation configuration for training transforms. Can be dict or AugmentationCfg object.
                 Controls random crop, color jitter, etc. If None, uses model defaults.
        pretrained_image: Load default (timm) base weights for image tower at creation if no CLIP weights loaded.
        pretrained_text: Load default (hf) base weights for text tower at creation if no CLIP weights loaded.
        pretrained_image_path: Path to load weights specifically into image tower after creation.
        pretrained_text_path: Path to load weights specifically into text tower after creation.
        cache_dir: Cache directory for downloads.
        output_dict: If True and model supports it, return dict output.
        weights_only: Use weights_only=True for torch.load (safer).
        **model_kwargs: Additional keyword arguments for model constructor (highest override priority).

    Returns:
        Tuple[torch.nn.Module, Callable, Callable]: A tuple containing:
            - model: The created model instance
            - preprocess_train: Image preprocessing transform for training (includes augmentation)
            - preprocess_val: Image preprocessing transform for validation/inference (no augmentation)

    Example:
        >>> # Basic usage with built-in model
        >>> model, train_transform, val_transform = create_model_and_transforms('ViT-B-32', pretrained='openai')
        >>>
        >>> # With custom augmentation
        >>> aug_cfg = {'scale': (0.9, 1.0), 'ratio': (1.0, 1.0)}
        >>> model, train_transform, val_transform = create_model_and_transforms(
        ...     'ViT-L-14',
        ...     pretrained='datacomp_xl_s13b_b90k',
        ...     aug_cfg=aug_cfg
        ... )
        >>>
        >>> # From Hugging Face Hub
        >>> model, train_transform, val_transform = create_model_and_transforms('hf-hub:org/model-repo')

    Note:
        The training transform includes data augmentation based on `aug_cfg`, while the validation
        transform performs only the necessary preprocessing (resize, center crop, normalize) without
        any random augmentation.
    """
    force_preprocess_cfg = merge_preprocess_kwargs(
        {},
        mean=image_mean,
        std=image_std,
        interpolation=image_interpolation,
        resize_mode=image_resize_mode,
    )

    model = create_model(
        model_name,
        pretrained,
        load_weights=load_weights,
        precision=precision,
        device=device,
        jit=jit,
        force_quick_gelu=force_quick_gelu,
        force_custom_text=force_custom_text,
        force_patch_dropout=force_patch_dropout,
        force_image_size=force_image_size,
        force_preprocess_cfg=force_preprocess_cfg,
        force_context_length=force_context_length,
        pretrained_image=pretrained_image,
        pretrained_text=pretrained_text,
        pretrained_image_path=pretrained_image_path,
        pretrained_text_path=pretrained_text_path,
        cache_dir=cache_dir,
        output_dict=output_dict,
        weights_only=weights_only,
        **model_kwargs,
    )

    pp_cfg = PreprocessCfg(**model.visual.preprocess_cfg)

    preprocess_train = image_transform_v2(
        pp_cfg,
        is_train=True,
        aug_cfg=aug_cfg,
    )
    preprocess_val = image_transform_v2(
        pp_cfg,
        is_train=False,
    )

    return model, preprocess_train, preprocess_val


def create_model_from_pretrained(
        model_name: str,
        pretrained: Optional[str] = None,
        precision: str = 'fp32',
        device: Union[str, torch.device] = 'cpu',
        jit: bool = False,
        force_quick_gelu: bool = False,
        force_custom_text: bool = False,
        force_image_size: Optional[Union[int, Tuple[int, int]]] = None,
        force_context_length: Optional[int] = None,
        image_mean: Optional[Tuple[float, ...]] = None,
        image_std: Optional[Tuple[float, ...]] = None,
        image_interpolation: Optional[str] = None,
        image_resize_mode: Optional[str] = None,  # only effective for inference
        return_transform: bool = True,
        cache_dir: Optional[str] = None,
        weights_only: bool = True,
        **model_kwargs,
):
    """
    Creates a contrastive vision-language model from pretrained weights with optional preprocessing transform.

    This function is a convenience wrapper around `create_model` that enforces loading of pretrained weights
    (require_pretrained=True) and optionally returns the appropriate preprocessing transform for inference.
    It's designed for use cases where a pretrained model is required, such as feature extraction,
    zero-shot classification, or fine-tuning.

    `model_name` specifies architecture/config source:
      - 'ViT-B-32': Built-in model name. `pretrained` specifies CLIP weights source (tag or file path).
      - 'hf-hub:org/repo': Loads config/weights from HF Hub. `pretrained` is IGNORED.
      - 'local-dir:/path/to/folder': Loads config/weights from local dir. `pretrained` is IGNORED.

    Unlike `create_model`, this function will raise an error if pretrained weights cannot be loaded.

    Args:
        model_name: Model identifier, potentially with schema ('hf-hub:', 'local-dir:').
        pretrained: Source for CLIP weights (tag or file path) ONLY if model_name has no schema.
                   If None and schema requires it, will raise an error.
        precision: Model precision ('fp32', 'fp16', 'bf16', ...).
        device: Device ('cpu', 'cuda', ...).
        jit: If True, JIT compile the model.
        force_quick_gelu: Force use of QuickGELU activation in model config.
        force_custom_text: Force use of custom text encoder architecture.
        force_image_size: Override image size in model config. Useful for using models at different resolutions.
        force_context_length: Override context length in model config.
        image_mean: Override default image normalization mean values (per channel).
        image_std: Override default image normalization std values (per channel).
        image_interpolation: Override default interpolation method for image resizing ('bicubic', 'bilinear', 'nearest').
        image_resize_mode: Override resize mode for inference preprocessing ('squash', 'longest', 'shortest').
            Only affects the returned preprocessing transform, not training.
        return_transform: If True, returns (model, preprocess). If False, returns only model.
        cache_dir: Cache directory for downloads.
        weights_only: Use weights_only=True for torch.load (safer).
        **model_kwargs: Additional keyword arguments for model constructor (highest override priority).

    Returns:
        Union[torch.nn.Module, Tuple[torch.nn.Module, Callable]]:
            - If return_transform=False: Just the model instance
            - If return_transform=True: Tuple of (model, preprocess) where preprocess is the
              inference preprocessing transform

    Raises:
        RuntimeError: If pretrained weights are required but cannot be loaded.

    Example:
        >>> # Load model with preprocessing
        >>> model, preprocess = create_model_from_pretrained('ViT-B-32', pretrained='openai')
        >>>
        >>> # Load model without preprocessing (e.g., when using custom preprocessing)
        >>> model = create_model_from_pretrained('ViT-B-32', pretrained='openai', return_transform=False)
        >>>
        >>> # Load from Hugging Face Hub
        >>> model, preprocess = create_model_from_pretrained('hf-hub:laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K')
        >>>
        >>> # Load with custom image size
        >>> model, preprocess = create_model_from_pretrained(
        ...     'ViT-L-14',
        ...     pretrained='openai',
        ...     force_image_size=336
        ... )

    Note:
        This function always requires pretrained weights to be available and loaded successfully.
        For cases where you want to create a model without pretrained weights or with only
        partial weight loading, use `create_model` or `create_model_and_transforms` instead.
    """
    force_preprocess_cfg = merge_preprocess_kwargs(
        {},
        mean=image_mean,
        std=image_std,
        interpolation=image_interpolation,
        resize_mode=image_resize_mode,
    )

    model = create_model(
        model_name,
        pretrained,
        precision=precision,
        device=device,
        jit=jit,
        force_quick_gelu=force_quick_gelu,
        force_custom_text=force_custom_text,
        force_image_size=force_image_size,
        force_preprocess_cfg=force_preprocess_cfg,
        force_context_length=force_context_length,
        cache_dir=cache_dir,
        require_pretrained=True,
        weights_only=weights_only,
        **model_kwargs,
    )

    if not return_transform:
        return model

    preprocess = image_transform_v2(
        PreprocessCfg(**model.visual.preprocess_cfg),
        is_train=False,
    )

    return model, preprocess

===== src/open_clip/coca_model.py =====
from typing import Dict, List, Optional, Union

import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
from dataclasses import dataclass

from .transformer import (
    LayerNormFp32,
    LayerNorm,
    QuickGELU,
    MultimodalTransformer,
)
from .model import CLIPTextCfg, CLIPVisionCfg, _build_vision_tower, _build_text_tower

try:
    from transformers import (
        BeamSearchScorer,
        LogitsProcessorList,
        TopPLogitsWarper,
        TopKLogitsWarper,
        RepetitionPenaltyLogitsProcessor,
        MinLengthLogitsProcessor,
        MaxLengthCriteria,
        StopStringCriteria,
        EosTokenCriteria,
        StoppingCriteriaList
    )

    GENERATION_TYPES = {
        "top_k": TopKLogitsWarper,
        "top_p": TopPLogitsWarper,
        "beam_search": "beam_search"
    }
    _has_transformers = True
except ImportError as e:
    GENERATION_TYPES = {
        "top_k": None,
        "top_p": None,
        "beam_search": "beam_search"
    }
    _has_transformers = False


@dataclass
class MultimodalCfg(CLIPTextCfg):
    mlp_ratio: int = 4
    dim_head: int = 64
    heads: int = 8
    n_queries: int = 256
    attn_pooler_heads: int = 8


def _build_text_decoder_tower(
        embed_dim,
        multimodal_cfg,
        quick_gelu: bool = False,
        cast_dtype: Optional[torch.dtype] = None,
):
    multimodal_cfg = MultimodalCfg(**multimodal_cfg) if isinstance(multimodal_cfg, dict) else multimodal_cfg
    act_layer = QuickGELU if quick_gelu else nn.GELU
    norm_layer = (
        LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm
    )

    decoder = MultimodalTransformer(
        context_length=multimodal_cfg.context_length,
        width=multimodal_cfg.width,
        heads=multimodal_cfg.heads,
        layers=multimodal_cfg.layers,
        ls_init_value=multimodal_cfg.ls_init_value,
        output_dim=embed_dim,
        act_layer=act_layer,
        norm_layer=norm_layer,
    )

    return decoder


def _token_to_tensor(token_id, device: str = "cpu") -> torch.Tensor:
    if not isinstance(token_id, torch.Tensor):
        if isinstance(token_id, int):
            token_id = [token_id]
        token_id = torch.tensor(token_id, device=device)
    return token_id


class CoCa(nn.Module):
    def __init__(
            self,
            embed_dim,
            multimodal_cfg: MultimodalCfg,
            text_cfg: CLIPTextCfg,
            vision_cfg: CLIPVisionCfg,
            quick_gelu: bool = False,
            init_logit_scale: float = np.log(1 / 0.07),
            init_logit_bias: Optional[float] = None,
            nonscalar_logit_scale: bool = False,
            cast_dtype: Optional[torch.dtype] = None,
            pad_id: int = 0,
    ):
        super().__init__()
        multimodal_cfg = MultimodalCfg(**multimodal_cfg) if isinstance(multimodal_cfg, dict) else multimodal_cfg
        text_cfg = CLIPTextCfg(**text_cfg) if isinstance(text_cfg, dict) else text_cfg
        vision_cfg = CLIPVisionCfg(**vision_cfg) if isinstance(vision_cfg, dict) else vision_cfg

        self.text = _build_text_tower(
            embed_dim=embed_dim,
            text_cfg=text_cfg,
            quick_gelu=quick_gelu,
            cast_dtype=cast_dtype,
        )

        vocab_size = (
            text_cfg.vocab_size  # for hf models
            if hasattr(text_cfg, "hf_model_name") and text_cfg.hf_model_name is not None
            else text_cfg.vocab_size
        )

        self.visual = _build_vision_tower(
            embed_dim=embed_dim,
            vision_cfg=vision_cfg,
            quick_gelu=quick_gelu,
            cast_dtype=cast_dtype,
        )

        self.text_decoder = _build_text_decoder_tower(
            vocab_size,
            multimodal_cfg=multimodal_cfg,
            quick_gelu=quick_gelu,
            cast_dtype=cast_dtype,
        )

        lshape = [1] if nonscalar_logit_scale else []
        self.logit_scale = nn.Parameter(torch.ones(lshape) * init_logit_scale)
        if init_logit_bias is not None:
            self.logit_bias = nn.Parameter(torch.ones(lshape) * init_logit_bias)
        else:
            self.logit_bias = None
        self.pad_id = pad_id

        self.context_length = multimodal_cfg.context_length

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable: bool = True):
        self.visual.set_grad_checkpointing(enable)
        self.text.set_grad_checkpointing(enable)
        self.text_decoder.set_grad_checkpointing(enable)

    def _encode_image(self, images, normalize: bool = True):
        image_latent, tokens_embs = self.visual(images)
        image_latent = F.normalize(image_latent, dim=-1) if normalize else image_latent
        return image_latent, tokens_embs

    def _encode_text(self, text, normalize: bool = True):
        text_latent, token_emb = self.text(text)
        text_latent = F.normalize(text_latent, dim=-1) if normalize else text_latent
        return text_latent, token_emb

    def encode_image(self, images, normalize: bool = True):
        image_latent, _ = self._encode_image(images, normalize=normalize)
        return image_latent

    def encode_text(self, text, normalize: bool = True):
        text_latent, _ = self._encode_text(text, normalize=normalize)
        return text_latent

    def forward_intermediates(
            self,
            image: Optional[torch.Tensor] = None,
            text: Optional[torch.Tensor] = None,
            image_indices: Optional[Union[int, List[int]]] = None,
            text_indices: Optional[Union[int, List[int]]] = None,
            stop_early: bool = False,
            normalize: bool = True,
            normalize_intermediates: bool = False,
            intermediates_only: bool = False,
            image_output_fmt: str = 'NCHW',
            image_output_extra_tokens: bool = False,
            text_output_fmt: str = 'NLC',
            text_output_extra_tokens: bool = False,
            output_logits: bool = False,
            output_logit_scale_bias: bool = False,
    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """ Forward features that returns intermediates.

        Args:
            image: Input image tensor
            text: Input text tensor
            image_indices: For image tower, Take last n blocks if int, all if None, select matching indices if sequence
            text_indices: Take last n blocks if int, all if None, select matching indices if sequence
            stop_early: Stop iterating over blocks when last desired intermediate hit
            normalize: L2 Normalize final image and text features (if present)
            normalize_intermediates: Apply final encoder norm layer to all intermediates (if possible)
            intermediates_only: Only return intermediate features, do not return final features
            image_output_fmt: Shape of intermediate image feature outputs
            image_output_extra_tokens: Return both prefix and spatial intermediate tokens
            text_output_fmt: Shape of intermediate text feature outputs
            text_output_extra_tokens: Return both prefix and spatial intermediate tokens
            output_logits: Include logits in output
            output_logit_scale_bias: Include the logit scale bias in the output
        Returns:

        """
        output = {}
        if intermediates_only:
            # intermediates only disables final feature normalization, and include logits
            normalize = False
            output_logits = False
        if output_logits:
            assert False, 'FIXME, needs implementing'

        if image is not None:
            image_output = self.visual.forward_intermediates(
                image,
                indices=image_indices,
                stop_early=stop_early,
                normalize_intermediates=normalize_intermediates,
                intermediates_only=intermediates_only,
                output_fmt=image_output_fmt,
                output_extra_tokens=image_output_extra_tokens,
            )
            if normalize and "image_features" in image_output:
                image_output["image_features"] = F.normalize(image_output["image_features"], dim=-1)
            output.update(image_output)

        if text is not None:
            text_output = self.text.forward_intermediates(
                text,
                indices=text_indices,
                stop_early=stop_early,
                normalize_intermediates=normalize_intermediates,
                intermediates_only=intermediates_only,
                output_fmt=text_output_fmt,
                output_extra_tokens=text_output_extra_tokens,
            )
            if normalize and "text_features" in text_output:
                text_output["text_features"] = F.normalize(text_output["text_features"], dim=-1)
            output.update(text_output)

        # FIXME text decoder
        logit_scale_exp = self.logit_scale.exp() if output_logits or output_logit_scale_bias else None
        if output_logit_scale_bias:
            output["logit_scale"] = logit_scale_exp
            if self.logit_bias is not None:
                output['logit_bias'] = self.logit_bias

        return output

    def forward(
            self,
            image,
            text: Optional[torch.Tensor] = None,
            image_latent: Optional[torch.Tensor] = None,
            image_embs: Optional[torch.Tensor] = None,
            output_labels: bool = True,
    ):
        if image_latent is None or image_embs is None:
            image_latent, image_embs = self._encode_image(image)

        if text is None:
            return {"image_features": image_latent, "image_embs": image_embs}

        text_latent, token_embs = self._encode_text(text)

        # FIXME this isn't an ideal solution, would like to improve -RW
        labels: Optional[torch.Tensor] = text[:, 1:] if output_labels else None
        if output_labels:
            # align text_embs and thus logits with labels for teacher-forcing caption loss
            token_embs = token_embs[:, :-1]

        logits = self.text_decoder(image_embs, token_embs)
        out_dict = {
            "image_features": image_latent,
            "text_features": text_latent,
            "logits": logits,
            "logit_scale": self.logit_scale.exp()
        }
        if labels is not None:
            out_dict["labels"] = labels
        if self.logit_bias is not None:
            out_dict["logit_bias"] = self.logit_bias
        return out_dict

    def generate(
        self,
        image,
        text=None,
        seq_len=30,
        max_seq_len=77,
        temperature=1.,
        generation_type="beam_search",
        top_p=0.1,  # keep tokens in the 1 - top_p quantile
        top_k=1,  # keeps the top_k most probable tokens
        pad_token_id=None,
        eos_token_id=None,
        sot_token_id=None,
        num_beams=6,
        num_beam_groups=3,
        min_seq_len=5,
        stopping_criteria=None,
        repetition_penalty=1.0,
        fixed_output_length=False # if True output.shape == (batch_size, seq_len)
    ):
        # taking many ideas and components from HuggingFace GenerationMixin
        # https://huggingface.co/docs/transformers/main/en/main_classes/text_generation
        assert _has_transformers, "Please install transformers for generate functionality. `pip install transformers`."
        assert seq_len > min_seq_len, "seq_len must be larger than min_seq_len"
        device = image.device

        with torch.no_grad():
            sot_token_id = _token_to_tensor(49406 if sot_token_id is None else sot_token_id, device=device)
            eos_token_id = _token_to_tensor(49407 if eos_token_id is None else eos_token_id, device=device)
            pad_token_id = self.pad_id if pad_token_id is None else pad_token_id
            logit_processor = LogitsProcessorList(
                [
                    MinLengthLogitsProcessor(min_seq_len, eos_token_id),
                    RepetitionPenaltyLogitsProcessor(repetition_penalty),
                ]
            )

            if stopping_criteria is None:
                stopping_criteria = [MaxLengthCriteria(max_length=seq_len)]
            stopping_criteria = StoppingCriteriaList(stopping_criteria)

            if generation_type == "beam_search":
                output = self._generate_beamsearch(
                    image_inputs=image,
                    pad_token_id=pad_token_id,
                    eos_token_id=eos_token_id,
                    sot_token_id=sot_token_id,
                    num_beams=num_beams,
                    num_beam_groups=num_beam_groups,
                    min_seq_len=min_seq_len,
                    stopping_criteria=stopping_criteria,
                    logit_processor=logit_processor,
                )
                if fixed_output_length and output.shape[1] < seq_len:
                    pad_len = seq_len - output.shape[1]
                    return torch.cat((
                            output,
                            torch.ones(output.shape[0], pad_len, device=device, dtype=output.dtype) * pad_token_id
                        ),
                        dim=1
                    )
                return output

            elif generation_type == "top_p":
                logit_warper = GENERATION_TYPES[generation_type](top_p)
            elif generation_type == "top_k":
                logit_warper = GENERATION_TYPES[generation_type](top_k)
            else:
                raise ValueError(
                    f"generation_type has to be one of "
                    f"{'| ' + ' | '.join(list(GENERATION_TYPES.keys())) + ' |'}."
                )

            image_latent, image_embs = self._encode_image(image)

            if text is None:
                text = torch.ones((image.shape[0], 1), device=device, dtype=torch.long) * sot_token_id

            was_training = self.training
            num_dims = len(text.shape)

            if num_dims == 1:
                text = text[None, :]

            self.eval()
            out = text

            while True:
                x = out[:, -max_seq_len:]
                cur_len = x.shape[1]
                logits = self(
                    image,
                    x,
                    image_latent=image_latent,
                    image_embs=image_embs,
                    output_labels=False,
                )["logits"][:, -1]
                mask = (out[:, -1] == eos_token_id) | (out[:, -1] == pad_token_id)
                sample = torch.ones((out.shape[0], 1), device=device, dtype=torch.long) * pad_token_id

                if mask.all():
                    if not fixed_output_length:
                        break
                else:
                    logits = logits[~mask, :]
                    filtered_logits = logit_processor(x[~mask, :], logits)
                    filtered_logits = logit_warper(x[~mask, :], filtered_logits)
                    probs = F.softmax(filtered_logits / temperature, dim=-1)

                    if (cur_len + 1 == seq_len):
                        sample[~mask, :] = torch.ones((sum(~mask), 1), device=device, dtype=torch.long) * eos_token_id
                    else:
                        sample[~mask, :] = torch.multinomial(probs, 1)

                out = torch.cat((out, sample), dim=-1)

                cur_len += 1

                if all(stopping_criteria(out, None)):
                    break

            if num_dims == 1:
                out = out.squeeze(0)

            self.train(was_training)
            return out

    def _generate_beamsearch(
            self,
            image_inputs,
            pad_token_id=None,
            eos_token_id=None,
            sot_token_id=None,
            num_beams=6,
            num_beam_groups=3,
            min_seq_len=5,
            stopping_criteria=None,
            logit_processor=None,
            logit_warper=None,
    ):
        device = image_inputs.device
        batch_size = image_inputs.shape[0]
        image_inputs = torch.repeat_interleave(image_inputs, num_beams, dim=0)
        image_latent, image_embs = self._encode_image(image_inputs)

        input_ids = torch.ones((batch_size * num_beams, 1), device=device, dtype=torch.long)
        input_ids = input_ids * sot_token_id
        beam_scorer = BeamSearchScorer(
            batch_size=batch_size,
            num_beams=num_beams,
            device=device,
            num_beam_groups=num_beam_groups,
        )
        # instantiate logits processors
        logits_processor = (
            LogitsProcessorList([MinLengthLogitsProcessor(min_seq_len, eos_token_id=eos_token_id)])
            if logit_processor is None
            else logit_processor
        )

        num_beams = beam_scorer.num_beams
        num_beam_groups = beam_scorer.num_beam_groups
        num_sub_beams = num_beams // num_beam_groups
        batch_size = len(beam_scorer._beam_hyps) // num_beam_groups
        batch_beam_size, cur_len = input_ids.shape
        beam_indices = None

        if num_beams * batch_size != batch_beam_size:
            raise ValueError(
                f"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}."
            )

        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)
        # initialise score of first beam of each group with 0 and the rest with 1e-9. This ensures that the beams in
        # the same group don't produce same tokens everytime.
        beam_scores[:, ::num_sub_beams] = 0
        beam_scores = beam_scores.view((batch_size * num_beams,))

        while True:

            # predicted tokens in cur_len step
            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)

            # indices which will form the beams in the next time step
            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)

            # do one decoder step on all beams of all sentences in batch
            model_inputs = prepare_inputs_for_generation(input_ids=input_ids, image_inputs=image_inputs)
            outputs = self(
                model_inputs['images'],
                model_inputs['text'],
                image_latent=image_latent,
                image_embs=image_embs,
                output_labels=False,
            )

            for beam_group_idx in range(num_beam_groups):
                group_start_idx = beam_group_idx * num_sub_beams
                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)
                group_size = group_end_idx - group_start_idx

                # indices of beams of current group among all sentences in batch
                batch_group_indices = []

                for batch_idx in range(batch_size):
                    batch_group_indices.extend(
                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]
                    )
                group_input_ids = input_ids[batch_group_indices]

                # select outputs of beams of currentg group only
                next_token_logits = outputs['logits'][batch_group_indices, -1, :]
                vocab_size = next_token_logits.shape[-1]

                next_token_scores_processed = logits_processor(
                    group_input_ids, next_token_logits, current_tokens=current_tokens, beam_group_idx=beam_group_idx
                )
                next_token_scores = next_token_scores_processed + beam_scores[batch_group_indices].unsqueeze(-1)
                next_token_scores = next_token_scores.expand_as(next_token_scores_processed)

                # reshape for beam search
                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)

                next_token_scores, next_tokens = torch.topk(
                    next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True
                )

                next_indices = torch.div(next_tokens, vocab_size, rounding_mode="floor")
                next_tokens = next_tokens % vocab_size

                # stateless
                process_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None
                beam_outputs = beam_scorer.process(
                    group_input_ids,
                    next_token_scores,
                    next_tokens,
                    next_indices,
                    pad_token_id=pad_token_id,
                    eos_token_id=eos_token_id,
                    beam_indices=process_beam_indices,
                    group_index=beam_group_idx,
                )
                beam_scores[batch_group_indices] = beam_outputs["next_beam_scores"]
                beam_next_tokens = beam_outputs["next_beam_tokens"]
                beam_idx = beam_outputs["next_beam_indices"]

                input_ids[batch_group_indices] = group_input_ids[beam_idx]
                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)
                current_tokens[batch_group_indices] = group_input_ids[:, -1]

                # (beam_idx // group_size) -> batch_idx
                # (beam_idx % group_size) -> offset of idx inside the group
                reordering_indices[batch_group_indices] = (
                    num_beams * torch.div(beam_idx, group_size, rounding_mode="floor") + group_start_idx + (beam_idx % group_size)
                )

            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)

            # increase cur_len
            cur_len = cur_len + 1
            if beam_scorer.is_done or all(stopping_criteria(input_ids, None)):
                break

        final_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None
        sequence_outputs = beam_scorer.finalize(
            input_ids,
            beam_scores,
            next_tokens,
            next_indices,
            pad_token_id=pad_token_id,
            eos_token_id=eos_token_id,
            max_length=stopping_criteria.max_length,
            beam_indices=final_beam_indices,
        )
        return sequence_outputs['sequences']


def prepare_inputs_for_generation(input_ids, image_inputs, past=None, **kwargs):
    if past:
        input_ids = input_ids[:, -1].unsqueeze(-1)

    attention_mask = kwargs.get("attention_mask", None)
    position_ids = kwargs.get("position_ids", None)

    if attention_mask is not None and position_ids is None:
        # create position_ids on the fly for batch generation
        position_ids = attention_mask.long().cumsum(-1) - 1
        position_ids.masked_fill_(attention_mask == 0, 1)
    else:
        position_ids = None
    return {
        "text": input_ids,
        "images": image_inputs,
        "past_key_values": past,
        "position_ids": position_ids,
        "attention_mask": attention_mask,
    }

===== src/open_clip/model_configs/convnext_large.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "timm_model_name": "convnext_large",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 224
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/convnext_base.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "timm_model_name": "convnext_base",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 224
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-L-14-quickgelu.json =====
{
    "embed_dim": 768,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": 24,
        "width": 1024,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/PE-Core-T-16-384.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_pe_core_tiny_patch16_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/convnext_base_w_320.json =====
{
    "embed_dim": 640,
    "vision_cfg": {
        "timm_model_name": "convnext_base",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 320
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 640,
        "heads": 10,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-L-16-SigLIP2-512.json =====
{
    "embed_dim": 1024,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 512,
        "timm_model_name": "vit_large_patch16_siglip_512",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-L-16-SigLIP2-512",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1024,
        "heads": 16,
        "layers": 24,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/RN50x64.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 448,
        "layers": [
            3,
            15,
            36,
            10
        ],
        "width": 128,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-B-32.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 32
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-e-14.json =====
{
    "embed_dim": 1280,
    "vision_cfg": {
        "image_size": 224,
        "layers": 56,
        "width": 1792,
        "head_width": 112,
        "mlp_ratio": 8.5715,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1280,
        "heads": 20,
        "layers": 36
    }
}
===== src/open_clip/model_configs/coca_base.json =====
{
    "embed_dim": 512,
    "multimodal_cfg": {
        "width": 768,
        "context_length": 76,
        "vocab_size": 64000,
        "mlp_ratio": 4,
        "layers": 12,
        "dim_head": 64,
        "heads": 12,
        "n_queries": 256,
        "attn_pooler_heads": 8
    },
    "vision_cfg": {
        "image_size": 288,
        "layers": 12,
        "width": 768,
        "patch_size": 18,
        "output_tokens": true
    },
    "text_cfg": {
        "context_length": 76,
        "vocab_size": 64000,
        "layers": 12,
        "heads": 12,
        "width": 768,
        "embed_cls": true,
        "output_tokens": true
    },
    "custom_text": true
}
===== src/open_clip/model_configs/convnext_small.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "timm_model_name": "convnext_small",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 224
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-B-16-SigLIP2.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "vit_base_patch16_siglip_224",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP2",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/ViT-L-14-336-quickgelu.json =====
{
    "embed_dim": 768,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 336,
        "layers": 24,
        "width": 1024,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-H-14-worldwide-378.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 378,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_tokenizer_name": "facebook/xlm-v-base",
        "pool_type": "eos",
        "eos_id": 2,
        "context_length": 77,
        "vocab_size": 901629,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/ViT-SO400M-16-SigLIP2-256.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_so400m_patch16_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-SO400M-16-SigLIP2-256",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/PE-Core-bigG-14-448.json =====
{
    "embed_dim": 1280,
    "vision_cfg": {
        "image_size": 448,
        "timm_model_name": "vit_pe_core_gigantic_patch14_448",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 72,
        "vocab_size": 49408,
        "width": 1280,
        "heads": 20,
        "layers": 24
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-bigG-14.json =====
{
    "embed_dim": 1280,
    "vision_cfg": {
        "image_size": 224,
        "layers": 48,
        "width": 1664,
        "head_width": 104,
        "mlp_ratio": 4.9231,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1280,
        "heads": 20,
        "layers": 32
    }
}
===== src/open_clip/model_configs/ViT-SO400M-16-SigLIP2-384.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_so400m_patch16_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-SO400M-16-SigLIP2-384",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/ViT-L-16-SigLIP2-256.json =====
{
    "embed_dim": 1024,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_large_patch16_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-L-16-SigLIP2-256",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1024,
        "heads": 16,
        "layers": 24,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/ViT-B-16-SigLIP-512.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 512,
        "timm_model_name": "vit_base_patch16_siglip_512",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/ViT-SO400M-14-SigLIP.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "vit_so400m_patch14_siglip_224",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 16,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/ViT-bigG-14-CLIPA.json =====
{
    "embed_dim": 1280,
    "vision_cfg": {
        "image_size": 224,
        "layers": 48,
        "width": 1664,
        "head_width": 104,
        "mlp_ratio": 4.9231,
        "patch_size": 14,
        "no_ln_pre": true,
        "pool_type": "avg",
        "final_ln_after_pool": true
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 32000,
        "hf_tokenizer_name": "bert-base-uncased",
        "tokenizer_kwargs": {
            "strip_sep_token": true
        },
        "width": 1280,
        "heads": 20,
        "layers": 32,
        "pool_type": "last",
        "no_causal_mask": true
    }
}
===== src/open_clip/model_configs/ViTamin-L-336.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
      "timm_model_name": "vitamin_large_336",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 336
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 768,
      "heads": 12,
      "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-B-32-256.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 256,
        "layers": 12,
        "width": 768,
        "patch_size": 32
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}

===== src/open_clip/model_configs/ViT-L-14-CLIPA-336.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 336,
        "layers": 24,
        "width": 1024,
        "patch_size": 14,
        "no_ln_pre": true,
        "pool_type": "avg",
        "final_ln_after_pool": true
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 32000,
        "hf_tokenizer_name": "bert-base-uncased",
        "tokenizer_kwargs": {
            "strip_sep_token": true
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "pool_type": "last",
        "no_causal_mask": true
    }
}
===== src/open_clip/model_configs/ViT-B-16-SigLIP-i18n-256.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_base_patch16_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 250000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP-i18n-256",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/ViTamin-L.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
      "timm_model_name": "vitamin_large_224",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 224
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 768,
      "heads": 12,
      "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/coca_ViT-L-14.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 224,
        "layers": 24,
        "width": 1024,
        "patch_size": 14,
        "attentional_pool": true,
        "attn_pooler_heads": 8,
        "output_tokens": true
    },
    "text_cfg": {
        "context_length": 76,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12,
        "embed_cls": true,
        "output_tokens": true
    },
    "multimodal_cfg": {
        "context_length": 76,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12,
        "attn_pooler_heads": 12
    },
    "custom_text": true
}

===== src/open_clip/model_configs/ViT-B-32-quickgelu.json =====
{
    "embed_dim": 512,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 32
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-L-14-280.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 280,
        "layers": 24,
        "width": 1024,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-M-32.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 512,
        "patch_size": 32
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/PE-Core-B-16.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "vit_pe_core_base_patch16_224",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    },
    "custom_text": true
}
===== src/open_clip/model_configs/EVA01-g-14.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "eva_giant_patch14_224",
        "timm_model_pretrained": false,
        "timm_pool": "token",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-B-16-SigLIP2-512.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 512,
        "timm_model_name": "vit_base_patch16_siglip_512",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP2-512",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/swin_base_patch4_window7_224.json =====
{
    "embed_dim": 640,
    "vision_cfg": {
        "timm_model_name": "swin_base_patch4_window7_224",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "image_size": 224
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 640,
        "heads": 10,
        "layers": 12
    }
}
===== src/open_clip/model_configs/nllb-clip-base.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 32
    },
    "text_cfg": {
        "hf_model_name": "facebook/nllb-200-distilled-600M",
        "hf_tokenizer_name": "facebook/nllb-200-distilled-600M",
        "hf_proj_type": "linear",
        "hf_pooler_type": "cls_pooler"
    }
}
===== src/open_clip/model_configs/coca_ViT-B-32.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 32,
        "attentional_pool": true,
        "attn_pooler_heads": 8,
        "output_tokens": true
    },
    "text_cfg": {
        "context_length": 76,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12,
        "embed_cls": true,
        "output_tokens": true
    },
    "multimodal_cfg": {
        "context_length": 76,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12,
        "attn_pooler_heads": 8
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-L-14-CLIPA.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 224,
        "layers": 24,
        "width": 1024,
        "patch_size": 14,
        "no_ln_pre": true,
        "pool_type": "avg",
        "final_ln_after_pool": true
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 32000,
        "hf_tokenizer_name": "bert-base-uncased",
        "tokenizer_kwargs": {
            "strip_sep_token": true
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "pool_type": "last",
        "no_causal_mask": true
    }
}
===== src/open_clip/model_configs/ViTamin-L2.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
      "timm_model_name": "vitamin_large2_224",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 224
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 1024,
      "heads": 16,
      "layers": 24
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-g-14.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 40,
        "width": 1408,
        "head_width": 88,
        "mlp_ratio": 4.3637,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/ViT-H-14-worldwide-quickgelu.json =====
{
    "embed_dim": 1024,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_tokenizer_name": "facebook/xlm-v-base",
        "pool_type": "eos",
        "eos_id": 2,
        "context_length": 77,
        "vocab_size": 901629,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/MobileCLIP-S1.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "timm_model_name": "fastvit_mci1",
        "timm_model_pretrained": false,
        "timm_pool": "avg",
        "timm_proj": null,
        "timm_drop": 0.0,
        "timm_drop_path": 0.0,
        "image_size": 256
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12,
        "no_causal_mask": true
    },
    "custom_text": true
}
===== src/open_clip/model_configs/convnext_large_d_320.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "timm_model_name": "convnext_large",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "mlp",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 320
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 16
    }
}
===== src/open_clip/model_configs/ViT-SO400M-14-SigLIP-384.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_so400m_patch14_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/EVA01-g-14-plus.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "eva_giant_patch14_224",
        "timm_model_pretrained": false,
        "timm_pool": "token",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    },
    "custom_text": true
}
===== src/open_clip/model_configs/EVA02-L-14.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "eva02_large_patch14_clip_224",
        "timm_model_pretrained": false,
        "timm_pool": "token",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-SO400M-14-SigLIP-378.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 378,
        "timm_model_name": "vit_so400m_patch14_siglip_378",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/nllb-clip-base-siglip.json =====
{
    "embed_dim": 768,
    "custom_text": true,
    "init_logit_bias": -10,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_base_patch16_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "hf_model_name": "facebook/nllb-200-distilled-600M",
        "hf_tokenizer_name": "facebook/nllb-200-distilled-600M",
        "hf_proj_type": "linear",
        "hf_pooler_type": "cls_pooler"
    }
}
===== src/open_clip/model_configs/ViT-B-16-plus-240.json =====
{
    "embed_dim": 640,
    "vision_cfg": {
        "image_size": 240,
        "layers": 12,
        "width": 896,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 640,
        "heads": 10,
        "layers": 12
    }
}
===== src/open_clip/model_configs/RN101.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": [
            3,
            4,
            23,
            3
        ],
        "width": 64,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-SO400M-14-SigLIP2-378.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 378,
        "timm_model_name": "vit_so400m_patch14_siglip_378",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-SO400M-14-SigLIP2-378",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/ViT-B-16-quickgelu.json =====
{
    "embed_dim": 512,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/convnext_large_d.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "timm_model_name": "convnext_large",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "mlp",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 256
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 16
    }
}
===== src/open_clip/model_configs/RN50x16.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 384,
        "layers": [
            6,
            8,
            18,
            8
        ],
        "width": 96,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-S-32-alt.json =====
{
    "embed_dim": 256,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 384,
        "patch_size": 32
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 256,
        "heads": 4,
        "layers": 10
    }
}
===== src/open_clip/model_configs/EVA02-E-14.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "eva02_enormous_patch14_clip_224",
        "timm_model_pretrained": false,
        "timm_pool": "token",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViTamin-S.json =====
{
    "embed_dim": 384,
    "vision_cfg": {
      "timm_model_name": "vitamin_small_224",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 224
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 384,
      "heads": 6,
      "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/vit_medium_patch16_gap_256.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "timm_model_name": "vit_medium_patch16_gap_256",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "image_size": 256
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/roberta-ViT-B-32.json =====
{
    "embed_dim": 512,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 32
    },
    "text_cfg": {
        "hf_model_name": "roberta-base",
        "hf_tokenizer_name": "roberta-base",
        "hf_pooler_type": "mean_pooler"
    }
}

===== src/open_clip/model_configs/ViT-L-16-320.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 320,
        "layers": 24,
        "width": 1024,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-SO400M-16-SigLIP-i18n-256.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_so400m_patch16_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 250000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP-i18n-256",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "pool_type": "last",
        "proj_type": "none",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/convnext_xxlarge.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "timm_model_name": "convnext_xxlarge",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 256
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/ViTamin-B.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
      "timm_model_name": "vitamin_base_224",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 224
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 512,
      "heads": 8,
      "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/coca_roberta-ViT-B-32.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 32,
        "output_tokens": true
    },
    "text_cfg": {
        "hf_model_name": "roberta-base",
        "hf_tokenizer_name": "roberta-base",
        "hf_proj_type": "linear",
        "width": 768,
        "output_tokens": true
    },
    "multimodal_cfg": {
        "context_length": 76,
        "width": 768,
        "heads": 8,
        "layers": 12
    },
    "custom_text": true
}

===== src/open_clip/model_configs/EVA02-L-14-336.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 336,
        "timm_model_name": "eva02_large_patch14_clip_336",
        "timm_model_pretrained": false,
        "timm_pool": "token",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/RN50x4-quickgelu.json =====
{
    "embed_dim": 640,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 288,
        "layers": [
            4,
            6,
            10,
            6
        ],
        "width": 80,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 640,
        "heads": 10,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-L-14-worldwide.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 224,
        "layers": 24,
        "width": 1024,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_tokenizer_name": "google/mt5-base",
        "pool_type": "eos",
        "eos_id": 1,
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "context_length": 77,
        "vocab_size": 250100,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViTamin-L2-336.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
      "timm_model_name": "vitamin_large2_336",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 336
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 1024,
      "heads": 16,
      "layers": 24
    },
    "custom_text": true
}
===== src/open_clip/model_configs/convnext_xxlarge_320.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "timm_model_name": "convnext_xxlarge",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 320
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/RN101-quickgelu.json =====
{
    "embed_dim": 512,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": [
            3,
            4,
            23,
            3
        ],
        "width": 64,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-S-16-alt.json =====
{
    "embed_dim": 256,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 384,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 256,
        "heads": 4,
        "layers": 10
    }
}
===== src/open_clip/model_configs/ViTamin-L-384.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
      "timm_model_name": "vitamin_large_384",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 384
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 768,
      "heads": 12,
      "layers": 12
    },
    "custom_text": true
}

===== src/open_clip/model_configs/ViT-B-16-SigLIP2-384.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_base_patch16_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP2-384",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/ViT-H-14-CLIPA-336.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 336,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14,
        "no_ln_pre": true,
        "pool_type": "avg",
        "final_ln_after_pool": true
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 32000,
        "hf_tokenizer_name": "bert-base-uncased",
        "tokenizer_kwargs": {
            "strip_sep_token": true
        },
        "width": 1024,
        "heads": 16,
        "layers": 24,
        "pool_type": "last",
        "no_causal_mask": true
    }
}
===== src/open_clip/model_configs/ViT-B-16-plus.json =====
{
    "embed_dim": 640,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 896,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 640,
        "heads": 10,
        "layers": 12
    }
}
===== src/open_clip/model_configs/MobileCLIP-B.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "timm_model_name": "vit_base_mci_224",
        "timm_model_pretrained": false,
        "timm_pool": "token",
        "timm_proj": null,
        "timm_drop": 0.0,
        "timm_drop_path": 0.0,
        "image_size": 224
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12,
        "no_causal_mask": false
    },
    "custom_text": true
}
===== src/open_clip/model_configs/RN50-quickgelu.json =====
{
    "embed_dim": 1024,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": [
            3,
            4,
            6,
            3
        ],
        "width": 64,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}

===== src/open_clip/model_configs/ViTamin-B-LTT.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
      "timm_model_name": "vitamin_base_224",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 224
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 768,
      "heads": 12,
      "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-B-32-plus-256.json =====
{
    "embed_dim": 640,
    "vision_cfg": {
        "image_size": 256,
        "layers": 12,
        "width": 896,
        "patch_size": 32
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 640,
        "heads": 10,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-H-14-quickgelu.json =====
{
    "embed_dim": 1024,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/ViT-L-16-SigLIP-384.json =====
{
    "embed_dim": 1024,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_large_patch16_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1024,
        "heads": 16,
        "layers": 24,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/ViT-B-16.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-M-16.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 512,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-SO400M-16-SigLIP2-512.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 512,
        "timm_model_name": "vit_so400m_patch16_siglip_512",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-SO400M-16-SigLIP2-512",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/ViT-H-16.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/ViTamin-L2-384.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
      "timm_model_name": "vitamin_large2_384",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 384
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 1024,
      "heads": 16,
      "layers": 24
    },
    "custom_text": true
}

===== src/open_clip/model_configs/ViTamin-L2-256.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
      "timm_model_name": "vitamin_large2_256",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 256
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 1024,
      "heads": 16,
      "layers": 24
    },
    "custom_text": true
}
===== src/open_clip/model_configs/EVA02-B-16.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "eva02_base_patch16_clip_224",
        "timm_model_pretrained": false,
        "timm_pool": "token",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-bigG-14-worldwide-378.json =====
{
    "embed_dim": 1280,
    "vision_cfg": {
        "image_size": 378,
        "layers": 48,
        "width": 1664,
        "head_width": 104,
        "mlp_ratio": 4.9231,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_tokenizer_name": "facebook/xlm-v-base",
        "pool_type": "eos",
        "eos_id": 2,
        "context_length": 77,
        "vocab_size": 901629,
        "width": 1280,
        "heads": 20,
        "layers": 32
    }
}
===== src/open_clip/model_configs/RN50x64-quickgelu.json =====
{
    "embed_dim": 1024,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 448,
        "layers": [
            3,
            15,
            36,
            10
        ],
        "width": 128,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViTamin-XL-384.json =====
{
    "embed_dim": 1152,
    "vision_cfg": {
      "timm_model_name": "vitamin_xlarge_384",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 256
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 1152,
      "heads": 16,
      "layers": 27
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-H-14.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/PE-Core-S-16-384.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_pe_core_small_patch16_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-L-16.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 224,
        "layers": 24,
        "width": 1024,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/RN50.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": [
            3,
            4,
            6,
            3
        ],
        "width": 64,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-H-14-CLIPA.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14,
        "no_ln_pre": true,
        "pool_type": "avg",
        "final_ln_after_pool": true
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 32000,
        "hf_tokenizer_name": "bert-base-uncased",
        "tokenizer_kwargs": {
            "strip_sep_token": true
        },
        "width": 1024,
        "heads": 16,
        "layers": 24,
        "pool_type": "last",
        "no_causal_mask": true
    }
}
===== src/open_clip/model_configs/ViT-L-14-336.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 336,
        "layers": 24,
        "width": 1024,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-H-14-worldwide.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_tokenizer_name": "facebook/xlm-v-base",
        "pool_type": "eos",
        "eos_id": 2,
        "context_length": 77,
        "vocab_size": 901629,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/ViT-L-14-worldwide-quickgelu.json =====
{
    "embed_dim": 768,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": 24,
        "width": 1024,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_tokenizer_name": "google/mt5-base",
        "pool_type": "eos",
        "eos_id": 1,
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "context_length": 77,
        "vocab_size": 250100,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/convnext_tiny.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "timm_model_name": "convnext_tiny",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 224
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-M-32-alt.json =====
{
    "embed_dim": 384,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 512,
        "patch_size": 32
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 384,
        "heads": 6,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViTamin-L-256.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
      "timm_model_name": "vitamin_large_256",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 256
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 768,
      "heads": 12,
      "layers": 12
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-B-32-SigLIP2-256.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_base_patch32_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-B-32-SigLIP2-256",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}

===== src/open_clip/model_configs/ViT-H-14-378-quickgelu.json =====
{
    "embed_dim": 1024,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 378,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/EVA02-E-14-plus.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "eva02_enormous_patch14_clip_224",
        "timm_model_pretrained": false,
        "timm_pool": "token",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1280,
        "heads": 20,
        "layers": 32
    },
    "custom_text": true
}
===== src/open_clip/model_configs/PE-Core-L-14-336.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 336,
        "timm_model_name": "vit_pe_core_large_patch14_336",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": null
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-bigG-14-worldwide.json =====
{
    "embed_dim": 1280,
    "vision_cfg": {
        "image_size": 224,
        "layers": 48,
        "width": 1664,
        "head_width": 104,
        "mlp_ratio": 4.9231,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_tokenizer_name": "facebook/xlm-v-base",
        "pool_type": "eos",
        "eos_id": 2,
        "context_length": 77,
        "vocab_size": 901629,
        "width": 1280,
        "heads": 20,
        "layers": 32
    }
}
===== src/open_clip/model_configs/ViT-bigG-14-CLIPA-336.json =====
{
    "embed_dim": 1280,
    "vision_cfg": {
        "image_size": 336,
        "layers": 48,
        "width": 1664,
        "head_width": 104,
        "mlp_ratio": 4.9231,
        "patch_size": 14,
        "no_ln_pre": true,
        "pool_type": "avg",
        "final_ln_after_pool": true
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 32000,
        "hf_tokenizer_name": "bert-base-uncased",
        "tokenizer_kwargs": {
            "strip_sep_token": true
        },
        "width": 1280,
        "heads": 20,
        "layers": 32,
        "pool_type": "last",
        "no_causal_mask": true
    }
}
===== src/open_clip/model_configs/RN50x16-quickgelu.json =====
{
    "embed_dim": 768,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 384,
        "layers": [
            6,
            8,
            18,
            8
        ],
        "width": 96,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-gopt-16-SigLIP2-384.json =====
{
    "embed_dim": 1536,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_giantopt_patch16_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-gopt-16-SigLIP2-384",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/nllb-clip-large-siglip.json =====
{
    "embed_dim": 1152,
    "custom_text": true,
    "init_logit_bias": -10,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_so400m_patch14_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "hf_model_name": "facebook/nllb-200-distilled-1.3B",
        "hf_tokenizer_name": "facebook/nllb-200-distilled-1.3B",
        "hf_proj_type": "linear",
        "hf_pooler_type": "cls_pooler"
    }
}
===== src/open_clip/model_configs/ViT-B-16-SigLIP-384.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_base_patch16_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/MobileCLIP-S2.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "timm_model_name": "fastvit_mci2",
        "timm_model_pretrained": false,
        "timm_pool": "avg",
        "timm_proj": null,
        "timm_drop": 0.0,
        "timm_drop_path": 0.0,
        "image_size": 256
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12,
        "no_causal_mask": true
    },
    "custom_text": true
}
===== src/open_clip/model_configs/nllb-clip-large.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_model_name": "facebook/nllb-200-distilled-1.3B",
        "hf_tokenizer_name": "facebook/nllb-200-distilled-1.3B",
        "hf_proj_type": "linear",
        "hf_pooler_type": "cls_pooler"
    }
}
===== src/open_clip/model_configs/vit_relpos_medium_patch16_cls_224.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "timm_model_name": "vit_relpos_medium_patch16_cls_224",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "image_size": 224
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 512,
        "heads": 8,
        "layers": 12
    }
}
===== src/open_clip/model_configs/xlm-roberta-base-ViT-B-32.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 32
    },
    "text_cfg": {
        "hf_model_name": "xlm-roberta-base",
        "hf_tokenizer_name": "xlm-roberta-base",
        "hf_pooler_type": "mean_pooler"
    }
}

===== src/open_clip/model_configs/ViTamin-XL-336.json =====
{
    "embed_dim": 1152,
    "vision_cfg": {
      "timm_model_name": "vitamin_xlarge_336",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 336
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 1152,
      "heads": 16,
      "layers": 27
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViT-L-16-SigLIP-256.json =====
{
    "embed_dim": 1024,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_large_patch16_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1024,
        "heads": 16,
        "layers": 24,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/ViT-B-16-SigLIP.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "vit_base_patch16_siglip_224",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/ViT-gopt-16-SigLIP2-256.json =====
{
    "embed_dim": 1536,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_giantopt_patch16_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-gopt-16-SigLIP2-256",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/ViT-B-16-SigLIP2-256.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_base_patch16_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP2-256",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/mt5-xl-ViT-H-14.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_model_name": "google/mt5-xl",
        "hf_tokenizer_name": "google/mt5-xl",
        "hf_pooler_type": "mean_pooler"
    }
}

===== src/open_clip/model_configs/ViT-M-16-alt.json =====
{
    "embed_dim": 384,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 512,
        "patch_size": 16,
        "ls_init_value": 1e-4
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 384,
        "heads": 6,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-L-14.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
        "image_size": 224,
        "layers": 24,
        "width": 1024,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 768,
        "heads": 12,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-B-16-SigLIP-256.json =====
{
    "embed_dim": 768,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 256,
        "timm_model_name": "vit_base_patch16_siglip_256",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 32000,
        "hf_tokenizer_name": "timm/ViT-B-16-SigLIP",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 768,
        "heads": 12,
        "layers": 12,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        }
    }
}
===== src/open_clip/model_configs/ViT-bigG-14-quickgelu.json =====
{
    "embed_dim": 1280,
    "quick_gelu": true,
    "vision_cfg": {
        "image_size": 224,
        "layers": 48,
        "width": 1664,
        "head_width": 104,
        "mlp_ratio": 4.9231,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1280,
        "heads": 20,
        "layers": 32
    }
}
===== src/open_clip/model_configs/ViT-H-14-378.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 378,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 24
    }
}
===== src/open_clip/model_configs/ViT-L-16-SigLIP2-384.json =====
{
    "embed_dim": 1024,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 384,
        "timm_model_name": "vit_large_patch16_siglip_384",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-L-16-SigLIP2-384",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1024,
        "heads": 16,
        "layers": 24,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/convnext_base_w.json =====
{
    "embed_dim": 640,
    "vision_cfg": {
        "timm_model_name": "convnext_base",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 256
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 640,
        "heads": 10,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-S-32.json =====
{
    "embed_dim": 384,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 384,
        "patch_size": 32
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 384,
        "heads": 6,
        "layers": 12
    }
}
===== src/open_clip/model_configs/xlm-roberta-large-ViT-H-14.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14
    },
    "text_cfg": {
        "hf_model_name": "xlm-roberta-large",
        "hf_tokenizer_name": "xlm-roberta-large",
        "hf_pooler_type": "mean_pooler"
    }
}

===== src/open_clip/model_configs/RN50x4.json =====
{
    "embed_dim": 640,
    "vision_cfg": {
        "image_size": 288,
        "layers": [
            4,
            6,
            10,
            6
        ],
        "width": 80,
        "patch_size": null
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 640,
        "heads": 10,
        "layers": 12
    }
}
===== src/open_clip/model_configs/ViT-S-16.json =====
{
    "embed_dim": 384,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 384,
        "patch_size": 16
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 384,
        "heads": 6,
        "layers": 12
    }
}
===== src/open_clip/model_configs/convnext_xlarge.json =====
{
    "embed_dim": 1024,
    "vision_cfg": {
        "timm_model_name": "convnext_xlarge",
        "timm_model_pretrained": false,
        "timm_pool": "",
        "timm_proj": "linear",
        "timm_drop": 0.0,
        "timm_drop_path": 0.1,
        "image_size": 256
    },
    "text_cfg": {
        "context_length": 77,
        "vocab_size": 49408,
        "width": 1024,
        "heads": 16,
        "layers": 20
    }
}
===== src/open_clip/model_configs/mt5-base-ViT-B-32.json =====
{
    "embed_dim": 512,
    "vision_cfg": {
        "image_size": 224,
        "layers": 12,
        "width": 768,
        "patch_size": 32
    },
    "text_cfg": {
        "hf_model_name": "google/mt5-base",
        "hf_tokenizer_name": "google/mt5-base",
        "hf_pooler_type": "mean_pooler"
    }
}

===== src/open_clip/model_configs/ViT-SO400M-14-SigLIP2.json =====
{
    "embed_dim": 1152,
    "init_logit_bias": -10,
    "custom_text": true,
    "vision_cfg": {
        "image_size": 224,
        "timm_model_name": "vit_so400m_patch14_siglip_224",
        "timm_model_pretrained": false,
        "timm_pool": "map",
        "timm_proj": "none"
    },
    "text_cfg": {
        "context_length": 64,
        "vocab_size": 256000,
        "hf_tokenizer_name": "timm/ViT-SO400M-14-SigLIP2",
        "tokenizer_kwargs": {
            "clean": "canonicalize"
        },
        "width": 1152,
        "heads": 16,
        "layers": 27,
        "mlp_ratio": 3.7362,
        "no_causal_mask": true,
        "proj_bias": true,
        "pool_type": "last",
        "norm_kwargs":{
            "eps": 1e-6
        },
        "act_kwargs": {
            "approximate": "tanh"
        }
    }
}
===== src/open_clip/model_configs/ViTamin-XL-256.json =====
{
    "embed_dim": 1152,
    "vision_cfg": {
      "timm_model_name": "vitamin_xlarge_256",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 256
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 1152,
      "heads": 16,
      "layers": 27
    },
    "custom_text": true
}
===== src/open_clip/model_configs/ViTamin-S-LTT.json =====
{
    "embed_dim": 768,
    "vision_cfg": {
      "timm_model_name": "vitamin_small_224",
      "timm_model_pretrained": false,
      "timm_pool": "",
      "timm_proj": "linear",
      "timm_drop": 0.0,
      "timm_drop_path": 0.1,
      "image_size": 224
    },
    "text_cfg": {
      "context_length": 77,
      "vocab_size": 49408,
      "width": 768,
      "heads": 12,
      "layers": 12
    },
    "custom_text": true
}
===== src/utils/pylogger.py =====
import logging
from typing import Mapping, Optional

from lightning_utilities.core.rank_zero import rank_prefixed_message, rank_zero_only


class RankedLogger(logging.LoggerAdapter):
    """A multi-GPU-friendly python command line logger."""

    def __init__(
        self,
        name: str = __name__,
        rank_zero_only: bool = False,
        extra: Optional[Mapping[str, object]] = None,
    ) -> None:
        """Initializes a multi-GPU-friendly python command line logger that logs on all processes
        with their rank prefixed in the log message.

        :param name: The name of the logger. Default is ``__name__``.
        :param rank_zero_only: Whether to force all logs to only occur on the rank zero process. Default is `False`.
        :param extra: (Optional) A dict-like object which provides contextual information. See `logging.LoggerAdapter`.
        """
        logger = logging.getLogger(name)
        super().__init__(logger=logger, extra=extra)
        self.rank_zero_only = rank_zero_only

    def log(self, level: int, msg: str, rank: Optional[int] = None, *args, **kwargs) -> None:
        """Delegate a log call to the underlying logger, after prefixing its message with the rank
        of the process it's being logged from. If `'rank'` is provided, then the log will only
        occur on that rank/process.

        :param level: The level to log at. Look at `logging.__init__.py` for more information.
        :param msg: The message to log.
        :param rank: The rank to log at.
        :param args: Additional args to pass to the underlying logging function.
        :param kwargs: Any additional keyword args to pass to the underlying logging function.
        """
        if self.isEnabledFor(level):
            msg, kwargs = self.process(msg, kwargs)
            current_rank = getattr(rank_zero_only, "rank", None)
            if current_rank is None:
                raise RuntimeError("The `rank_zero_only.rank` needs to be set before use")
            msg = rank_prefixed_message(msg, current_rank)
            if self.rank_zero_only:
                if current_rank == 0:
                    self.logger.log(level, msg, *args, **kwargs)
            else:
                if rank is None:
                    self.logger.log(level, msg, *args, **kwargs)
                elif current_rank == rank:
                    self.logger.log(level, msg, *args, **kwargs)

===== src/utils/__init__.py =====
from src.utils.instantiators import instantiate_callbacks, instantiate_loggers
from src.utils.logging_utils import log_hyperparameters
from src.utils.pylogger import RankedLogger
from src.utils.rich_utils import enforce_tags, print_config_tree
from src.utils.utils import extras, get_metric_value, task_wrapper

===== src/utils/utils.py =====
import warnings
from importlib.util import find_spec
from typing import Any, Callable, Dict, Optional, Tuple

from omegaconf import DictConfig

from src.utils import pylogger, rich_utils

log = pylogger.RankedLogger(__name__, rank_zero_only=True)


def extras(cfg: DictConfig) -> None:
    """Applies optional utilities before the task is started.

    Utilities:
        - Ignoring python warnings
        - Setting tags from command line
        - Rich config printing

    :param cfg: A DictConfig object containing the config tree.
    """
    # return if no `extras` config
    if not cfg.get("extras"):
        log.warning("Extras config not found! <cfg.extras=null>")
        return

    # disable python warnings
    if cfg.extras.get("ignore_warnings"):
        log.info("Disabling python warnings! <cfg.extras.ignore_warnings=True>")
        warnings.filterwarnings("ignore")

    # prompt user to input tags from command line if none are provided in the config
    if cfg.extras.get("enforce_tags"):
        log.info("Enforcing tags! <cfg.extras.enforce_tags=True>")
        rich_utils.enforce_tags(cfg, save_to_file=True)

    # pretty print config tree using Rich library
    if cfg.extras.get("print_config"):
        log.info("Printing config tree with Rich! <cfg.extras.print_config=True>")
        rich_utils.print_config_tree(cfg, resolve=True, save_to_file=True)


def task_wrapper(task_func: Callable) -> Callable:
    """Optional decorator that controls the failure behavior when executing the task function.

    This wrapper can be used to:
        - make sure loggers are closed even if the task function raises an exception (prevents multirun failure)
        - save the exception to a `.log` file
        - mark the run as failed with a dedicated file in the `logs/` folder (so we can find and rerun it later)
        - etc. (adjust depending on your needs)

    Example:
    ```
    @utils.task_wrapper
    def train(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        ...
        return metric_dict, object_dict
    ```

    :param task_func: The task function to be wrapped.

    :return: The wrapped task function.
    """

    def wrap(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        # execute the task
        try:
            metric_dict, object_dict = task_func(cfg=cfg)

        # things to do if exception occurs
        except Exception as ex:
            # save exception to `.log` file
            log.exception("")

            # some hyperparameter combinations might be invalid or cause out-of-memory errors
            # so when using hparam search plugins like Optuna, you might want to disable
            # raising the below exception to avoid multirun failure
            raise ex

        # things to always do after either success or exception
        finally:
            # display output dir path in terminal
            log.info(f"Output dir: {cfg.paths.output_dir}")

            # always close wandb run (even if exception occurs so multirun won't fail)
            if find_spec("wandb"):  # check if wandb is installed
                import wandb

                if wandb.run:
                    log.info("Closing wandb!")
                    wandb.finish()

        return metric_dict, object_dict

    return wrap


def get_metric_value(metric_dict: Dict[str, Any], metric_name: Optional[str]) -> Optional[float]:
    """Safely retrieves value of the metric logged in LightningModule.

    :param metric_dict: A dict containing metric values.
    :param metric_name: If provided, the name of the metric to retrieve.
    :return: If a metric name was provided, the value of the metric.
    """
    if not metric_name:
        log.info("Metric name is None! Skipping metric value retrieval...")
        return None

    if metric_name not in metric_dict:
        raise Exception(
            f"Metric value not found! <metric_name={metric_name}>\n"
            "Make sure metric name logged in LightningModule is correct!\n"
            "Make sure `optimized_metric` name in `hparams_search` config is correct!"
        )

    metric_value = metric_dict[metric_name].item()
    log.info(f"Retrieved metric value! <{metric_name}={metric_value}>")

    return metric_value

===== src/utils/rich_utils.py =====
from pathlib import Path
from typing import Sequence

import rich
import rich.syntax
import rich.tree
from hydra.core.hydra_config import HydraConfig
from lightning_utilities.core.rank_zero import rank_zero_only
from omegaconf import DictConfig, OmegaConf, open_dict
from rich.prompt import Prompt

from src.utils import pylogger

log = pylogger.RankedLogger(__name__, rank_zero_only=True)


@rank_zero_only
def print_config_tree(
    cfg: DictConfig,
    print_order: Sequence[str] = (
        "data",
        "model",
        "callbacks",
        "logger",
        "trainer",
        "paths",
        "extras",
    ),
    resolve: bool = False,
    save_to_file: bool = False,
) -> None:
    """Prints the contents of a DictConfig as a tree structure using the Rich library.

    :param cfg: A DictConfig composed by Hydra.
    :param print_order: Determines in what order config components are printed. Default is ``("data", "model",
    "callbacks", "logger", "trainer", "paths", "extras")``.
    :param resolve: Whether to resolve reference fields of DictConfig. Default is ``False``.
    :param save_to_file: Whether to export config to the hydra output folder. Default is ``False``.
    """
    style = "dim"
    tree = rich.tree.Tree("CONFIG", style=style, guide_style=style)

    queue = []

    # add fields from `print_order` to queue
    for field in print_order:
        queue.append(field) if field in cfg else log.warning(
            f"Field '{field}' not found in config. Skipping '{field}' config printing..."
        )

    # add all the other fields to queue (not specified in `print_order`)
    for field in cfg:
        if field not in queue:
            queue.append(field)

    # generate config tree from queue
    for field in queue:
        branch = tree.add(field, style=style, guide_style=style)

        config_group = cfg[field]
        if isinstance(config_group, DictConfig):
            branch_content = OmegaConf.to_yaml(config_group, resolve=resolve)
        else:
            branch_content = str(config_group)

        branch.add(rich.syntax.Syntax(branch_content, "yaml"))

    # print config tree
    rich.print(tree)

    # save config tree to file
    if save_to_file:
        with open(Path(cfg.paths.output_dir, "config_tree.log"), "w") as file:
            rich.print(tree, file=file)


@rank_zero_only
def enforce_tags(cfg: DictConfig, save_to_file: bool = False) -> None:
    """Prompts user to input tags from command line if no tags are provided in config.

    :param cfg: A DictConfig composed by Hydra.
    :param save_to_file: Whether to export tags to the hydra output folder. Default is ``False``.
    """
    if not cfg.get("tags"):
        if "id" in HydraConfig().cfg.hydra.job:
            raise ValueError("Specify tags before launching a multirun!")

        log.warning("No tags provided in config. Prompting user to input tags...")
        tags = Prompt.ask("Enter a list of comma separated tags", default="dev")
        tags = [t.strip() for t in tags.split(",") if t != ""]

        with open_dict(cfg):
            cfg.tags = tags

        log.info(f"Tags: {cfg.tags}")

    if save_to_file:
        with open(Path(cfg.paths.output_dir, "tags.log"), "w") as file:
            rich.print(cfg.tags, file=file)

===== src/utils/logging_utils.py =====
from typing import Any, Dict

from lightning_utilities.core.rank_zero import rank_zero_only
from omegaconf import OmegaConf

from src.utils import pylogger

log = pylogger.RankedLogger(__name__, rank_zero_only=True)


@rank_zero_only
def log_hyperparameters(object_dict: Dict[str, Any]) -> None:
    """Controls which config parts are saved by Lightning loggers.

    Additionally saves:
        - Number of model parameters

    :param object_dict: A dictionary containing the following objects:
        - `"cfg"`: A DictConfig object containing the main config.
        - `"model"`: The Lightning model.
        - `"trainer"`: The Lightning trainer.
    """
    hparams = {}

    cfg = OmegaConf.to_container(object_dict["cfg"])
    model = object_dict["model"]
    trainer = object_dict["trainer"]

    if not trainer.logger:
        log.warning("Logger not found! Skipping hyperparameter logging...")
        return

    hparams["model"] = cfg["model"]

    # save number of model parameters
    hparams["model/params/total"] = sum(p.numel() for p in model.parameters())
    hparams["model/params/trainable"] = sum(
        p.numel() for p in model.parameters() if p.requires_grad
    )
    hparams["model/params/non_trainable"] = sum(
        p.numel() for p in model.parameters() if not p.requires_grad
    )

    hparams["data"] = cfg["data"]
    hparams["trainer"] = cfg["trainer"]

    hparams["callbacks"] = cfg.get("callbacks")
    hparams["extras"] = cfg.get("extras")

    hparams["task_name"] = cfg.get("task_name")
    hparams["tags"] = cfg.get("tags")
    hparams["ckpt_path"] = cfg.get("ckpt_path")
    hparams["seed"] = cfg.get("seed")

    # send hparams to all loggers
    for logger in trainer.loggers:
        logger.log_hyperparams(hparams)

===== src/utils/instantiators.py =====
from typing import List

import hydra
from lightning import Callback
from lightning.pytorch.loggers import Logger
from omegaconf import DictConfig

from src.utils import pylogger

log = pylogger.RankedLogger(__name__, rank_zero_only=True)


def instantiate_callbacks(callbacks_cfg: DictConfig) -> List[Callback]:
    """Instantiates callbacks from config.

    :param callbacks_cfg: A DictConfig object containing callback configurations.
    :return: A list of instantiated callbacks.
    """
    callbacks: List[Callback] = []

    if not callbacks_cfg:
        log.warning("No callback configs found! Skipping..")
        return callbacks

    if not isinstance(callbacks_cfg, DictConfig):
        raise TypeError("Callbacks config must be a DictConfig!")

    for _, cb_conf in callbacks_cfg.items():
        if isinstance(cb_conf, DictConfig) and "_target_" in cb_conf:
            log.info(f"Instantiating callback <{cb_conf._target_}>")
            callbacks.append(hydra.utils.instantiate(cb_conf))

    return callbacks


def instantiate_loggers(logger_cfg: DictConfig) -> List[Logger]:
    """Instantiates loggers from config.

    :param logger_cfg: A DictConfig object containing logger configurations.
    :return: A list of instantiated loggers.
    """
    logger: List[Logger] = []

    if not logger_cfg:
        log.warning("No logger configs found! Skipping...")
        return logger

    if not isinstance(logger_cfg, DictConfig):
        raise TypeError("Logger config must be a DictConfig!")

    for _, lg_conf in logger_cfg.items():
        if isinstance(lg_conf, DictConfig) and "_target_" in lg_conf:
            log.info(f"Instantiating logger <{lg_conf._target_}>")
            logger.append(hydra.utils.instantiate(lg_conf))

    return logger

===== src/models/spatial_clip_module.py =====
# src/models/spatial_clip_module.py
import inspect  # <-- 1. ÂØºÂÖ• inspect Ê®°Âùó
from typing import Any, Dict

import torch
from lightning import LightningModule
from omegaconf import DictConfig
from torch.nn import Module as LossFunction
from torchmetrics import MetricCollection

from .components.spatial_clip_net import SpatialClipNet


class SpatialClipLitModule(LightningModule):
    def __init__(
        self,
        net: SpatialClipNet,
        loss_fn: LossFunction,
        optimizer_cfg: DictConfig,
        scheduler_cfg: DictConfig,
        train_metrics: MetricCollection,
        val_metrics: MetricCollection,
        test_metrics: MetricCollection,
    ):
        super().__init__()
        self.save_hyperparameters(logger=True, ignore=["net", "loss_fn"])
        self.net = net
        self.loss_fn = loss_fn
        self.train_metrics = train_metrics
        self.val_metrics = val_metrics
        self.test_metrics = test_metrics

        # 2. ‚úÖ CodeGuardian: Âú®ÂàùÂßãÂåñÊó∂Ôºå‰∏ÄÊ¨°ÊÄßÊ£ÄÊü•Âπ∂ÁºìÂ≠òÊçüÂ§±ÂáΩÊï∞ÈúÄË¶ÅÁöÑÂèÇÊï∞ÂêçÈõÜÂêà„ÄÇ
        self._loss_fn_arg_names = set(inspect.signature(self.loss_fn.forward).parameters.keys())
        # ËøôÂ∞ÜÂæóÂà∞‰∏Ä‰∏™Á±ª‰ºº {'image_features', 'text_features', 'logit_scale', ...} ÁöÑÈõÜÂêà

    def forward(self, images: torch.Tensor, texts: torch.Tensor) -> Dict[str, torch.Tensor]:
        return self.net(images, texts)

    def model_step(self, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        features = self.forward(batch["images"], batch["texts"])

        # 3. ‚úÖ CodeGuardian: ÂÆûÁé∞‚ÄúÊô∫ËÉΩÂàÜÂèëÂô®‚Äù
        # 3a. ÂáÜÂ§á‰∏Ä‰∏™ÂåÖÂê´ÊâÄÊúâÂèØÁî®Êï∞ÊçÆÁöÑ‚ÄúÂ§ßÂ≠óÂÖ∏‚Äù (The available data pool)
        available_data = {**features, **batch}

        # 3b. Ê†πÊçÆ‰πãÂâçÁºìÂ≠òÁöÑÂèÇÊï∞ÂêçÈõÜÂêàÔºåÊô∫ËÉΩÁ≠õÈÄâÂá∫ÂΩìÂâç loss_fn ÁúüÊ≠£ÈúÄË¶ÅÁöÑÂèÇÊï∞„ÄÇ
        loss_input = {
            key: value for key, value in available_data.items() 
            if key in self._loss_fn_arg_names
        }
        
        # 3c. ÂÆâÂÖ®Âú∞Ë∞ÉÁî®ÊçüÂ§±ÂáΩÊï∞
        loss_dict = self.loss_fn(**loss_input)
        
        # ... ÂêéÁª≠ÈÄªËæë‰øùÊåÅ‰∏çÂèò ...
        output = {"loss": loss_dict["contrastive_loss"]}
        logits_per_image = features["image_features"] @ features["text_features"].T * features["logit_scale"]
        output["logits"] = logits_per_image
        return output

    # ... training_step, validation_step, test_step, Âíå configure_optimizers ‰øùÊåÅ‰∏çÂèò ...
    def training_step(self, batch: Dict[str, Any], batch_idx: int) -> torch.Tensor:
        output = self.model_step(batch)
        self.log("train/loss", output["loss"], on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)
        self.train_metrics(output["logits"], torch.arange(len(output["logits"]), device=self.device))
        self.log_dict(self.train_metrics, on_step=False, on_epoch=True, sync_dist=True)
        return output["loss"]

    def validation_step(self, batch: Dict[str, Any], batch_idx: int) -> None:
        output = self.model_step(batch)
        self.log("val/loss", output["loss"], on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)
        self.val_metrics(output["logits"], torch.arange(len(output["logits"]), device=self.device))
        self.log_dict(self.val_metrics, on_step=False, on_epoch=True, sync_dist=True)

    def test_step(self, batch: Dict[str, Any], batch_idx: int) -> None:
        output = self.model_step(batch)
        self.log("test/loss", output["loss"], on_step=False, on_epoch=True, sync_dist=True)
        self.test_metrics(output["logits"], torch.arange(len(output["logits"]), device=self.device))
        self.log_dict(self.test_metrics, on_step=False, on_epoch=True, sync_dist=True)
        
    def configure_optimizers(self) -> Dict[str, Any]:
        optimizer = self.hparams.optimizer_cfg(params=self.parameters())
        if self.trainer is None:
            return {"optimizer": optimizer}
        if self.trainer.max_steps == -1:
            total_steps = self.trainer.estimated_stepping_batches if self.trainer.max_epochs is not None else 1_000_000
        else:
            total_steps = self.trainer.max_steps
        if total_steps == float('inf') or total_steps == -1:
            total_steps = 1_000_000 
            self.print(f"Warning: Could not estimate total steps. Using default: {total_steps}.")
        scheduler = self.hparams.scheduler_cfg(optimizer=optimizer, num_training_steps=int(total_steps))
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": self.hparams.get("optimized_metric", "val/loss"),
                "interval": "step",
                "frequency": 1,
            },
        }
===== src/models/components/spatial_clip_net.py =====
# src/models/components/spatial_clip_net.py

# src/models/components/spatial_clip_net.py
from dataclasses import is_dataclass, asdict
from typing import Dict, Optional, Any, Union
import torch
import torch.nn as nn
import open_clip
from omegaconf import DictConfig, ListConfig, OmegaConf

class SpatialClipNet(nn.Module):
    def __init__(
        self,
        model_name: str,
        pretrained: str,
        aug_cfg: Optional[Union[Dict[str, Any], DictConfig, Any]] = None,
        cache_dir: Optional[str] = None
    ):
        super().__init__()

        # Robust handling of aug_cfg from Hydra or user code
        aug_cfg_obj = None
        if aug_cfg is not None:
            if isinstance(aug_cfg, (DictConfig, ListConfig)):
                # 1) OmegaConf -> dict
                aug_dict = OmegaConf.to_container(aug_cfg, resolve=True)
                # 2) dict -> AugmentationCfg dataclass
                aug_cfg_obj = open_clip.AugmentationCfg(**aug_dict)
            elif isinstance(aug_cfg, dict):
                aug_cfg_obj = open_clip.AugmentationCfg(**aug_cfg)
            elif is_dataclass(aug_cfg):
                aug_cfg_obj = aug_cfg  # already AugmentationCfg
            else:
                raise TypeError(f"Unsupported type for aug_cfg: {type(aug_cfg)}")

        self.model, self.preprocess_train, self.preprocess_val = open_clip.create_model_and_transforms(
            model_name=model_name,
            pretrained=pretrained,
            aug_cfg=aug_cfg_obj,   # ‰º†ÂÖ•Êï∞ÊçÆÁ±ªÂÆû‰æãÔºåÂÖºÂÆπ‰∏äÊ∏∏ open_clip
            cache_dir=cache_dir,
        )
        self.tokenizer = open_clip.get_tokenizer(model_name)

    def forward(self, images: torch.Tensor, texts: torch.Tensor) -> Dict[str, torch.Tensor]:
        image_features = self.model.encode_image(images, normalize=True)
        text_features = self.model.encode_text(texts, normalize=True)
        
        return {
            "image_features": image_features,
            "text_features": text_features,
            "logit_scale": self.model.logit_scale.exp(),
            "logit_bias": getattr(self.model, 'logit_bias', None),
        }
===== src/models/components/losses.py =====
# src/models/components/losses.py
from typing import Dict, Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from open_clip.loss import gather_features, ClipLoss as OpenClipLoss


class SpatialLoss(nn.Module):
    """
    CodeGuardian: This is the 'Thick Implementation' for your spatial loss.
    It is a pure nn.Module, containing only the logic for WHAT it does.
    """
    def __init__(
        self,
        local_loss: bool = False,
        gather_with_grad: bool = False,
        rank: int = 0,
        world_size: int = 1,
        use_horovod: bool = False,
        cap_logit_scale: Optional[float] = None,
        temp_reg_weight: float = 0.0,
        float32_logits: bool = False,
        neighbor_alpha_scale: float = 1.0,
    ):
        super().__init__()
        if dist.is_available() and dist.is_initialized():
            self.rank = dist.get_rank()
            self.world_size = dist.get_world_size()
        else:
            self.rank = rank
            self.world_size = world_size
        
        self.local_loss = local_loss
        self.gather_with_grad = gather_with_grad
        self.use_horovod = use_horovod
        self.cap_logit_scale = cap_logit_scale
        self.temp_reg_weight = temp_reg_weight
        self.float32_logits = float32_logits
        self.neighbor_alpha_scale = neighbor_alpha_scale

    def forward(
        self,
        image_features: torch.Tensor,
        text_features: torch.Tensor,
        logit_scale: torch.Tensor,
        image_tile_ids: torch.Tensor,
        text_tile_ids: torch.Tensor,
        neighbor_tile_ids: torch.Tensor,
        neighbor_alphas: torch.Tensor,
        logit_bias: Optional[torch.Tensor] = None,
        output_dict: bool = True, # For consistency with LightningModule logging
    ) -> Dict[str, torch.Tensor]:
        device = image_features.device

        if self.world_size > 1:
            all_image_features, all_text_features = gather_features(
                image_features=image_features, text_features=text_features, 
                local_loss=self.local_loss, gather_with_grad=self.gather_with_grad,
                rank=self.rank, world_size=self.world_size, use_horovod=self.use_horovod)
            gathered_img_ids = [torch.empty_like(image_tile_ids) for _ in range(self.world_size)]
            gathered_txt_ids = [torch.empty_like(text_tile_ids) for _ in range(self.world_size)]
            dist.all_gather(gathered_img_ids, image_tile_ids)
            dist.all_gather(gathered_txt_ids, text_tile_ids)
            all_image_tile_ids = torch.cat(gathered_img_ids)
            all_text_tile_ids = torch.cat(gathered_txt_ids)
        else:
            all_image_features, all_text_features = image_features, text_features
            all_image_tile_ids, all_text_tile_ids = image_tile_ids, text_tile_ids

        s_eff = logit_scale
        if self.cap_logit_scale is not None:
            s_clipped = torch.clamp(logit_scale, max=self.cap_logit_scale)
            s_eff = logit_scale + (s_clipped - logit_scale).detach()

        z_i_t = image_features @ all_text_features.T
        z_t_i = text_features @ all_image_features.T
        logits_per_image = s_eff * z_i_t
        logits_per_text = s_eff * z_t_i

        if logit_bias is not None:
            logits_per_image += logit_bias
            logits_per_text += logit_bias
        
        if self.float32_logits:
            logits_per_image = logits_per_image.float()
            logits_per_text = logits_per_text.float()

        B_local, N_global = image_features.size(0), all_image_features.size(0)
        txt_id_to_idx = {tid.item(): i for i, tid in enumerate(all_text_tile_ids)}
        img_id_to_idx = {tid.item(): i for i, tid in enumerate(all_image_tile_ids)}
        ground_truth = torch.arange(B_local, device=device, dtype=torch.long) + B_local * self.rank
        labels_i_t = torch.zeros(B_local, N_global, device=device)
        labels_t_i = torch.zeros(B_local, N_global, device=device)
        labels_i_t.scatter_(1, ground_truth.unsqueeze(1), 1.0)
        labels_t_i.scatter_(1, ground_truth.unsqueeze(1), 1.0)

        alphas = (neighbor_alphas * self.neighbor_alpha_scale).clamp_min(0)

        for i in range(B_local):
            for nbr_id, alpha in zip(neighbor_tile_ids[i], alphas[i]):
                if alpha.item() <= 0: continue
                col_txt = txt_id_to_idx.get(int(nbr_id.item()))
                if col_txt is not None: labels_i_t[i, col_txt] += float(alpha.item())
                col_img = img_id_to_idx.get(int(nbr_id.item()))
                if col_img is not None: labels_t_i[i, col_img] += float(alpha.item())

        labels_i_t = F.normalize(labels_i_t, p=1, dim=1)
        labels_t_i = F.normalize(labels_t_i, p=1, dim=1)

        loss_i = -torch.sum(F.log_softmax(logits_per_image, dim=1) * labels_i_t, dim=1).mean()
        loss_t = -torch.sum(F.log_softmax(logits_per_text, dim=1) * labels_t_i, dim=1).mean()
        total_loss = 0.5 * (loss_i + loss_t)

        if self.temp_reg_weight > 0:
            p_i, p_t = F.softmax(logits_per_image, dim=1), F.softmax(logits_per_text, dim=1)
            Ez_p_i, Ez_q_i = (p_i * z_i_t).sum(dim=1).mean(), (labels_i_t * z_i_t).sum(dim=1).mean()
            Ez_p_t, Ez_q_t = (p_t * z_t_i).sum(dim=1).mean(), (labels_t_i * z_t_i).sum(dim=1).mean()
            gap = 0.5 * ((Ez_p_i - Ez_q_i) + (Ez_p_t - Ez_q_t))
            total_loss += self.temp_reg_weight * (gap ** 2)

        return {"contrastive_loss": total_loss}

class ClipLoss(OpenClipLoss):
    """
    CodeGuardian: ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂåÖË£ÖÂô®ÔºåÁé∞Âú®Êã•Êúâ‰∏Ä‰∏™Âπ≤ÂáÄ„ÄÅÊòéÁ°ÆÁöÑÊé•Âè£„ÄÇ
    ÂÆÉ‰∏çÂÜçÈúÄË¶Å **kwargs„ÄÇ
    """
    def forward(
        self,
        image_features: torch.Tensor,
        text_features: torch.Tensor,
        logit_scale: torch.Tensor,
        logit_bias: Optional[torch.Tensor] = None,
        # **kwargs,  <-- ÁßªÈô§Ëøô‰∏ÄË°å
    ) -> Dict[str, torch.Tensor]:
        # Á°Æ‰øù output_dict=False ‰ª•Ëé∑ÂèñÂéüÂßãÊçüÂ§±Âº†Èáè
        loss = super().forward(image_features, text_features, logit_scale, logit_bias, output_dict=False)
        return {"contrastive_loss": loss}
===== src/models/components/metrics.py =====
# src/models/components/metrics.py (New, Robust Version)

import torch
import torchmetrics
from torchmetrics import Metric

class RecallAtK(Metric):
    """
    Custom Metric to calculate Recall@k for in-batch retrieval.
    It is robust to varying batch sizes.
    """
    is_differentiable = False
    higher_is_better = True
    full_state_update = False

    def __init__(self, k: int):
        super().__init__()
        self.k = k
        self.add_state("correct", default=torch.tensor(0), dist_reduce_fx="sum")
        self.add_state("total", default=torch.tensor(0), dist_reduce_fx="sum")

    def update(self, logits: torch.Tensor, target: torch.Tensor) -> None:
        # Get top-k predictions
        _, top_k_preds = torch.topk(logits, self.k, dim=1)
        
        # Check if the target is in the top-k predictions
        # target.view(-1, 1) expands target to be comparable with each of the k predictions
        correct = torch.any(top_k_preds == target.view(-1, 1), dim=1)
        
        self.correct += torch.sum(correct)
        self.total += target.numel()

    def compute(self) -> torch.Tensor:
        return self.correct.float() / self.total


class ContrastiveMetrics(torchmetrics.MetricCollection):
    """
    A collection of robust metrics for contrastive learning tasks.
    """
    def __init__(self, prefix: str):
        # CodeGuardian: Notice `num_classes` is GONE. This component is now flexible.
        super().__init__(
            {
                "R@1": RecallAtK(k=1),
                "R@5": RecallAtK(k=5),
                "R@10": RecallAtK(k=10),
            },
            prefix=prefix,
        )
===== src/spaglam_preproc/cli.py =====
# spaglam_preproc/cli.py

import json
import logging
from pathlib import Path

import typer
import yaml
from rich.console import Console

from .core.dataset_writer import create_dataset_shards
from .utils.logging_setup import setup_logging

app = typer.Typer(
    name="spaglam-preproc",
    help="A high-performance, single-pass preprocessing pipeline for SpaGLaM.",
    add_completion=False,
)
console = Console()

@app.command()
def run(
    config_path: Path = typer.Option(
        ..., 
        "--config", 
        "-c",
        help="Path to the YAML configuration file.",
        exists=True,
        file_okay=True,
        dir_okay=False,
        readable=True,
        resolve_path=True,
    )
):
    """
    Run the full SpaGLaM data preprocessing pipeline using a configuration file.
    """
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    except Exception as e:
        console.print(f"[bold red]Error parsing config file '{config_path}':[/bold red] {e}")
        raise typer.Exit(code=1)
    
    # Ensure output directory exists before setting up logging
    output_dir = Path(config['paths']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Setup logging to file and console
    log_file_name = config.get('qc', {}).get('log_file_name', 'preprocessing.log')
    log_path = output_dir / log_file_name
    setup_logging(str(log_path))
    
    # Log the configuration for reproducibility
    logging.info("üöÄ Starting SpaGLaM preprocessing pipeline with configuration:")
    # Pretty print the config to the log file
    logging.info("\n" + json.dumps(config, indent=2))
    
    try:
        create_dataset_shards(config)
        console.print(f"\n[bold green]‚úÖ Pipeline finished successfully! Check outputs in '{output_dir}'.[/bold green]")
    except Exception:
        # The rich handler will automatically log the traceback
        logging.error("Pipeline failed with an unhandled exception.")
        console.print(f"\n[bold red]‚ùå Pipeline failed. See full traceback in the log file: {log_path}[/bold red]")
        raise typer.Exit(code=1)

if __name__ == "__main__":
    app()

===== src/spaglam_preproc/__init__.py =====
# spaglam_preproc/__init__.py

__version__ = "0.1.0"

# Expose the main user-facing classes and functions for easy import
# This allows users to write `from spaglam_preproc import SpaglamPipeline`
from .core.dataset_writer import SpaglamPipeline, create_dataset_shards
from .core.image_tiler import ImageHandler

__all__ = [
    "SpaglamPipeline",
    "create_dataset_shards",
    "ImageHandler",
    "__version__",
]

===== src/spaglam_preproc/config.py =====
# spaglam_preproc/config.py

from dataclasses import dataclass, field
from typing import Optional, Dict, Any

@dataclass
class PathConfig:
    """Configuration for all input and output paths."""
    adata_path: str
    output_dir: str
    # Image can be a WSI file, a standard image file, or loaded from adata.
    # If None, the pipeline will attempt to load the image from adata.uns['spatial'].
    image_path: Optional[str] = None
    # Optional path to a pre-computed list of highly variable genes (one gene per line or CSV column).
    hvg_list_path: Optional[str] = None

@dataclass
class PreprocessingConfig:
    """Parameters for data transformation and graph construction."""
    neighborhood_hops: int = 2
    n_top_genes_in_sentence: int = 50
    tile_size: int = 224
    precompute_embeddings: bool = True

@dataclass
class ModelConfig:
    """Model parameters, only required if precompute_embeddings is True."""
    model_path: str = "path/to/your/omiclip_model.pt"
    model_name: str = "ViT-B-32"

@dataclass
class QualityControlConfig:
    """Configuration for quality control, logging, and visualization."""
    enabled: bool = True
    num_visual_samples: int = 16  # Number of samples to include in the visual grid
    log_file_name: str = "preprocessing.log" # Name for the detailed log file

@dataclass
class PerformanceConfig:
    """Parameters to control performance and parallelization."""
    max_workers: int = 32
    max_samples_per_shard: int = 10000
    # Process a subset for quick testing. Set to -1 to process all spots.
    num_spots_to_process: int = -1 

@dataclass
class MainConfig:
    """Root configuration object that nests all other configurations."""
    paths: PathConfig
    preprocessing: PreprocessingConfig
    performance: PerformanceConfig
    qc: QualityControlConfig = field(default_factory=QualityControlConfig)
    # The model config is optional and only needed for one mode.
    model: Optional[ModelConfig] = None

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'MainConfig':
        """Creates a MainConfig object from a dictionary, handling nested structures."""
        # This allows for easy loading from a parsed YAML file.
        return cls(
            paths=PathConfig(**config_dict['paths']),
            preprocessing=PreprocessingConfig(**config_dict['preprocessing']),
            performance=PerformanceConfig(**config_dict['performance']),
            qc=QualityControlConfig(**config_dict.get('qc', {})),
            model=ModelConfig(**config_dict['model']) if 'model' in config_dict else None
        )

===== src/spaglam_preproc/core/gene_encoder.py =====
# spaglam_preproc/core/gene_encoder.py

import numpy as np

def generate_gene_sentence(
    expression_vector: np.ndarray,
    gene_names: np.ndarray,
    n_top_genes: int
) -> str:
    """
    Generates a gene sentence string from a single spot's expression vector.
    This function operates entirely in memory.

    Args:
        expression_vector: A 1D numpy array of gene expression values.
        gene_names: A 1D numpy array of corresponding gene names for the expression vector.
        n_top_genes: The number of top genes to include in the sentence.

    Returns:
        A space-separated string of the top N gene names.
    """
    # np.argsort is highly optimized for this task. We reverse it to get descending order.
    sorted_indices = np.argsort(expression_vector)[-1::-1]
    
    # Slice the top N indices and corresponding names
    top_n_indices = sorted_indices[:n_top_genes]
    top_genes = gene_names[top_n_indices]
    
    return " ".join(top_genes)

===== src/spaglam_preproc/core/dataset_writer.py =====
# spaglam_preproc/core/dataset_writer.py

import os
import io
import json
import logging
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
from typing import Dict, Any, Optional

import torch
import pandas as pd
import anndata
import webdataset as wds
import numpy as np
from scipy.sparse import issparse, csr_matrix

from ..utils.validation import pre_run_validation
from ..utils.qc_tools import generate_summary_report, generate_visual_artifact, display_visual_artifact_notebook
from ..utils.anndata_utils import safe_get_spatial_coords
from .graph_builder import get_k_hop_neighborhood
from .gene_encoder import generate_gene_sentence
from .image_tiler import ImageHandler

# Import open_clip conditionally for pre-computing embeddings
try:
    import open_clip
except ImportError:
    open_clip = None

# Detect if running in a notebook for TQDM
def _is_notebook():
    try:
        from IPython import get_ipython
        if 'IPKernelApp' in get_ipython().config:
            return True
    except:
        return False

TQDM_BAR = None
if _is_notebook():
    from tqdm.notebook import tqdm
    TQDM_BAR = tqdm
else:
    from tqdm import tqdm
    TQDM_BAR = tqdm



def _process_subgraph_to_sample(
    center_spot_info: pd.Series,
    *, # Enforce keyword-only arguments
    adata: anndata.AnnData,
    adata_hvg: anndata.AnnData,
    adjacency_matrix: csr_matrix,
    gene_names_hvg: np.ndarray,
    image_handler: ImageHandler,
    config: Dict[str, Any],
    model_resources: Dict[str, Any],
    collect_qc_sample: bool = False
) -> tuple[Optional[Dict], Optional[Dict], Optional[str]]:
    """
    Worker function for a single center spot. It performs the entire pipeline in memory.
    Returns the sample for the shard, an optional sample for QC, and an error message.
    """
    center_spot_id = center_spot_info.name
    qc_sample = None
    try:
        # a. Get k-hop neighborhood using BFS
        center_node_idx = adata.obs_names.get_loc(center_spot_id)
        k = config['preprocessing']['neighborhood_hops']
        global_indices = get_k_hop_neighborhood(adjacency_matrix, center_node_idx, k)
        all_spot_ids = adata.obs_names[global_indices].tolist()
        num_nodes = len(all_spot_ids)

        # b. Build local graph structure
        global_id_to_local_idx = {sid: i for i, sid in enumerate(all_spot_ids)}
        local_edge_index = []
        for local_u_idx, u_id in enumerate(all_spot_ids):
            u_global_idx = adata.obs_names.get_loc(u_id)
            start, end = adjacency_matrix.indptr[u_global_idx], adjacency_matrix.indptr[u_global_idx + 1]
            for v_global_idx in adjacency_matrix.indices[start:end]:
                v_id = adata.obs_names[v_global_idx]
                if v_id in global_id_to_local_idx:
                    local_v_idx = global_id_to_local_idx[v_id]
                    if local_u_idx < local_v_idx:
                        local_edge_index.append([local_u_idx, local_v_idx])
        
        # c. In-memory generation of raw data
        images_to_process = []
        texts_to_process = []
        spatial_coords = safe_get_spatial_coords(adata)
        for spot_id in all_spot_ids:
            spot_idx = adata.obs_names.get_loc(spot_id)
            coords = spatial_coords[spot_idx]
            tile = image_handler.get_tile(coords, config['preprocessing']['tile_size'])
            images_to_process.append(tile)
            
            expression = adata_hvg.X[spot_idx]
            expression_vector = expression.toarray().flatten() if issparse(expression) else np.array(expression).flatten()
            sentence = generate_gene_sentence(
                expression_vector, gene_names_hvg, config['preprocessing']['n_top_genes_in_sentence'])
            texts_to_process.append(sentence)
        
        # Collect a QC sample if requested (only for the center node)
        if collect_qc_sample:
            qc_sample = {
                'id': center_spot_id,
                'tile': images_to_process[0],
                'sentence': texts_to_process[0]
            }

        # d. Construct the final sample
        output_sample = {
            "__key__": center_spot_id,
            "json": json.dumps({"num_nodes": num_nodes, "edge_index": local_edge_index}).encode('utf-8')
        }

        if config['preprocessing']['precompute_embeddings']:
            model = model_resources['model']
            image_preprocessor = model_resources['image_preprocessor']
            tokenizer = model_resources['tokenizer']
            device = model_resources['device']

            image_input = torch.stack([image_preprocessor(img) for img in images_to_process])
            text_input = tokenizer(texts_to_process)
            with torch.no_grad(), torch.cuda.amp.autocast():
                image_input, text_input = image_input.to(device), text_input.to(device)
                image_embeddings = model.encode_image(image_input).cpu()
                text_embeddings = model.encode_text(text_input).cpu()

            # ===== SOTA MODIFICATION START =====
            # Instead of saving 2*N files per sample, we save ONE file containing all embeddings.
            # This is a massive I/O performance improvement for the data loader.
            
            embeddings_dict = {
                'image': image_embeddings, # Shape: [num_nodes, embed_dim]
                'text': text_embeddings,   # Shape: [num_nodes, embed_dim]
            }
            emb_buf = io.BytesIO()
            torch.save(embeddings_dict, emb_buf)
            output_sample["embeddings.pth"] = emb_buf.getvalue()
            # ===== SOTA MODIFICATION END =====

        else:
            for i in range(num_nodes):
                img_buf = io.BytesIO()
                images_to_process[i].save(img_buf, format="PNG")
                output_sample[f"{i}.png"] = img_buf.getvalue()
                output_sample[f"{i}.txt"] = texts_to_process[i].encode('utf-8')
        
        return output_sample, qc_sample, None

    except Exception as e:
        logging.error(f"Error processing {center_spot_id}", exc_info=True)
        return None, None, f"Skipping {center_spot_id}: {type(e).__name__} - {e}"

class SpaglamPipeline:
    """
    An object-oriented wrapper for the SpaGLaM preprocessing pipeline.
    Designed for easy use in both scripts and interactive notebook environments.
    """
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.adata = None
        self.adata_hvg = None
        self.adjacency_matrix = None
        self.gene_names_hvg = None
        self.image_handler = None
        self.model_resources = {}
        self.qc_samples_collected = []
        self.metrics = {
            'config': self.config,
            'timing': {},
            'counts': {},
            'graph_stats': {'num_nodes': [], 'num_edges': []},
        }

        self._load_resources()
        pre_run_validation(self.adata, self.image_handler, self.config)

    def _load_resources(self):
        """Loads all heavy resources like data and models into memory."""
        logging.info("--- Loading and Preparing Resources ---")
        
        logging.info(f"üíæ Loading AnnData from: {self.config['paths']['adata_path']}")
        self.adata = anndata.read_h5ad(self.config['paths']['adata_path'])
        
        logging.info("üñºÔ∏è Initializing ImageHandler...")
        self.image_handler = ImageHandler(
            source=self.config['paths'].get('image_path'), 
            adata=self.adata
        )

        logging.info("üß¨ Preparing gene lists...")
        hvg_list_path = self.config['paths'].get('hvg_list_path')
        if hvg_list_path and os.path.exists(hvg_list_path):
            hvg_list = np.loadtxt(hvg_list_path, dtype=str)
        else:
            hvg_list = self.adata.var_names
            logging.warning("No HVG list provided or found. Using all genes.")
        
        self.adata_hvg = self.adata[:, self.adata.var_names.isin(hvg_list)].copy()
        self.gene_names_hvg = self.adata_hvg.var_names.to_numpy()
        
        self.adjacency_matrix = self.adata.obsp['spatial_connectivities'].tocsr()

        if self.config['preprocessing']['precompute_embeddings']:
            if open_clip is None:
                raise ImportError("`open-clip-torch` is required to pre-compute embeddings. Install with `pip install open-clip-torch`.")
            logging.info("üîß Loading OmiCLIP model...")
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model, _, image_preprocessor = open_clip.create_model_and_transforms(
                self.config['model']['model_name'], pretrained=self.config['model']['model_path'], device=device)
            model.eval()
            self.model_resources = {
                "model": model, 
                "image_preprocessor": image_preprocessor, 
                "tokenizer": open_clip.get_tokenizer(self.config['model']['model_name']),
                "device": device
            }
            logging.info(f"üîå Using device: {device}")
        logging.info("--- Resource Loading Complete ---")

    def run(self):
        """Executes the main parallel processing pipeline."""
        start_time = time.time()
        output_dir = self.config['paths']['output_dir']
        os.makedirs(output_dir, exist_ok=True)
        shards_pattern = os.path.join(output_dir, "shard-%06d.tar")
        
        num_to_process = self.config['performance'].get('num_spots_to_process', -1)
        if num_to_process != -1:
            spots_to_process = self.adata.obs.head(num_to_process)
            logging.info(f"üî¨ Processing a subset of {len(spots_to_process)} spots.")
        else:
            spots_to_process = self.adata.obs
            logging.info(f"‚úÖ Processing all {len(spots_to_process)} spots.")
        
        qc_config = self.config.get('qc', {})
        qc_enabled = qc_config.get('enabled', True)
        num_visual_samples = qc_config.get('num_visual_samples', 0)
        self.qc_samples_collected.clear()
        qc_indices = set(random.sample(range(len(spots_to_process)), k=min(num_visual_samples, len(spots_to_process)))) if qc_enabled else set()

        worker_fn = partial(
            _process_subgraph_to_sample,
            adata=self.adata, adata_hvg=self.adata_hvg, adjacency_matrix=self.adjacency_matrix,
            gene_names_hvg=self.gene_names_hvg, image_handler=self.image_handler,
            config=self.config, model_resources=self.model_resources
        )
        
        success_count, error_count = 0, 0
        with wds.ShardWriter(shards_pattern, maxcount=self.config['performance']['max_samples_per_shard']) as sink:
            with ThreadPoolExecutor(max_workers=self.config['performance']['max_workers']) as executor:
                futures = {
                    executor.submit(worker_fn, spots_to_process.iloc[i], collect_qc_sample=(i in qc_indices)): i 
                    for i in range(len(spots_to_process))
                }
                
                pbar = TQDM_BAR(as_completed(futures), total=len(spots_to_process), desc="Generating Shards", unit="spot")
                for future in pbar:
                    sample, qc_sample, error_msg = future.result()
                    if sample:
                        sink.write(sample)
                        success_count += 1
                        if qc_sample:
                            self.qc_samples_collected.append(qc_sample)
                        
                        graph_info = json.loads(sample['json'].decode('utf-8'))
                        self.metrics['graph_stats']['num_nodes'].append(graph_info['num_nodes'])
                        self.metrics['graph_stats']['num_edges'].append(len(graph_info['edge_index']))
                    else:
                        error_count += 1
                        if error_count < 20:
                            logging.warning(error_msg)
        
        self._finalize_run(start_time, success_count, error_count)
        return self

    def _finalize_run(self, start_time, success_count, error_count):
        """Logs metrics and generates QC artifacts after the run."""
        elapsed_time = time.time() - start_time
        self.metrics['timing']['total_runtime_minutes'] = round(elapsed_time / 60, 2)
        self.metrics['timing']['spots_per_second'] = round(success_count / elapsed_time if elapsed_time > 0 else 0, 2)
        self.metrics['counts']['spots_processed'] = success_count
        self.metrics['counts']['spots_failed'] = error_count
        
        if self.metrics['graph_stats']['num_nodes']:
            self.metrics['graph_stats']['avg_nodes_per_subgraph'] = round(np.mean(self.metrics['graph_stats']['num_nodes']), 2)
            self.metrics['graph_stats']['avg_edges_per_subgraph'] = round(np.mean(self.metrics['graph_stats']['num_edges']), 2)
            self.metrics['graph_stats']['max_nodes_per_subgraph'] = int(np.max(self.metrics['graph_stats']['num_nodes']))
        
        logging.info("\n" + "="*80)
        logging.info("üèÅ Preprocessing Pipeline Finished!")
        logging.info(f"  - Successfully processed: {success_count} spots")
        logging.info(f"  - Skipped due to errors:  {error_count} spots")
        logging.info(f"  - Total time:             {self.metrics['timing']['total_runtime_minutes']:.2f} minutes")
        logging.info(f"  - Avg. throughput:        {self.metrics['timing']['spots_per_second']:.2f} spots/sec")
        logging.info(f"  - Output saved to:        {self.config['paths']['output_dir']}")
        
        if self.config.get('qc', {}).get('enabled', True):
            generate_summary_report(self.metrics, self.config['paths']['output_dir'])
            generate_visual_artifact(self.qc_samples_collected, self.config['paths']['output_dir'], self.config['qc']['num_visual_samples'])
        
        logging.info("="*80)
        
    def display_samples(self):
        """Displays the QC visual artifact directly in a notebook environment."""
        if not _is_notebook():
            logging.warning("Sample display is only available in a notebook environment.")
            return
        
        artifact_path = os.path.join(self.config['paths']['output_dir'], "qc_sample_grid.png")
        if os.path.exists(artifact_path):
            display_visual_artifact_notebook(artifact_path)
        else:
            logging.error("QC visual artifact not found. Please run the pipeline first with QC enabled.")


def create_dataset_shards(config: Dict[str, Any]):
    """
    High-level function to instantiate and run the SpaglamPipeline.
    This can be called from the CLI or a script.
    
    Args:
        config: A dictionary containing the pipeline configuration.
    """
    pipeline = SpaglamPipeline(config)
    pipeline.run()

===== src/spaglam_preproc/core/__init__.py =====

===== src/spaglam_preproc/core/image_tiler.py =====
# spaglam_preproc/core/image_tiler.py

import logging
from pathlib import Path
from typing import Union, Optional

import numpy as np
from PIL import Image

# openslide-python is an optional dependency
try:
    import openslide
except ImportError:
    openslide = None

# squidpy is an optional dependency for reading from AnnData


class ImageHandler:
    """
    A unified interface to handle various image sources for tile extraction.
    It can be initialized with an AnnData object, a file path (WSI or standard image),
    or a pre-loaded image object (PIL Image or NumPy array).
    """
    def __init__(self, source: Optional[Union[str, Path, object]], adata: Optional[object] = None):
        self.image_obj = None
        self.width, self.height = 0, 0
        self._load_image(source, adata)

    def _load_image(self, source, adata):
        """Internal method to load the image from the specified source."""
        # 1. Try to load from AnnData object if it's the primary source
        if source is None and adata is not None:
            spatial_key = list(adata.uns.get('spatial', {}).keys())
            if spatial_key:
                # Assuming standard squidpy storage format
                img_container = adata.uns['spatial'][spatial_key[0]]['images'].get('hires')
                self.image_obj = img_container
                self.width, self.height = self.image_obj.shape[1], self.image_obj.shape[0]
                logging.info(f"Loaded image '{spatial_key[0]}' from AnnData object.")
                return
            else:
                 raise ValueError("Image source is None and no spatial image found in adata.uns['spatial'].")
        
        if source is None:
            raise ValueError("No image source provided (path or AnnData).")

        # 2. Handle file paths (WSI or standard formats)
        if isinstance(source, (str, Path)):
            path = Path(source)
            if not path.is_file():
                raise FileNotFoundError(f"Image file not found at: {path}")

            wsi_extensions = {".svs", ".tiff", ".tif", ".vms", ".vmu", ".ndpi", ".scn", ".mrxs", ".svslide"}
            if openslide and path.suffix.lower() in wsi_extensions:
                try:
                    self.image_obj = openslide.OpenSlide(str(path))
                    self.width, self.height = self.image_obj.dimensions
                    logging.info(f"Loaded WSI image: {path}")
                    return
                except openslide.OpenSlideError:
                    logging.warning(f"Could not open {path} with openslide, trying Pillow.")

            try:
                img = Image.open(path)
                self.image_obj = img.convert("RGB")
                self.width, self.height = self.image_obj.size
                logging.info(f"Loaded standard image with Pillow: {path}")
                return
            except Exception as e:
                raise IOError(f"Failed to load image file {path} with both OpenSlide and Pillow.") from e

        # 3. Handle pre-loaded image objects
        elif isinstance(source, Image.Image):
            self.image_obj = source.convert("RGB")
            self.width, self.height = self.image_obj.size
            logging.info("Loaded image from pre-loaded PIL.Image object.")
            return
        elif isinstance(source, np.ndarray):
            self.image_obj = Image.fromarray(source).convert("RGB")
            self.width, self.height = self.image_obj.size
            logging.info("Loaded image from pre-loaded NumPy array.")
            return
        
        raise TypeError(f"Unsupported image source type: {type(source)}")

    def get_dimensions(self) -> tuple[int, int]:
        return self.width, self.height

    def get_tile(self, coordinates: np.ndarray, tile_size: int) -> Image.Image:
        """
        Extracts a single image tile in memory. Handles boundary conditions.
        """
        col, row = int(round(coordinates[0])), int(round(coordinates[1]))
        half_tile = tile_size // 2

        top_left_x = col - half_tile
        top_left_y = row - half_tile

        read_left = max(top_left_x, 0)
        read_top = max(top_left_y, 0)
        read_right = min(top_left_x + tile_size, self.width)
        read_bottom = min(top_left_y + tile_size, self.height)
        
        read_width = read_right - read_left
        read_height = read_bottom - read_top

        if read_width <= 0 or read_height <= 0:
            return Image.new("RGB", (tile_size, tile_size), (255, 255, 255))
        
        if openslide and isinstance(self.image_obj, openslide.OpenSlide):
            region = self.image_obj.read_region((read_left, read_top), 0, (read_width, read_height)).convert("RGB")

        elif isinstance(self.image_obj, Image.Image):
            region = self.image_obj.crop((read_left, read_top, read_right, read_bottom))
        else:
            raise TypeError(f"Cannot extract tile from unsupported image object type: {type(self.image_obj)}")

        tile_img = Image.new("RGB", (tile_size, tile_size), (255, 255, 255))
        paste_x = read_left - top_left_x
        paste_y = read_top - top_left_y
        tile_img.paste(region, (paste_x, paste_y))
        
        return tile_img

===== src/spaglam_preproc/core/graph_builder.py =====
# spaglam_preproc/core/graph_builder.py

from collections import deque
from scipy.sparse import csr_matrix

def get_k_hop_neighborhood(
    adjacency_matrix: csr_matrix, start_node_idx: int, k: int
) -> list[int]:
    """
    Finds all unique nodes within k hops of a starting node using Breadth-First Search (BFS).
    This is the most efficient method for this task.

    Args:
        adjacency_matrix: The sparse adjacency matrix (e.g., from adata.obsp['spatial_connectivities']).
        start_node_idx: The integer index of the starting node.
        k: The number of hops (e.g., 1 for immediate neighbors, 2 for neighbors of neighbors).

    Returns:
        A list of unique integer indices of all nodes in the k-hop neighborhood, including the start node.
    """
    if k == 0:
        return [start_node_idx]

    visited = {start_node_idx}
    # queue stores tuples of (node_index, current_hop_level)
    queue = deque([(start_node_idx, 0)])
    
    # We add the start node to the final list
    neighborhood = [start_node_idx]

    while queue:
        current_node, level = queue.popleft()

        if level >= k:
            continue

        # Get neighbors using the efficient .indices attribute of CSR matrices
        # adjacency_matrix.indices[start:end] slices the column indices for a given row
        start_ptr = adjacency_matrix.indptr[current_node]
        end_ptr = adjacency_matrix.indptr[current_node + 1]
        neighbors = adjacency_matrix.indices[start_ptr:end_ptr]

        for neighbor in neighbors:
            if neighbor not in visited:
                visited.add(neighbor)
                neighborhood.append(neighbor)
                queue.append((neighbor, level + 1))
                
    return neighborhood

===== src/spaglam_preproc/utils/logging_setup.py =====
# spaglam_preproc/utils/logging_setup.py

import logging
from rich.logging import RichHandler

def setup_logging(log_path: str):
    """
    Configures a rich logger to print to the console and a file.

    Args:
        log_path: Path to the output log file.
    """
    # Configure the rich handler for beautiful console output
    rich_handler = RichHandler(
        rich_tracebacks=True,
        show_path=False,
        log_time_format="[%Y-%m-%d %H:%M:%S]",
        tracebacks_suppress=[__import__("typer")], # Suppress typer's internal traceback frames
    )
    
    # Configure the file handler for persistent logging
    file_handler = logging.FileHandler(log_path, mode='w') # Overwrite log on each run
    file_handler.setFormatter(
        logging.Formatter("%(asctime)s [%(levelname)-8s] %(name)s - %(message)s")
    )

    # Get the root logger and add handlers
    # We set the level on the handlers individually to control verbosity
    rich_handler.setLevel(logging.INFO)
    file_handler.setLevel(logging.INFO)
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
        datefmt="[%X]",
        handlers=[rich_handler, file_handler]
    )

===== src/spaglam_preproc/utils/anndata_utils.py =====
# spaglam_preproc/utils/anndata_utils.py
import anndata
import numpy as np

def safe_get_spatial_coords(adata: anndata.AnnData) -> np.ndarray:
    """
    Safely retrieves spatial coordinates from an AnnData object.
    Checks for common keys and validates the shape.

    Args:
        adata: The AnnData object.

    Returns:
        A NumPy array of shape (n_obs, 2) with spatial coordinates.
    
    Raises:
        ValueError: If no valid spatial coordinates are found.
    """
    if 'spatial' in adata.obsm:
        coords = adata.obsm['spatial']
        if isinstance(coords, np.ndarray) and coords.ndim == 2 and coords.shape[1] >= 2:
            return coords[:, :2]  # Return only the first two columns (x, y)
    
    raise ValueError(
        "Could not find valid spatial coordinates in `adata.obsm['spatial']`. "
        "Expected a NumPy array of shape (n_obs, 2)."
    )

===== src/spaglam_preproc/utils/hest_loading.py =====
# Êñá‰ª∂Ôºöhest_loading.py

import os
import glob
import json  # Êñ∞Â¢ûÔºöÁî®‰∫éÂä†ËΩΩ JSON Êñá‰ª∂
import pandas as pd
import numpy as np
import scanpy as sc
import anndata
import openslide
from typing import Optional, Union, List, Dict
from tqdm import tqdm
import matplotlib.pyplot as plt
from PIL import Image

class HESTSample:
    """
    Ë°®Á§∫Âçï‰∏™ HEST Ê†∑Êú¨ÔºåÂåÖÊã¨:
      - Sample ID (Â¶Ç 'TENX95')
      - AnnData ÂØπË±° (Á©∫Èó¥ËΩ¨ÂΩïÁªÑÊï∞ÊçÆ)
      - Whole Slide Image (WSI) ÂØπË±°
      - ÂÖ∂‰ªñÂèØÈÄâÊñá‰ª∂ÔºåÂ¶Ç patches, transcripts Á≠â
    ÊîØÊåÅÊáíÂä†ËΩΩÂíåÂÖ®Âä†ËΩΩÊ®°Âºè„ÄÇ
    """

    def __init__(
        self,
        sample_id: str,
        st_path: str,
        wsi_path: str,
        patches_dir: Optional[str] = None,
        transcripts_path: Optional[str] = None,
        metadata_dict: Optional[Dict] = None,
        spatial_plot_path: Optional[str] = None,
        load_adata: bool = True,
        adata_lazy: bool = True,
        load_wsi: bool = True
    ):
        """
        ÂàùÂßãÂåñÊ†∑Êú¨ÂØπË±°„ÄÇ

        ÂèÇÊï∞:
          sample_id (str): Ê†∑Êú¨ ID„ÄÇ
          st_path (str): .h5ad Êñá‰ª∂Ë∑ØÂæÑ„ÄÇ
          wsi_path (str): WSI Êñá‰ª∂Ë∑ØÂæÑ„ÄÇ
          patches_dir (Optional[str]): patches Êñá‰ª∂Â§πË∑ØÂæÑ„ÄÇ
          transcripts_path (Optional[str]): transcripts Êñá‰ª∂Ë∑ØÂæÑ„ÄÇ
          metadata_dict (Optional[Dict]): ÂÖÉÊï∞ÊçÆÂ≠óÂÖ∏„ÄÇ
          spatial_plot_path (Optional[str]): È¢ÑÁîüÊàêÁöÑÁ©∫Èó¥ËΩ¨ÂΩïÁªÑÂõæÂÉèË∑ØÂæÑ„ÄÇ
          load_adata (bool): ÊòØÂê¶Âä†ËΩΩ AnnData ÂØπË±°„ÄÇ
          adata_lazy (bool): Â¶ÇÊûúÂä†ËΩΩ AnnDataÔºåÊòØÂê¶‰ΩøÁî®ÊáíÂä†ËΩΩÊ®°Âºè„ÄÇ
          load_wsi (bool): ÊòØÂê¶Âä†ËΩΩ WSI ÂõæÂÉè„ÄÇ
        """
        self.sample_id = sample_id
        self.st_path = st_path
        self.wsi_path = wsi_path
        self.patches_dir = patches_dir
        self.transcripts_path = transcripts_path
        self.metadata_dict = metadata_dict if metadata_dict else {}
        self.spatial_plot_path = spatial_plot_path  # È¢ÑÁîüÊàêÁöÑÁ©∫Èó¥ËΩ¨ÂΩïÁªÑÂõæË∑ØÂæÑ

        self.adata = None  # AnnData ÂØπË±°
        self.wsi = None    # OpenSlide ÂØπË±°

        if load_adata:
            self.adata = self.load_st_data(lazy=adata_lazy)
        
        if load_wsi:
            self.wsi = self.load_wsi()

    def __repr__(self):
        repr_str = f"HESTSample(sample_id={self.sample_id})\n"
        repr_str += f"  ST file: {self.st_path}\n"
        repr_str += f"  WSI file: {self.wsi_path}\n"
        if self.transcripts_path:
            repr_str += f"  transcripts: {self.transcripts_path}\n"
        if self.patches_dir:
            repr_str += f"  patches dir: {self.patches_dir}\n"
        if self.spatial_plot_path:
            repr_str += f"  spatial plot: {self.spatial_plot_path}\n"
        return repr_str

    # ---------------------------
    # 1) ËØªÂèñ/Âä†ËΩΩ ST Êï∞ÊçÆ
    # ---------------------------
    def load_st_data(self, lazy: bool = True) -> Optional[anndata.AnnData]:
        """
        Âä†ËΩΩ ST Êï∞ÊçÆ (AnnData ÂØπË±°)„ÄÇ

        ÂèÇÊï∞:
          lazy (bool): ÊòØÂê¶‰ΩøÁî®ÊáíÂä†ËΩΩÊ®°Âºè (backed='r')„ÄÇ

        ËøîÂõû:
          Optional[anndata.AnnData]: Âä†ËΩΩÁöÑ AnnData ÂØπË±°Êàñ None„ÄÇ
        """
        if self.adata is not None:
            return self.adata  # Â∑≤Âä†ËΩΩ

        if not os.path.exists(self.st_path):
            print(f"AnnData Êñá‰ª∂‰∏çÂ≠òÂú®: {self.st_path}")
            return None

        try:
            if lazy:
                self.adata = sc.read_h5ad(self.st_path, backed='r')
            else:
                self.adata = sc.read_h5ad(self.st_path)
            return self.adata
        except Exception as e:
            print(f"Âä†ËΩΩ AnnData Â§±Ë¥•: {e}")
            return None

    def visualize_comparison(
        self, 
        color: Optional[Union[str, List[str]]] = None,
        use_precomputed_spatial_plot: bool = True
    ) -> pd.DataFrame:
        """
        ÂàõÂª∫‰∏Ä‰∏™ÂåÖÂê´ WSI Âíå ST Êï∞ÊçÆÁöÑÁªºÂêàÂõæÂÉèÔºåÂπ∂ËøîÂõû QC Êï∞ÊçÆÁöÑ DataFrame„ÄÇ
        """
        if self.adata is None:
            print("AnnData ÂØπË±°Êú™Âä†ËΩΩ„ÄÇËØ∑Á°Æ‰øùÂú®ÂàùÂßãÂåñÊó∂Âä†ËΩΩÊàñÊâãÂä®Âä†ËΩΩ„ÄÇ")
            return pd.DataFrame()

        # Ê£ÄÊü•ÊòØÂê¶‰ΩøÁî®È¢ÑÁîüÊàêÁöÑÁ©∫Èó¥ËΩ¨ÂΩïÁªÑÂõæÂÉè
        if use_precomputed_spatial_plot and self.spatial_plot_path and os.path.exists(self.spatial_plot_path):
            spatial_img = Image.open(self.spatial_plot_path)
            use_precomputed = True
        else:
            spatial_img = self.generate_spatial_plot(color=color)
            use_precomputed = False

        # ÂàõÂª∫Â≠êÂõæÔºåË∞ÉÊï¥Â∏ÉÂ±Ä‰ΩøÂÖ∂Êõ¥Á¥ßÂáë
        fig, axes = plt.subplots(1, 2, figsize=(16, 8), gridspec_kw={'width_ratios': [1, 1]})

        # 1. ÊòæÁ§∫ WSI Áº©Áï•Âõæ
        if self.wsi:
            thumb = self.get_wsi_thumbnail(level=0, downsample=128)
            axes[0].imshow(thumb)
            axes[0].set_title(f"{self.sample_id} - WSI Thumbnail")
            axes[0].axis('off')
            
            # Âú® WSI ÂõæÂÉè‰∏äÊ∑ªÂä†ÈáçË¶ÅÁöÑÂÖÉÊï∞ÊçÆ
            metadata_text = "\n".join([
                f"Sample ID: {self.metadata_dict.get('Sample ID', 'N/A')}",
                f"Organ: {self.metadata_dict.get('organ', 'N/A')}",
                f"Species: {self.metadata_dict.get('species', 'N/A')}",
                f"Disease State: {self.metadata_dict.get('disease_state', 'N/A')}",
                f"Technology: {self.metadata_dict.get('st_technology', 'N/A')}"
            ])
            axes[0].text(10, 30, metadata_text, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))
        else:
            axes[0].text(0.5, 0.5, 'WSI not loaded.', 
                        horizontalalignment='center', 
                        verticalalignment='center', 
                        fontsize=12)
            axes[0].axis('off')

        # 2. ÊòæÁ§∫ ST Êï∞ÊçÆ
        if use_precomputed:
            axes[1].imshow(spatial_img)
            axes[1].set_title(f"{self.sample_id} - Spatial Transcriptomics (Precomputed)")
            axes[1].axis('off')
        else:
            sc.pl.spatial(
                self.adata, 
                img=self.get_wsi_thumbnail(level=0, downsample=64),  # ‰ΩøÁî®Áº©Áï•Âõæ‰Ωú‰∏∫ËÉåÊôØ
                color=color if color else 'clusters',
                show=False,
                ax=axes[1]
            )
            axes[1].set_title(f"{self.sample_id} - Spatial Transcriptomics")

        plt.tight_layout()
        plt.show()

        # ============ ‰øÆÊîπÁöÑ QC ÊåáÊ†áÈÉ®ÂàÜ ============
        # Âè™ÁªüËÆ°Êàë‰ª¨Á°ÆÂÆûÊã•ÊúâÁöÑ„ÄÅÂØπÊï∞ÊçÆÈáçË¶ÅÁöÑÂ≠óÊÆµ
        # ÂèØÊåâÂÆûÈôÖÈúÄË¶ÅËøõË°åÂà†ÂáèÊàñË∞ÉÊï¥
        qc_metrics = {
            'spots_under_tissue':  self.metadata_dict.get('spots_under_tissue', np.nan),
            'nb_genes':            self.metadata_dict.get('nb_genes', np.nan),
            'inter_spot_dist':     self.metadata_dict.get('inter_spot_dist', np.nan),
            'spot_diameter':       self.metadata_dict.get('spot_diameter', np.nan),
            'pixel_size_um_embed': self.metadata_dict.get('pixel_size_um_embedded', np.nan),
            'pixel_size_um_est':   self.metadata_dict.get('pixel_size_um_estimated', np.nan),
            'fullres_px_width':    self.metadata_dict.get('fullres_px_width', np.nan),
            'fullres_px_height':   self.metadata_dict.get('fullres_px_height', np.nan)
        }

        qc_df = pd.DataFrame([qc_metrics])
        return qc_df

    def generate_spatial_plot(self, color: Optional[Union[str, List[str]]] = None) -> Image.Image:
        """
        ÂÆûÊó∂ÁîüÊàêÁ©∫Èó¥ËΩ¨ÂΩïÁªÑÂõæÂÉè„ÄÇ

        ÂèÇÊï∞:
          color (Optional[Union[str, List[str]]]): Scanpy ÂèØËßÜÂåñÁöÑÈ¢úËâ≤ÂèÇÊï∞„ÄÇ

        ËøîÂõû:
          Image.Image: ÁîüÊàêÁöÑÁ©∫Èó¥ËΩ¨ÂΩïÁªÑÂõæÂÉè„ÄÇ
        """
        try:
            # ËøõË°åÂü∫Êú¨ÁöÑÈ¢ÑÂ§ÑÁêÜ
            sc.pp.normalize_total(self.adata, target_sum=1e4)
            sc.pp.log1p(self.adata)
            sc.pp.highly_variable_genes(self.adata, flavor='seurat', n_top_genes=2000)
            self.adata = self.adata[:, self.adata.var['highly_variable']]
            sc.pp.scale(self.adata, max_value=10)
            sc.tl.pca(self.adata, svd_solver='arpack')
            sc.pp.neighbors(self.adata, n_neighbors=10, n_pcs=40)
            sc.tl.umap(self.adata)
            sc.tl.leiden(self.adata, key_added='clusters')

            # Ê£ÄÊü• scalefactors
            scalefactors = self.adata.uns.get('spatial', {}).get('scalefactors', {})
            if scalefactors:
                # ÂèñÁ¨¨‰∏Ä‰∏™ scalefactor
                scalefactor_key = next(iter(scalefactors))
                scale_factor = scalefactors[scalefactor_key]
            else:
                print("Êú™ÊâæÂà∞ scalefactorsÔºå‰ΩøÁî®ÈªòËÆ§Áº©ÊîæÂõ†Â≠ê 1.0")
                scalefactors = {'tissue_image_scalef': 1.0}
                self.adata.uns['spatial'] = {'scalefactors': scalefactors}
                scale_factor = 1.0

            # ‰ΩøÁî® Scanpy ÁîüÊàêÂõæÂÉè
            fig = sc.pl.spatial(
                self.adata, 
                img=self.get_wsi_thumbnail(level=0, downsample=64),  # ‰ΩøÁî®Áº©Áï•Âõæ‰Ωú‰∏∫ËÉåÊôØ
                color=color if color else 'clusters',
                show=False
            )
            fig.savefig("temp_spatial_plot.png")
            spatial_img = Image.open("temp_spatial_plot.png")
            os.remove("temp_spatial_plot.png")
            return spatial_img
        except Exception as e:
            print(f"ÁîüÊàêÁ©∫Èó¥ËΩ¨ÂΩïÁªÑÂõæÂÉèÂ§±Ë¥•: {e}")
            return Image.new('RGB', (640, 480), color = 'white')

    # ---------------------------
    # 2) ËØªÂèñ/Âä†ËΩΩ WSI Êï∞ÊçÆ
    # ---------------------------
    def load_wsi(self) -> Optional[openslide.OpenSlide]:
        """
        ÊâìÂºÄÂØπÂ∫îÁöÑ H&E Whole Slide Image (WSI)„ÄÇ

        ËøîÂõû:
          Optional[openslide.OpenSlide]: Âä†ËΩΩÁöÑ OpenSlide ÂØπË±°Êàñ None„ÄÇ
        """
        if self.wsi is not None:
            return self.wsi  # Â∑≤Âä†ËΩΩ

        if not os.path.exists(self.wsi_path):
            print(f"WSI Êñá‰ª∂‰∏çÂ≠òÂú®: {self.wsi_path}")
            return None

        try:
            self.wsi = openslide.OpenSlide(self.wsi_path)
            return self.wsi
        except Exception as e:
            print(f"Âä†ËΩΩ WSI Â§±Ë¥•: {e}")
            return None

    def get_wsi_thumbnail(self, level: int = 0, downsample: int = 32) -> np.ndarray:
        """
        Ëé∑Âèñ WSI ÁöÑÁº©Áï•ÂõæÔºåÁî®‰∫éÂø´ÈÄüÂèØËßÜÂåñ„ÄÇ

        ÂèÇÊï∞:
          level (int): ËØªÂèñ OpenSlide ÁöÑÂ±ÇÁ∫ß (0 ‰∏∫ÊúÄÈ´òÂàÜËæ®Áéá)„ÄÇ
          downsample (int): È¢ùÂ§ñÁöÑ downsample Âõ†Â≠ê„ÄÇ

        ËøîÂõû:
          np.ndarray: Áº©Áï•ÂõæÁöÑ NumPy Êï∞ÁªÑ„ÄÇ
        """
        if not self.wsi:
            print("WSI ÂØπË±°Êú™Âä†ËΩΩ„ÄÇ")
            return np.zeros((100, 100, 3), dtype=np.uint8)  # ËøîÂõû‰∏Ä‰∏™Á©∫ÁôΩÂõæÂÉè

        try:
            dims = self.wsi.level_dimensions[level]  # (width, height)
            new_size = (dims[0] // downsample, dims[1] // downsample)
            region = self.wsi.read_region((0, 0), level, dims)
            region = region.resize(new_size)
            return np.array(region.convert("RGB"))
        except Exception as e:
            print(f"Ëé∑Âèñ WSI Áº©Áï•ÂõæÂ§±Ë¥•: {e}")
            return np.zeros((100, 100, 3), dtype=np.uint8)  # ËøîÂõû‰∏Ä‰∏™Á©∫ÁôΩÂõæÂÉè

    # ---------------------------
    # 3) ËØªÂèñ Patches / Transcripts Á≠â
    # ---------------------------
    def list_patches(self) -> List[str]:
        """
        ÂàóÂá∫ËØ•Ê†∑Êú¨ patches Êñá‰ª∂Â§π‰∏≠ÁöÑÊñá‰ª∂ (Ëã•Â≠òÂú®)„ÄÇ

        ËøîÂõû:
          List[str]: patches Êñá‰ª∂Ë∑ØÂæÑÂàóË°®„ÄÇ
        """
        if not self.patches_dir or not os.path.isdir(self.patches_dir):
            return []
        return sorted(glob.glob(os.path.join(self.patches_dir, "*.h5")))

    def load_transcripts(self) -> Optional[pd.DataFrame]:
        """
        Ëã•ÊòØ Xenium ÊäÄÊúØÊàñÊúâËΩ¨ÂΩïÊú¨ parquet Êñá‰ª∂ÔºåÂèØ‰ª•‰ªé self.transcripts_path ËØªÂèñ„ÄÇ

        ËøîÂõû:
          Optional[pd.DataFrame]: ËΩ¨ÂΩïÊú¨Êï∞ÊçÆÁöÑ DataFrame Êàñ None„ÄÇ
        """
        if self.transcripts_path and os.path.exists(self.transcripts_path):
            try:
                return pd.read_parquet(self.transcripts_path)
            except Exception as e:
                print(f"Âä†ËΩΩ transcripts Â§±Ë¥•: {e}")
                return None
        else:
            return None

class HESTDataset:
    """
    ÁÆ°ÁêÜÊï¥‰∏™ HEST Êï∞ÊçÆÈõÜÔºåËØªÂèñÂÖ®Â±ÄÂÖÉÊï∞ÊçÆÂπ∂Ê†πÊçÆÊü•ËØ¢Êù°‰ª∂Á≠õÈÄâÊ†∑Êú¨„ÄÇ
    """

    def __init__(self, data_dir: str):
        self.data_dir = data_dir
        self.metadata_csv = os.path.join(data_dir, "HEST_v1_1_0.csv")
        if not os.path.exists(self.metadata_csv):
            raise FileNotFoundError(f"Êú™ÊâæÂà∞ metadata CSV: {self.metadata_csv}")

        # ËØªÂèñ metadata
        self.meta_df = pd.read_csv(self.metadata_csv)
        self.samples_dict = {}  # Â≠òÂÇ® sample_id -> HESTSample

    def query_samples(
            self, 
            organ: Optional[str] = None, 
            oncotree_code: Optional[str] = None, 
            sample_ids: Optional[List[str]] = None,
            disease_state: Optional[str] = None,
            species: Optional[str] = None,
            st_technology: Optional[str] = None,
            preservation_method: Optional[str] = None,
            nb_genes: Optional[int] = None,
            data_publication_date: Optional[str] = None,
            license: Optional[str] = None,
            tissue: Optional[str] = None,
            subseries: Optional[str] = None,
            # ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅÁªßÁª≠Ê∑ªÂä†Êõ¥Â§öÁöÑÁ≠õÈÄâÊù°‰ª∂
        ) -> pd.DataFrame:
            """
            Ê†πÊçÆÂ§ö‰∏™Á≠õÈÄâÊù°‰ª∂Âú® metadata ÈáåËøáÊª§Ê†∑Êú¨„ÄÇ

            ÂèÇÊï∞:
                organ (Optional[str]): Âô®ÂÆòÁ±ªÂûã„ÄÇ
                oncotree_code (Optional[str]): ÁôåÁóáÁ±ªÂûã‰ª£Á†Å„ÄÇ
                sample_ids (Optional[List[str]]): ÁâπÂÆöÁöÑÊ†∑Êú¨ ID ÂàóË°®„ÄÇ
                disease_state (Optional[str]): ÁñæÁóÖÁä∂ÊÄÅ„ÄÇ
                species (Optional[str]): Áâ©Áßç„ÄÇ
                st_technology (Optional[str]): ÊäÄÊúØÁ±ªÂûã„ÄÇ
                preservation_method (Optional[str]): ‰øùÂ≠òÊñπÊ≥ï„ÄÇ
                nb_genes (Optional[int]): Âü∫Âõ†Êï∞Èáè„ÄÇ
                data_publication_date (Optional[str]): Êï∞ÊçÆÂèëË°®Êó•Êúü„ÄÇ
                license (Optional[str]): ËÆ∏ÂèØËØÅÁ±ªÂûã„ÄÇ
                tissue (Optional[str]): ÁªÑÁªáÁ±ªÂûã„ÄÇ
                subseries (Optional[str]): Â≠êÁ≥ªÂàó„ÄÇ
                # ÂÖ∂‰ªñÂèÇÊï∞...

            ËøîÂõû:
                pd.DataFrame: ËøáÊª§ÂêéÁöÑÂÖÉÊï∞ÊçÆ DataFrame„ÄÇ
            """
            df = self.meta_df.copy()
            if organ:
                df = df[df['organ'] == organ]
            if oncotree_code:
                df = df[df['oncotree_code'] == oncotree_code]
            if sample_ids:
                df = df[df['id'].isin(sample_ids)]
            if disease_state:
                df = df[df['disease_state'] == disease_state]
            if species:
                df = df[df['species'] == species]
            if st_technology:
                df = df[df['st_technology'] == st_technology]
            if preservation_method:
                df = df[df['preservation_method'] == preservation_method]
            if nb_genes is not None:
                df = df[df['nb_genes'] == nb_genes]
            if data_publication_date:
                df = df[df['data_publication_date'] == data_publication_date]
            if license:
                df = df[df['license'] == license]
            if tissue:
                df = df[df['tissue'] == tissue]
            if subseries:
                df = df[df['subseries'] == subseries]
            # ÁªßÁª≠Ê∑ªÂä†Êõ¥Â§öÁöÑÁ≠õÈÄâÊù°‰ª∂
            return df


    def get_samples(
        self, 
        organ: Optional[str] = None, 
        oncotree_code: Optional[str] = None, 
        sample_ids: Optional[List[str]] = None,
        disease_state: Optional[str] = None,
        species: Optional[str] = None,
        st_technology: Optional[str] = None,
        preservation_method: Optional[str] = None,
        nb_genes: Optional[int] = None,
        data_publication_date: Optional[str] = None,
        license: Optional[str] = None,
        tissue: Optional[str] = None,
        subseries: Optional[str] = None,
        # ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅÁªßÁª≠Ê∑ªÂä†Êõ¥Â§öÁöÑÁ≠õÈÄâÊù°‰ª∂
    ) -> List[HESTSample]:
        """
        ËøîÂõûÊª°Ë∂≥Êù°‰ª∂ÁöÑ HESTSample ÂÆû‰æãÂàóË°®„ÄÇ

        ÂèÇÊï∞:
            organ (Optional[str]): Âô®ÂÆòÁ±ªÂûã„ÄÇ
            oncotree_code (Optional[str]): ÁôåÁóáÁ±ªÂûã‰ª£Á†Å„ÄÇ
            sample_ids (Optional[List[str]]): ÁâπÂÆöÁöÑÊ†∑Êú¨ ID ÂàóË°®„ÄÇ
            disease_state (Optional[str]): ÁñæÁóÖÁä∂ÊÄÅ„ÄÇ
            species (Optional[str]): Áâ©Áßç„ÄÇ
            st_technology (Optional[str]): ÊäÄÊúØÁ±ªÂûã„ÄÇ
            preservation_method (Optional[str]): ‰øùÂ≠òÊñπÊ≥ï„ÄÇ
            nb_genes (Optional[int]): Âü∫Âõ†Êï∞Èáè„ÄÇ
            data_publication_date (Optional[str]): Êï∞ÊçÆÂèëË°®Êó•Êúü„ÄÇ
            license (Optional[str]): ËÆ∏ÂèØËØÅÁ±ªÂûã„ÄÇ
            tissue (Optional[str]): ÁªÑÁªáÁ±ªÂûã„ÄÇ
            subseries (Optional[str]): Â≠êÁ≥ªÂàó„ÄÇ
            # ÂÖ∂‰ªñÂèÇÊï∞...

        ËøîÂõû:
            List[HESTSample]: Êª°Ë∂≥Êù°‰ª∂ÁöÑÊ†∑Êú¨ÂàóË°®„ÄÇ
        """
        df = self.query_samples(
            organ=organ,
            oncotree_code=oncotree_code,
            sample_ids=sample_ids,
            disease_state=disease_state,
            species=species,
            st_technology=st_technology,
            preservation_method=preservation_method,
            nb_genes=nb_genes,
            data_publication_date=data_publication_date,
            license=license,
            tissue=tissue,
            subseries=subseries
            # ÁªßÁª≠‰º†ÈÄíÊõ¥Â§öÁöÑÁ≠õÈÄâÊù°‰ª∂
        )
        samples = []
        metadata_dir = os.path.join(self.data_dir, "metadata")  # QC metrics ÁöÑ JSON Êñá‰ª∂ÁõÆÂΩï
        for i, row in df.iterrows():
            sid = row['id']
            # ÊûÑÈÄ† AnnData Êñá‰ª∂Ë∑ØÂæÑ
            st_path = os.path.join(self.data_dir, "st", f"{sid}.h5ad")
            if not os.path.exists(st_path):
                # ÂÖÅËÆ∏Êõ¥ÁÅµÊ¥ªÁöÑÂåπÈÖç
                st_candidates = glob.glob(os.path.join(self.data_dir, "st", f"*{sid}*.h5ad"))
                if len(st_candidates) == 0:
                    print(f"Êú™ÊâæÂà∞ AnnData Êñá‰ª∂ for {sid}")
                    continue
                st_path = st_candidates[0]

            # ÊûÑÈÄ† WSI Êñá‰ª∂Ë∑ØÂæÑ
            wsi_candidates = glob.glob(os.path.join(self.data_dir, "wsis", f"{sid}.*"))
            if len(wsi_candidates) == 0:
                print(f"Êú™ÊâæÂà∞ WSI Êñá‰ª∂ for {sid}")
                wsi_path = ""
            else:
                wsi_path = wsi_candidates[0]  # ÂèñÁ¨¨‰∏Ä‰∏™ÂåπÈÖçÁöÑ

            # patches ÁõÆÂΩï
            patches_dir = os.path.join(self.data_dir, "patches", f"{sid}")
            if not os.path.isdir(patches_dir):
                patches_dir = None

            # transcripts ÁõÆÂΩï
            transcripts_candidates = glob.glob(os.path.join(self.data_dir, "transcripts", f"{sid}*.parquet"))
            transcripts_path = transcripts_candidates[0] if transcripts_candidates else None

            # spatial_plot ÁõÆÂΩï
            spatial_plot_path = os.path.join(self.data_dir, "spatial_plots", f"{sid}_spatial_plots.png")
            if not os.path.exists(spatial_plot_path):
                spatial_plot_path = None  # Â¶ÇÊûúÊ≤°ÊúâÈ¢ÑÁîüÊàêÁöÑÂõæÂÉè

            # Âä†ËΩΩÂØπÂ∫îÁöÑ JSON Êñá‰ª∂‰Ωú‰∏∫ QC ÊåáÊ†á
            json_path = os.path.join(metadata_dir, f"{sid}.json")
            if os.path.exists(json_path):
                try:
                    with open(json_path, 'r') as f:
                        qc_data = json.load(f)
                except Exception as e:
                    print(f"Âä†ËΩΩ QC JSON Êñá‰ª∂Â§±Ë¥• for {sid}: {e}")
                    qc_data = {}
            else:
                print(f"Êú™ÊâæÂà∞ QC JSON Êñá‰ª∂ for {sid}")
                qc_data = {}

            # ÂêàÂπ∂ row.to_dict() Âíå qc_data
            metadata = row.to_dict()
            metadata.update(qc_data)  # qc_data Ë¶ÜÁõñ metadata ‰∏≠ÁöÑÁõ∏ÂêåÈîÆ

            # ÊûÑÈÄ† HESTSample ÂØπË±°
            sample = HESTSample(
                sample_id = sid,
                st_path = st_path,
                wsi_path = wsi_path,
                patches_dir = patches_dir,
                transcripts_path = transcripts_path,
                metadata_dict = metadata,
                spatial_plot_path = spatial_plot_path,
                load_adata=False,  # Á°Æ‰øùÂä†ËΩΩ AnnData ÂØπË±°
                adata_lazy=False,  # Ê†πÊçÆÈúÄË¶ÅÈÄâÊã©ÊáíÂä†ËΩΩÊàñÂÖ®Âä†ËΩΩ
                load_wsi=False      # Ê†πÊçÆÈúÄË¶ÅÈÄâÊã©Âä†ËΩΩ WSI
            )
            samples.append(sample)
        return samples

    def compute_metrics_statistics(self, samples: List[HESTSample]) -> pd.DataFrame:
        """
        ÁªüËÆ°ËøáÊª§ÂêéÁöÑÊï∞ÊçÆÈõÜÁöÑÂÖ≥ÈîÆ QC ÊåáÊ†áÁöÑÁªüËÆ°ÂÄº„ÄÇ
        """
        qc_list = []
        for sample in samples:
            # ============ ‰øÆÊîπÁöÑ QC ÊåáÊ†áÈÉ®ÂàÜ ============
            # ‰∏é‰∏äÈù¢ visual_comparison ‰∏≠‰øùÊåÅ‰∏ÄËá¥
            qc_metrics = {
                'spots_under_tissue':  sample.metadata_dict.get('spots_under_tissue', np.nan),
                'nb_genes':            sample.metadata_dict.get('nb_genes', np.nan),
                'inter_spot_dist':     sample.metadata_dict.get('inter_spot_dist', np.nan),
                'spot_diameter':       sample.metadata_dict.get('spot_diameter', np.nan),
                'pixel_size_um_embed': sample.metadata_dict.get('pixel_size_um_embedded', np.nan),
                'pixel_size_um_est':   sample.metadata_dict.get('pixel_size_um_estimated', np.nan),
                'fullres_px_width':    sample.metadata_dict.get('fullres_px_width', np.nan),
                'fullres_px_height':   sample.metadata_dict.get('fullres_px_height', np.nan)
            }
            qc_list.append(qc_metrics)
        
        qc_df = pd.DataFrame(qc_list)
        stats_df = qc_df.describe().T  # ËΩ¨ÁΩÆ‰ª•‰æøÊØè‰∏™ÊåáÊ†á‰Ωú‰∏∫Ë°å
        return stats_df
===== src/spaglam_preproc/utils/validation.py =====
# spaglam_preproc/utils/validation.py

import logging
import anndata
import numpy as np
import os
from ..core.image_tiler import ImageHandler
from .anndata_utils import safe_get_spatial_coords

def pre_run_validation(adata: anndata.AnnData, image_handler: ImageHandler, config: dict):
    """
    Performs a series of checks on inputs before starting the main processing loop.
    Raises RuntimeError if a critical validation fails.
    """
    logging.info("üî¨ Performing pre-run validation checks...")
    valid = True

    # 1. Check for required AnnData fields
    if 'spatial_connectivities' not in adata.obsp:
        logging.error("Validation failed: `adata.obsp['spatial_connectivities']` not found. A spatial graph is required.")
        valid = False
    
    try:
        coords = safe_get_spatial_coords(adata)
        if coords is None:
            raise ValueError("No valid spatial coordinates found.")
    except ValueError as e:
        logging.error(f"Validation failed: {e}")
        valid = False

    # 2. Check a sample coordinate against image boundaries
    if valid:
        img_w, img_h = image_handler.get_dimensions()
        sample_coord = coords[0]
        if not (0 <= sample_coord[0] < img_w and 0 <= sample_coord[1] < img_h):
            logging.warning(
                f"Validation warning: First spot coordinate ({sample_coord}) is outside image "
                f"dimensions (Width={img_w}, Height={img_h}). This may be acceptable for some datasets."
            )
        logging.info(f"Image dimensions (W x H): {img_w} x {img_h}. AnnData spots: {adata.n_obs}.")

    # 3. Check HVG list coverage
    hvg_path = config['paths'].get('hvg_list_path')
    if hvg_path:
        try:
            hvg_list = set(np.loadtxt(hvg_path, dtype=str))
            adata_genes = set(adata.var_names)
            overlap = len(hvg_list.intersection(adata_genes))
            if overlap == 0:
                logging.error("Validation failed: No overlap between provided HVG list and genes in AnnData object.")
                valid = False
            else:
                coverage = (overlap / len(hvg_list)) * 100
                logging.info(f"HVG list coverage: {overlap}/{len(hvg_list)} genes from the list found in AnnData ({coverage:.2f}%).")
        except FileNotFoundError:
            logging.error(f"Validation failed: HVG list file not found at '{hvg_path}'.")
            valid = False


    # 4. Check model config if precomputing embeddings
    if config['preprocessing']['precompute_embeddings']:
        if 'model' not in config or config['model'] is None:
            logging.error("Validation failed: `model` configuration is required when `precompute_embeddings` is true.")
            valid = False
        else:
            if not os.path.exists(config['model']['model_path']):
                logging.error(f"Validation failed: Model checkpoint not found at '{config['model']['model_path']}'.")
                valid = False


    if not valid:
        raise RuntimeError("Pre-run validation failed. Please check the logs for errors and correct your configuration or data.")
    
    logging.info("‚úÖ Pre-run validation passed successfully.")

===== src/spaglam_preproc/utils/__init__.py =====

===== src/spaglam_preproc/utils/qc_tools.py =====
# spaglam_preproc/utils/qc_tools.py

import json
import logging
import math
import os
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import textwrap
import numpy as np

# For notebook display
try:
    from IPython.display import display
except ImportError:
    display = None

try:
    import matplotlib.pyplot as plt
except ImportError:
    plt = None

def generate_summary_report(metrics: dict, output_dir: str):
    """Saves a JSON summary of the preprocessing run."""
    report_path = Path(output_dir) / "qc_summary.json"
    try:
        # Convert NumPy types to native Python types for JSON serialization
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj

        with open(report_path, 'w') as f:
            json.dump(metrics, f, indent=2, default=convert_numpy)
        logging.info(f"üìä QC summary report saved to: {report_path}")
    except Exception as e:
        logging.error(f"Failed to save QC summary report: {e}")

def generate_visual_artifact(samples: list, output_dir: str, num_samples: int):
    """Creates and saves a grid image of sample tiles and their gene sentences."""
    if not samples:
        logging.warning("No samples were collected, skipping visual QC artifact generation.")
        return

    num_to_display = min(num_samples, len(samples))
    if num_to_display == 0:
        return
        
    grid_size = math.ceil(math.sqrt(num_to_display))
    
    first_tile = samples[0]['tile']
    tile_w, tile_h = first_tile.size
    
    cell_w, cell_h = tile_w + 20, tile_h + 90  # Extra space for padding and text
    grid_img = Image.new("RGB", (grid_size * cell_w, grid_size * cell_h), "#F0F0F0")
    
    try:
        # Try a common system font, fallback to default
        font = ImageFont.truetype("DejaVuSans.ttf", 10)
        font_bold = ImageFont.truetype("DejaVuSans-Bold.ttf", 11)
    except IOError:
        font = ImageFont.load_default()
        font_bold = font
    
    draw = ImageDraw.Draw(grid_img)

    for i, sample in enumerate(samples[:num_to_display]):
        row, col = divmod(i, grid_size)
        x_offset, y_offset = col * cell_w, row * cell_h

        # Paste tile with a small border
        grid_img.paste(sample['tile'], (x_offset + 10, y_offset + 10))
        
        # Draw spot ID
        draw.text(
            (x_offset + 10, y_offset + tile_h + 20),
            f"Spot ID: {sample['id']}", fill="black", font=font_bold)
        
        # Draw wrapped gene sentence
        wrapped_text = textwrap.fill(f"Top Genes: {sample['sentence']}", width=45)
        draw.multiline_text(
            (x_offset + 10, y_offset + tile_h + 35),
            wrapped_text, fill="#555555", font=font)

    artifact_path = Path(output_dir) / "qc_sample_grid.png"
    try:
        grid_img.save(artifact_path)
        logging.info(f"üñºÔ∏è QC visual artifact with {num_to_display} samples saved to: {artifact_path}")
    except Exception as e:
        logging.error(f"Failed to save QC visual artifact: {e}")

def display_visual_artifact_notebook(artifact_path: str):
    """Displays the visual artifact image directly in a Jupyter notebook."""
    if not (plt and display):
        logging.warning("Matplotlib or IPython not found. Cannot display image in this environment.")
        return
        
    try:
        img = Image.open(artifact_path)
        plt.figure(figsize=(12, 12))
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"Visual QC Samples from {Path(artifact_path).name}")
        plt.show()
    except FileNotFoundError:
        logging.error(f"Artifact file not found at {artifact_path}. Cannot display.")
    except Exception as e:
        logging.error(f"Error displaying visual artifact: {e}")

