{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e8d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ± Seed=42; è¾“å‡ºç›®å½•=/cwStorage/nodecw_group/jijh/hest_sentences_human_all_second\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 0) Imports & Config\n",
    "# =========================================================\n",
    "import os, sys, time, random, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import scanpy as sc\n",
    "from scipy.sparse import issparse, csr_matrix\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"/home1/jijh/diffusion_project/git_repo/GoldSpace\")\n",
    "\n",
    "from src.spaglam_preproc.utils.hest_loading import HESTDataset\n",
    "\n",
    "\n",
    "# ---------- å…¨å±€é…ç½® ----------\n",
    "DATA_DIR = \"/cwStorage/nodecw_group/jijh/hest_1k\"\n",
    "OUT_BASE = \"/cwStorage/nodecw_group/jijh/hest_sentences_human_all_second\"\n",
    "CACHE_DIR = os.path.join(OUT_BASE, \"cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "CACHE_H5AD = os.path.join(CACHE_DIR, \"adata_merged_preprocessed_canonical_collapsed.h5ad\")\n",
    "HVG_TXT = os.path.join(OUT_BASE, \"global_hvgs.txt\")\n",
    "\n",
    "SAMPLES_TO_EXCLUDE = {\n",
    "    'TENX15','MEND16','MEND15','MEND14','MEND13','MEND12',\n",
    "    'MEND11','MEND10','MEND9','MEND8','MEND7','MEND2',\n",
    "    'MEND1','NCBI657','NCBI814'\n",
    "}\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "N_TOP_HVGS = 5000\n",
    "N_TOP_GENES_IN_SENT = 50\n",
    "BATCH_KEY = 'sample_id'\n",
    "\n",
    "print(f\"ğŸŒ± Seed={SEED}; è¾“å‡ºç›®å½•={OUT_BASE}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7772c36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹åŒ– HESTDataset...\n",
      "æœ‰æ•ˆæ ·æœ¬æ•°: 634ï¼ˆå·²æ’é™¤: 15ï¼‰\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83eec80f4be419e85299deb513be018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "åŠ è½½æ ·æœ¬:   0%|          | 0/634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå¹¶å®Œæˆ: (1351225, 220010)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================================================\n",
    "# 1) Utilities\n",
    "# =========================================================\n",
    "def format_time(seconds: float) -> str:\n",
    "    m, s = divmod(seconds, 60)\n",
    "    return f\"{int(m)}åˆ† {s:.2f}ç§’\"\n",
    "\n",
    "def load_sample_data(sample, batch_key: str = 'sample_id'):\n",
    "    \"\"\"æŠŠå•ä¸ª HESTSample åŠ è½½åˆ°å†…å­˜, æ ‡æ³¨ obs_names ä¸ batch_key.\"\"\"\n",
    "    try:\n",
    "        sample.load_st_data(lazy=False)\n",
    "        if sample.adata is None:\n",
    "            return None\n",
    "        ad = sample.adata.to_memory()\n",
    "        # obs_names åŠ å‰ç¼€é¿å…ä¸åŒæ ·æœ¬é‡å¤\n",
    "        ad.obs_names = [f\"{sample.sample_id}_{x}\" for x in ad.obs_names]\n",
    "        ad.obs[batch_key] = sample.sample_id\n",
    "        return ad\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {sample.sample_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def try_load_canonical_gene_set(cache_dir: str):\n",
    "    \"\"\"å°è¯•ä» cache_dir å¯¼å…¥ human_gene_symbols.pyï¼›è‹¥å¤±è´¥è¿”å› None.\"\"\"\n",
    "    try:\n",
    "        sys.path.insert(0, cache_dir)\n",
    "        from human_gene_symbols import human_gene_symbols\n",
    "        return set([g.upper() for g in human_gene_symbols])\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥: human_gene_symbols.pyï¼›å°†ä»…åšèšåˆä¸åšæƒå¨è¿‡æ»¤ã€‚åŸå› : {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if sys.path[0] == cache_dir:\n",
    "            sys.path.pop(0)\n",
    "\n",
    "def canonicalize_var_names_to_upper(adata: sc.AnnData):\n",
    "    \"\"\"å»å‰ç¼€/ç‰ˆæœ¬å·ï¼Œå¹¶ç»Ÿä¸€å¤§å†™ï¼Œä¾¿äºä¸ HGNC å¯¹é½ã€‚\"\"\"\n",
    "    cleaned = []\n",
    "    for name in adata.var_names:\n",
    "        base = name.replace('GRCh38______','').split('.')[0]\n",
    "        cleaned.append(base.upper())\n",
    "    adata.var_names = cleaned\n",
    "    return adata\n",
    "\n",
    "def collapse_duplicate_genes(adata: sc.AnnData) -> sc.AnnData:\n",
    "    \"\"\"\n",
    "    èšåˆåŒååŸºå› ï¼ˆæŒ‰åˆ—æ±‚å’Œï¼‰ï¼Œé¿å… var_names_make_unique() å¸¦æ¥çš„ -1/-2 åç¼€ã€‚\n",
    "    ç¨€ç–/ç¨ å¯†å‡æ”¯æŒï¼›ä¿ç•™ obs/obsm/unsã€‚\n",
    "    \"\"\"\n",
    "    names = np.asarray(adata.var_names)\n",
    "    uniq, inv = np.unique(names, return_inverse=True)\n",
    "    if uniq.size == names.size:\n",
    "        return adata  # æ— é‡å¤ï¼Œæ— éœ€èšåˆ\n",
    "\n",
    "    # æ„é€ åˆ—æ˜ å°„ç¨€ç–çŸ©é˜µ G (n_vars x n_unique)\n",
    "    ones = np.ones(inv.size, dtype=np.float32)\n",
    "    G = csr_matrix((ones, (np.arange(inv.size), inv)), shape=(inv.size, uniq.size))\n",
    "\n",
    "    if issparse(adata.X):\n",
    "        X_new = adata.X @ G\n",
    "    else:\n",
    "        # dense @ sparse -> dense ndarray\n",
    "        X_new = np.asarray(adata.X) @ G\n",
    "\n",
    "    ad_new = sc.AnnData(X_new, obs=adata.obs.copy(), var=pd.DataFrame(index=uniq))\n",
    "    ad_new.obsm = adata.obsm.copy()\n",
    "    ad_new.uns = adata.uns.copy()\n",
    "    return ad_new\n",
    "\n",
    "def enforce_canonical_and_collapse(adata: sc.AnnData, canonical_genes_upper: set | None):\n",
    "    \"\"\"\n",
    "    1) è§„èŒƒåŸºå› åï¼ˆå»å‰ç¼€/ç‰ˆæœ¬å·å¹¶å¤§å†™ï¼‰\n",
    "    2) ï¼ˆå¯é€‰ï¼‰åŸºäºæƒå¨è¡¨è¿‡æ»¤\n",
    "    3) èšåˆåŒååŸºå› ï¼ˆåˆ—æ±‚å’Œï¼‰\n",
    "    \"\"\"\n",
    "    adata = canonicalize_var_names_to_upper(adata)\n",
    "\n",
    "    if canonical_genes_upper is not None:\n",
    "        keep = pd.Index(adata.var_names).isin(list(canonical_genes_upper))\n",
    "        before = adata.n_vars\n",
    "        adata = adata[:, keep].copy()\n",
    "        print(f\"âœ… æƒå¨è¿‡æ»¤: åŸºå›  {before} â†’ {adata.n_vars}\")\n",
    "\n",
    "    # èšåˆåŒåï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰\n",
    "    before = adata.n_vars\n",
    "    adata = collapse_duplicate_genes(adata)\n",
    "    print(f\"âœ… èšåˆåŒå: åŸºå›  {before} â†’ {adata.n_vars}ï¼ˆå»æ‰ -1/-2 åç¼€çš„æ ¹æºï¼‰\")\n",
    "    # æœ€ç»ˆä¿è¯æ— é‡å¤ä¸”ä¸åšâ€œå”¯ä¸€åŒ–â€åŠ åç¼€\n",
    "    assert not pd.Index(adata.var_names).duplicated().any(), \"èšåˆåä»æœ‰é‡å¤åŸºå› å\"\n",
    "\n",
    "    return adata\n",
    "\n",
    "def qc_and_basic_filters(adata: sc.AnnData) -> sc.AnnData:\n",
    "    \"\"\"ç»†èƒ/spot è¿‡æ»¤ + åŸºå› è¿‡æ»¤ + çº¿ç²’ä½“å æ¯”è¿‡æ»¤.\"\"\"\n",
    "    ad = adata.copy()\n",
    "    ad.var['mt'] = ad.var_names.str.startswith('MT-')\n",
    "    sc.pp.calculate_qc_metrics(ad, qc_vars=['mt'], inplace=True, percent_top=None, log1p=False)\n",
    "\n",
    "    n0 = ad.n_obs; p0 = ad.n_vars\n",
    "    sc.pp.filter_cells(ad, min_genes=200)\n",
    "    ad = ad[ad.obs.pct_counts_mt < 20, :].copy()\n",
    "\n",
    "    min_cells = max(1, int(ad.n_obs * 0.001))  # è‡³å°‘ 0.1% çš„ spotsï¼›ä¸” â‰¥1\n",
    "    sc.pp.filter_genes(ad, min_cells=min_cells)\n",
    "\n",
    "    print(f\"âœ… QCå: spots {n0}â†’{ad.n_obs}, genes {p0}â†’{ad.n_vars} (min_cells={min_cells})\")\n",
    "    if 'in_tissue' in ad.obs:\n",
    "        ad.obs['in_tissue'] = ad.obs['in_tissue'].fillna(0).astype(bool)\n",
    "    return ad\n",
    "\n",
    "def normalize_log1p(adata: sc.AnnData) -> sc.AnnData:\n",
    "    ad = adata.copy()\n",
    "    sc.pp.normalize_total(ad, target_sum=1e4)\n",
    "    sc.pp.log1p(ad)\n",
    "    return ad\n",
    "\n",
    "def compute_hvgs(adata: sc.AnnData, n_top: int = 5000, batch_key: str = 'sample_id',\n",
    "                 flavor: str = 'seurat_v3_paper') -> pd.Index:\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=n_top, batch_key=batch_key, flavor=flavor)\n",
    "    hvgs = adata.var.index[adata.var['highly_variable']]\n",
    "    return hvgs\n",
    "\n",
    "def validate_hvgs(hvgs: pd.Index, canonical_genes_upper: set | None, n_expected: int):\n",
    "    # 1) æ— é‡å¤\n",
    "    assert not pd.Index(hvgs).duplicated().any(), \"HVG åˆ—è¡¨å­˜åœ¨é‡å¤åŸºå› å\"\n",
    "    # 2) æ—  -æ•°å­— åç¼€\n",
    "    bad = [g for g in hvgs if re.search(r\"-\\d+$\", g)]\n",
    "    assert len(bad) == 0, f\"HVG ä¸­ä»å«éæ ‡å‡†åï¼ˆå¦‚ -1 åç¼€ï¼‰: {bad[:5]} ...\"\n",
    "    # 3) ï¼ˆå¯é€‰ï¼‰å…¨éƒ¨åœ¨æƒå¨é›†åˆ\n",
    "    if canonical_genes_upper is not None:\n",
    "        outside = [g for g in hvgs if g.upper() not in canonical_genes_upper]\n",
    "        assert len(outside) == 0, f\"HVG ä¸­å­˜åœ¨ä¸åœ¨æƒå¨é›†åˆçš„åŸºå› ï¼ˆå‰è‹¥å¹²ï¼‰ï¼š{outside[:5]} ...\"\n",
    "    # 4) æ•°é‡\n",
    "    if len(hvgs) != n_expected:\n",
    "        print(f\"âš ï¸ HVG æ•°é‡ {len(hvgs)} != æœŸæœ› {n_expected}ã€‚è‹¥å¼ºè¿‡æ»¤ååŸºå› ä¸è¶³, è¿™æ˜¯å¯è§£é‡Šçš„ã€‚\")\n",
    "\n",
    "def save_hvgs(hvgs: pd.Index, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        for g in hvgs:\n",
    "            f.write(f\"{g}\\n\")\n",
    "    print(f\"ğŸ’¾ HVG åˆ—è¡¨å·²ä¿å­˜: {path}\")\n",
    "\n",
    "# --------- å¥å­ç”Ÿæˆç›¸å…³ ----------\n",
    "def topk_indices(x: np.ndarray, k: int):\n",
    "    \"\"\"ç”¨ argpartition å¿«é€Ÿå– top-k ç´¢å¼•ï¼ˆé™åºï¼‰ã€‚\"\"\"\n",
    "    k = min(k, x.size)\n",
    "    if k <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    idx = np.argpartition(x, -k)[-k:]\n",
    "    return idx[np.argsort(x[idx])[::-1]]\n",
    "\n",
    "def make_sentences_for_sample(sample_id: str, adata_sample: sc.AnnData,\n",
    "                              hvgs: pd.Index, out_base: str,\n",
    "                              k: int = 50):\n",
    "    out_dir = os.path.join(out_base, f\"{sample_id}_sentences_hvg\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    ad = adata_sample[:, adata_sample.var_names.isin(hvgs)].copy()\n",
    "    if ad.n_vars == 0:\n",
    "        print(f\"âš ï¸ {sample_id}: æ—  HVG äº¤é›†, è·³è¿‡\")\n",
    "        return\n",
    "\n",
    "    obs = ad.obs.copy()\n",
    "    genes = ad.var_names.to_numpy()\n",
    "\n",
    "    # åæ ‡é²æ£’è·å–\n",
    "    if 'pxl_col_in_fullres' not in obs.columns or 'pxl_row_in_fullres' not in obs.columns:\n",
    "        if 'spatial' in ad.obsm:\n",
    "            obs['pxl_col_in_fullres'] = ad.obsm['spatial'][:, 0]\n",
    "            obs['pxl_row_in_fullres'] = ad.obsm['spatial'][:, 1]\n",
    "        else:\n",
    "            obs['pxl_col_in_fullres'] = np.arange(ad.n_obs)\n",
    "            obs['pxl_row_in_fullres'] = 0\n",
    "\n",
    "    for i in range(ad.n_obs):\n",
    "        r, c = obs.iloc[i]['pxl_row_in_fullres'], obs.iloc[i]['pxl_col_in_fullres']\n",
    "        if pd.isna(r) or pd.isna(c):\n",
    "            suffix = f\"{i}_0_no_coord\"\n",
    "        else:\n",
    "            suffix = f\"{int(round(r))}_{int(round(c))}\"\n",
    "        path = os.path.join(out_dir, f\"{sample_id}_{suffix}.txt\")\n",
    "        if os.path.exists(path):\n",
    "            continue\n",
    "\n",
    "        # å–è¡¨è¾¾å‘é‡\n",
    "        if issparse(ad.X):\n",
    "            vec = ad.X.getrow(i).toarray().ravel()\n",
    "        else:\n",
    "            vec = np.asarray(ad.X[i]).ravel()\n",
    "\n",
    "        idx = topk_indices(vec, k)\n",
    "        toks = genes[idx]\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\" \".join(toks))\n",
    "\n",
    "def validate_sentences_random(hvgs: set, out_base: str, sample_ids: list, n_checks: int = 20):\n",
    "    \"\"\"éšæœºæŠ½æ ·å¥å­æ–‡ä»¶ï¼ŒéªŒè¯æ‰€æœ‰ token å‡âˆˆ HVG.\"\"\"\n",
    "    import random, os\n",
    "    sample_ids = sample_ids[:] if len(sample_ids) <= n_checks else random.sample(sample_ids, n_checks)\n",
    "    ok = 0; bad = []\n",
    "    for sid in sample_ids:\n",
    "        d = os.path.join(out_base, f\"{sid}_sentences_hvg\")\n",
    "        if not (os.path.exists(d) and os.listdir(d)):\n",
    "            continue\n",
    "        fname = random.choice(os.listdir(d))\n",
    "        with open(os.path.join(d, fname), 'r') as f:\n",
    "            toks = f.read().split()\n",
    "        if all(t in hvgs for t in toks):\n",
    "            ok += 1\n",
    "        else:\n",
    "            bad.append((sid, fname, [t for t in toks if t not in hvgs]))\n",
    "    print(f\"ğŸ” å¥å­éªŒè¯ï¼šé€šè¿‡ {ok}/{len(sample_ids)} ä¸ªæ ·æœ¬ã€‚\")\n",
    "    if bad:\n",
    "        print(\"âŒ éHVG token ç¤ºä¾‹ï¼ˆæœ€å¤šæ˜¾ç¤º5ä¾‹ï¼‰ï¼š\")\n",
    "        for sid, fn, notin in bad[:5]:\n",
    "            print(f\"  - {sid}/{fn}: {notin[:10]} ...\")\n",
    "\n",
    "# =========================================================\n",
    "# 2) Main: åŠ è½½â†’åˆå¹¶â†’æ¸…æ´—â†’èšåˆâ†’QCâ†’HVGâ†’å¥å­\n",
    "# =========================================================\n",
    "t0 = time.time()\n",
    "\n",
    "# 2.1 è¯»å–æ ·æœ¬\n",
    "print(\"åˆå§‹åŒ– HESTDataset...\")\n",
    "dataset = HESTDataset(data_dir=DATA_DIR)\n",
    "samples = [s for s in dataset.get_samples(species=\"Homo sapiens\")\n",
    "           if s.sample_id not in SAMPLES_TO_EXCLUDE]\n",
    "print(f\"æœ‰æ•ˆæ ·æœ¬æ•°: {len(samples)}ï¼ˆå·²æ’é™¤: {len(SAMPLES_TO_EXCLUDE)}ï¼‰\")\n",
    "\n",
    "# 2.2 åˆå¹¶ï¼ˆå¸¦ batch_keyï¼‰\n",
    "adatas = []\n",
    "for s in tqdm(samples, desc=\"åŠ è½½æ ·æœ¬\"):\n",
    "    ad = load_sample_data(s, batch_key=BATCH_KEY)\n",
    "    if ad is None: \n",
    "        continue\n",
    "    # æ ·æœ¬å†…å¦‚æœ‰é‡å¤åˆ—ï¼Œå…ˆåšä¸€æ¬¡ç®€å•å»é‡ï¼ˆä¿å®ˆåšæ³•ï¼Œæœ€ç»ˆè¿˜ä¼šç»Ÿä¸€èšåˆï¼‰\n",
    "    if pd.Index(ad.var_names).duplicated().any():\n",
    "        ad = ad[:, ~pd.Index(ad.var_names).duplicated()].copy()\n",
    "    adatas.append(ad)\n",
    "if not adatas:\n",
    "    raise ValueError(\"æœªèƒ½åŠ è½½ä»»ä½• AnnData\")\n",
    "\n",
    "adata = sc.concat(adatas, join='outer', uns_merge='unique')\n",
    "print(f\"åˆå¹¶å®Œæˆ: {adata.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48e526",
   "metadata": {},
   "source": [
    "# åˆ©ç”¨hugo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86683e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed=42; è¾“å‡ºç›®å½•=/cwStorage/nodecw_group/jijh/hest_sentences_hugo\n"
     ]
    }
   ],
   "source": [
    "# 0) Imports & Config\n",
    "# =========================================================\n",
    "import os, sys, time, random, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import scanpy as sc\n",
    "from scipy.sparse import issparse, csr_matrix\n",
    "\n",
    "\n",
    "# ---------- å…¨å±€é…ç½® ----------\n",
    "DATA_DIR = \"/cwStorage/nodecw_group/jijh/hest_1k\"\n",
    "OUT_BASE = \"/cwStorage/nodecw_group/jijh/hest_sentences_hugo\"\n",
    "CACHE_DIR = os.path.join(OUT_BASE, \"cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "CACHE_H5AD = os.path.join(CACHE_DIR, \"adata_merged_preprocessed_canonical_collapsed.h5ad\")\n",
    "HVG_TXT = os.path.join(OUT_BASE, \"global_hvgs.txt\")\n",
    "\n",
    "# â˜…â˜… NEW: HGNC å®˜æ–¹å…¨è¡¨è·¯å¾„ï¼ˆä½ åˆšç»™çš„ realpathï¼‰\n",
    "HGNC_TSV = \"/public/home/jijh/diffusion_project/git_repo/GoldSpace/hgnc_complete_set.txt\"\n",
    "\n",
    "# ï¼ˆå¯é€‰ï¼‰è¿‡æ»¤æ¡ä»¶\n",
    "KEEP_STATUS = {\"Approved\"}  # åªä¿ç•™â€œå·²æ‰¹å‡†â€å‘½å\n",
    "# KEEP_LOCUS_TYPES = {\"protein-coding gene\"}  # è‹¥åªä¿ç•™è›‹ç™½ç¼–ç åŸºå› ï¼Œè§£å¼€æ­¤è¡Œ\n",
    "KEEP_LOCUS_TYPES = None  # ä¸é™å®šç±»å‹æ—¶ç”¨ None\n",
    "\n",
    "SAMPLES_TO_EXCLUDE = {\n",
    "    'TENX15','MEND16','MEND15','MEND14','MEND13','MEND12',\n",
    "    'MEND11','MEND10','MEND9','MEND8','MEND7','MEND2',\n",
    "    'MEND1','NCBI657','NCBI814'\n",
    "}\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "N_TOP_HVGS = 5000\n",
    "N_TOP_GENES_IN_SENT = 50\n",
    "BATCH_KEY = 'sample_id'\n",
    "\n",
    "print(f\"Seed={SEED}; è¾“å‡ºç›®å½•={OUT_BASE}\")\n",
    "\n",
    "# =========================================================\n",
    "# 1) Utilities\n",
    "# =========================================================\n",
    "def format_time(seconds: float) -> str:\n",
    "    m, s = divmod(seconds, 60)\n",
    "    return f\"{int(m)}åˆ† {s:.2f}ç§’\"\n",
    "\n",
    "def load_sample_data(sample, batch_key: str = 'sample_id'):\n",
    "    \"\"\"æŠŠå•ä¸ª HESTSample åŠ è½½åˆ°å†…å­˜, æ ‡æ³¨ obs_names ä¸ batch_key.\"\"\"\n",
    "    try:\n",
    "        sample.load_st_data(lazy=False)\n",
    "        if sample.adata is None:\n",
    "            return None\n",
    "        ad = sample.adata.to_memory()\n",
    "        ad.obs_names = [f\"{sample.sample_id}_{x}\" for x in ad.obs_names]  # é¿å… obs_names å†²çª\n",
    "        ad.obs[batch_key] = sample.sample_id\n",
    "        return ad\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {sample.sample_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- NEW: è½½å…¥ HGNC å®˜æ–¹å‘½åèµ„æº ----------\n",
    "def load_hgnc_resources(tsv_path: str,\n",
    "                        keep_status: set[str] | None = None,\n",
    "                        keep_locus_types: set[str] | None = None):\n",
    "    \"\"\"\n",
    "    ä» HGNC å®Œæ•´è¡¨æ„å»ºï¼š\n",
    "    - canonical_setï¼šå½“å‰å®˜æ–¹ç¬¦å·é›†åˆï¼ˆå¤§å†™ï¼‰\n",
    "    - synonym_mapï¼šåˆ«å/æ—§åï¼ˆå¤§å†™ï¼‰åˆ°å®˜æ–¹ç¬¦å·ï¼ˆå¤§å†™ï¼‰çš„æ˜ å°„\n",
    "    - symbol2typeï¼šå®˜æ–¹ç¬¦å· -> locus_type\n",
    "    \"\"\"\n",
    "    # HGNC æ–‡ä»¶æ˜¯ TSVï¼ˆåˆ¶è¡¨ç¬¦åˆ†éš”ï¼‰\n",
    "    df = pd.read_csv(tsv_path, sep='\\t', dtype=str, na_filter=False)\n",
    "    # æ ‡å‡†åˆ—åï¼ˆä¸åŒç‰ˆæœ¬è¡¨å¤´ä¸€è‡´ï¼šsymbol, alias_symbol, prev_symbol, status, locus_typeï¼‰\n",
    "    # ç»Ÿä¸€å¤§å†™ç¬¦å·\n",
    "    df['symbol_u'] = df['symbol'].str.upper()\n",
    "\n",
    "    # è¿‡æ»¤ status\n",
    "    if keep_status:\n",
    "        df = df[df['status'].isin(keep_status)].copy()\n",
    "\n",
    "    # è¿‡æ»¤ locus_typeï¼ˆå¯é€‰ï¼‰\n",
    "    if keep_locus_types:\n",
    "        df = df[df['locus_type'].isin(keep_locus_types)].copy()\n",
    "\n",
    "    # å®˜æ–¹é›†åˆä¸ç±»å‹æ˜ å°„\n",
    "    canonical_set = set(df['symbol_u'])\n",
    "    symbol2type = dict(zip(df['symbol_u'], df['locus_type']))\n",
    "\n",
    "    # æ„å»ºåŒä¹‰å/æ—§åæ˜ å°„\n",
    "    def _split_multi(s: str):\n",
    "        if not s:\n",
    "            return []\n",
    "        s = s.strip().strip('\"')\n",
    "        parts = re.split(r'[|,;/]\\s*', s)\n",
    "        return [p for p in parts if p]\n",
    "\n",
    "    synonym_map = {}\n",
    "    for _, r in df.iterrows():\n",
    "        sym = r['symbol_u']\n",
    "        # ä»…ç”¨ alias_symbol / prev_symbolï¼Œé¿å…ç”¨ name/alias_name é€ æˆè¯¯æ˜ å°„\n",
    "        for col in ('alias_symbol', 'prev_symbol'):\n",
    "            if col in r and r[col]:\n",
    "                for a in _split_multi(r[col]):\n",
    "                    synonym_map[a.upper()] = sym\n",
    "\n",
    "    print(f\"HGNC è½½å…¥ï¼šå®˜æ–¹ç¬¦å· {len(canonical_set)} ä¸ªï¼›åŒä¹‰/æ—§åæ˜ å°„ {len(synonym_map)} æ¡ã€‚\")\n",
    "    return canonical_set, synonym_map, symbol2type\n",
    "\n",
    "# ---------- åç§°è§„èŒƒåŒ– + åŒä¹‰åæ˜ å°„ + åˆ—èšåˆ ----------\n",
    "def canonicalize_var_names_to_upper(adata: sc.AnnData):\n",
    "    \"\"\"å»å‰ç¼€/ç‰ˆæœ¬å·ï¼Œå¹¶ç»Ÿä¸€å¤§å†™ï¼Œä¾¿äºä¸ HGNC å¯¹é½ã€‚\"\"\"\n",
    "    cleaned = []\n",
    "    for name in adata.var_names:\n",
    "        base = name.replace('GRCh38______','').split('.')[0]\n",
    "        cleaned.append(base.upper())\n",
    "    adata.var_names = cleaned\n",
    "    return adata\n",
    "\n",
    "def map_synonyms_to_official(adata: sc.AnnData, synonym_map: dict[str, str]) -> tuple[sc.AnnData, int]:\n",
    "    \"\"\"æŠŠ var_names ä¸­çš„åŒä¹‰/æ—§åæ˜ å°„ä¸ºå®˜æ–¹ç¬¦å·ï¼›è¿”å›æ˜ å°„æ•°é‡ã€‚\"\"\"\n",
    "    if not synonym_map:\n",
    "        return adata, 0\n",
    "    names = list(adata.var_names)\n",
    "    n_map = 0\n",
    "    for i, g in enumerate(names):\n",
    "        if g in synonym_map:\n",
    "            names[i] = synonym_map[g]\n",
    "            n_map += 1\n",
    "    adata.var_names = names\n",
    "    return adata, n_map\n",
    "\n",
    "def collapse_duplicate_genes(adata: sc.AnnData) -> sc.AnnData:\n",
    "    \"\"\"\n",
    "    èšåˆåŒååŸºå› ï¼ˆæŒ‰åˆ—æ±‚å’Œï¼‰ï¼Œé¿å… var_names_make_unique() å¸¦æ¥çš„ -1/-2 åç¼€ã€‚\n",
    "    ç¨€ç–/ç¨ å¯†å‡æ”¯æŒï¼›ä¿ç•™ obs/obsm/unsã€‚\n",
    "    \"\"\"\n",
    "    names = np.asarray(adata.var_names)\n",
    "    uniq, inv = np.unique(names, return_inverse=True)\n",
    "    if uniq.size == names.size:\n",
    "        return adata  # æ— é‡å¤\n",
    "\n",
    "    ones = np.ones(inv.size, dtype=np.float32)\n",
    "    G = csr_matrix((ones, (np.arange(inv.size), inv)), shape=(inv.size, uniq.size))\n",
    "\n",
    "    if issparse(adata.X):\n",
    "        X_new = adata.X @ G\n",
    "    else:\n",
    "        X_new = np.asarray(adata.X) @ G\n",
    "\n",
    "    ad_new = sc.AnnData(X_new, obs=adata.obs.copy(), var=pd.DataFrame(index=uniq))\n",
    "    ad_new.obsm = adata.obsm.copy()\n",
    "    ad_new.uns = adata.uns.copy()\n",
    "    return ad_new\n",
    "\n",
    "def enforce_hgnc_and_collapse(adata: sc.AnnData,\n",
    "                              canonical_set: set[str] | None,\n",
    "                              synonym_map: dict[str, str] | None,\n",
    "                              audit_dir: str):\n",
    "    \"\"\"\n",
    "    1) è§„èŒƒåŒ–ï¼ˆå»å‰ç¼€/ç‰ˆæœ¬å· + å¤§å†™ï¼‰\n",
    "    2) åŒä¹‰å/æ—§å â†’ å®˜æ–¹ç¬¦å·\n",
    "    3) ï¼ˆå¯é€‰ï¼‰åªä¿ç•™å®˜æ–¹é›†åˆ\n",
    "    4) èšåˆåŒåï¼ˆåˆ—æ±‚å’Œï¼‰\n",
    "    å¹¶è¾“å‡ºå®¡è®¡æŠ¥å‘Šï¼šæ˜ å°„äº†å¤šå°‘ã€è¢«è¿‡æ»¤å¤šå°‘ã€æœªå‘½ä¸­å¤šå°‘ã€‚\n",
    "    \"\"\"\n",
    "    os.makedirs(audit_dir, exist_ok=True)\n",
    "\n",
    "    # 0) è®°å½•åŸå§‹å\n",
    "    before_names = pd.Index(adata.var_names)\n",
    "\n",
    "    # 1) ç»Ÿä¸€ + å»ç‰ˆæœ¬å·\n",
    "    ad = canonicalize_var_names_to_upper(adata.copy())\n",
    "\n",
    "    # 2) åŒä¹‰åæ˜ å°„\n",
    "    mapped_n = 0\n",
    "    if synonym_map:\n",
    "        ad, mapped_n = map_synonyms_to_official(ad, synonym_map)\n",
    "\n",
    "    # 3) å®˜æ–¹é›†åˆè¿‡æ»¤ï¼ˆå¯é€‰ï¼‰\n",
    "    if canonical_set:\n",
    "        keep_mask = pd.Index(ad.var_names).isin(list(canonical_set))\n",
    "        kept = int(keep_mask.sum()); dropped = int((~keep_mask).sum())\n",
    "        ad = ad[:, keep_mask].copy()\n",
    "    else:\n",
    "        kept = ad.n_vars; dropped = 0\n",
    "\n",
    "    # 4) èšåˆåŒåï¼ˆæ ¸å¿ƒä¿®å¤ï¼Œæœç» -1/-2ï¼‰\n",
    "    ad = collapse_duplicate_genes(ad)\n",
    "\n",
    "    # ---- å®¡è®¡è¾“å‡º ----\n",
    "    after_names = pd.Index(ad.var_names)\n",
    "    df_audit = pd.DataFrame({\n",
    "        \"stage\": [\"init\", \"after_canon+map+filter+collapse\"],\n",
    "        \"n_genes\": [len(before_names), len(after_names)],\n",
    "        \"n_mapped_from_alias_prev\": [mapped_n, mapped_n],\n",
    "        \"n_filtered_by_canonical\": [dropped, dropped]\n",
    "    })\n",
    "    df_audit.to_csv(os.path.join(audit_dir, \"gene_name_audit.tsv\"), sep='\\t', index=False)\n",
    "\n",
    "    # åˆ—å‡ºæœªå‘½ä¸­å®˜æ–¹é›†åˆ/æœªè¢«æ˜ å°„è€…ï¼ˆä»…é‡‡æ ·å±•ç¤ºï¼‰\n",
    "    if canonical_set:\n",
    "        unresolved = sorted(set(g for g in before_names.str.upper()\n",
    "                                if (g not in canonical_set) and (g not in synonym_map)))\n",
    "        sample_unresolved = unresolved[:20]\n",
    "        with open(os.path.join(audit_dir, \"unresolved_non_hgnc_samples.txt\"), \"w\") as f:\n",
    "            for g in sample_unresolved:\n",
    "                f.write(g + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… åç§°å¯¹é½å®Œæˆï¼šæ˜ å°„ {mapped_n}ï¼›è¿‡æ»¤ {dropped}ï¼›æœ€ç»ˆåŸºå›  {ad.n_vars}\")\n",
    "    assert not pd.Index(ad.var_names).duplicated().any(), \"èšåˆåä»æœ‰é‡å¤åŸºå› å\"\n",
    "    return ad\n",
    "\n",
    "def qc_and_basic_filters(adata: sc.AnnData) -> sc.AnnData:\n",
    "    \"\"\"ç»†èƒ/spot è¿‡æ»¤ + åŸºå› è¿‡æ»¤ + çº¿ç²’ä½“å æ¯”è¿‡æ»¤.\"\"\"\n",
    "    ad = adata.copy()\n",
    "    ad.var['mt'] = ad.var_names.str.startswith('MT-')\n",
    "    sc.pp.calculate_qc_metrics(ad, qc_vars=['mt'], inplace=True, percent_top=None, log1p=False)\n",
    "\n",
    "    n0 = ad.n_obs; p0 = ad.n_vars\n",
    "    sc.pp.filter_cells(ad, min_genes=200)\n",
    "    ad = ad[ad.obs.pct_counts_mt < 20, :].copy()\n",
    "\n",
    "    min_cells = max(1, int(ad.n_obs * 0.001))  # â‰¥1\n",
    "    sc.pp.filter_genes(ad, min_cells=min_cells)\n",
    "\n",
    "    print(f\"âœ… QCå: spots {n0}â†’{ad.n_obs}, genes {p0}â†’{ad.n_vars} (min_cells={min_cells})\")\n",
    "    if 'in_tissue' in ad.obs:\n",
    "        ad.obs['in_tissue'] = ad.obs['in_tissue'].fillna(0).astype(bool)\n",
    "    return ad\n",
    "\n",
    "def normalize_log1p(adata: sc.AnnData) -> sc.AnnData:\n",
    "    ad = adata.copy()\n",
    "    sc.pp.normalize_total(ad, target_sum=1e4)\n",
    "    sc.pp.log1p(ad)\n",
    "    return ad\n",
    "\n",
    "def compute_hvgs(adata: sc.AnnData, n_top: int = 5000, batch_key: str = 'sample_id',\n",
    "                 flavor: str = 'seurat_v3_paper') -> pd.Index:\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=n_top, batch_key=batch_key, flavor=flavor)\n",
    "    hvgs = adata.var.index[adata.var['highly_variable']]\n",
    "    return hvgs\n",
    "\n",
    "# ---------- HVG éªŒè¯ï¼šå¯¹ç…§ HGNC + ç±»å‹æ¦‚è§ˆ ----------\n",
    "def validate_hvgs(hvgs: pd.Index,\n",
    "                  canonical_set: set[str] | None,\n",
    "                  symbol2type: dict[str, str] | None,\n",
    "                  n_expected: int):\n",
    "    # 1) æ— é‡å¤\n",
    "    assert not pd.Index(hvgs).duplicated().any(), \"HVG åˆ—è¡¨å­˜åœ¨é‡å¤åŸºå› å\"\n",
    "\n",
    "    # 2) ä¸å®˜æ–¹é›†åˆå¯¹é½ï¼ˆä¸»åˆ¤æ®ï¼‰\n",
    "    outside = []\n",
    "    if canonical_set:\n",
    "        # æ³¨æ„ï¼šæˆ‘ä»¬çš„ç®¡çº¿å·²ç»Ÿä¸€å¤§å†™ï¼›ä¸ºç¨³å¦¥ä»ä½¿ç”¨ upper() å¯¹é½\n",
    "        outside = [g for g in hvgs if g.upper() not in canonical_set]\n",
    "        if outside:\n",
    "            print(f\"âš ï¸ æœ‰ {len(outside)} ä¸ª HVG ä¸åœ¨ HGNC å®˜æ–¹é›†åˆï¼ˆç¤ºä¾‹ï¼‰ï¼š{outside[:10]}\")\n",
    "\n",
    "    # 3) ä»…å¯¹â€œä¸åœ¨å®˜æ–¹é›†åˆâ€çš„åŸºå› å†åšåç¼€å¯ç–‘æ€§æ£€æµ‹\n",
    "    suspicious_suffix = [g for g in outside if re.search(r\"-\\d+$\", g)]\n",
    "    # â€”â€” å¦‚æœä½ å¸Œæœ›å¯¹è¿™ç±»â€œå¼ºç–‘ä¼¼ make_unique ä¼ªåç¼€â€ä¸¥è‹›å¤„ç†ï¼Œç”¨ assertï¼›å¦åˆ™æ”¹æˆ warningï¼š\n",
    "    assert len(suspicious_suffix) == 0, (\n",
    "        \"HVG ä¸­å­˜åœ¨ç–‘ä¼¼å”¯ä¸€åŒ–ä¼ªåç¼€ï¼ˆä¸åœ¨HGNCä¸”å½¢å¦‚ `-æ•°å­—`ï¼‰çš„åŸºå› ï¼š\"\n",
    "        f\"{suspicious_suffix[:10]}\"\n",
    "    )\n",
    "\n",
    "    # 4) å¯é€‰ï¼šlocus_type åˆ†å¸ƒï¼Œä¾¿äºå®¡è®¡ï¼ˆä¿¡æ¯æ€§è¾“å‡ºï¼Œä¸åšå¼ºæ ¡éªŒï¼‰\n",
    "    if symbol2type:\n",
    "        types = pd.Series([symbol2type.get(g.upper(), \"UNK\") for g in hvgs]).value_counts()\n",
    "        types.to_csv(os.path.join(CACHE_DIR, \"hvg_locus_type_distribution.tsv\"),\n",
    "                     sep='\\t', header=['count'])\n",
    "        print(\"HVG locus_type åˆ†å¸ƒå·²å†™å…¥ cache/hvg_locus_type_distribution.tsv\")\n",
    "\n",
    "    # 5) æ•°é‡æé†’ï¼ˆä¸å¼ºåˆ¶ï¼‰\n",
    "    if len(hvgs) != n_expected:\n",
    "        print(f\"âš ï¸ HVG æ•°é‡ {len(hvgs)} != æœŸæœ› {n_expected}ã€‚è‹¥å‰é¢åšäº†ä¸¥æ ¼è¿‡æ»¤ï¼Œè¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ã€‚\")\n",
    "\n",
    "\n",
    "def save_hvgs(hvgs: pd.Index, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        for g in hvgs:\n",
    "            f.write(f\"{g}\\n\")\n",
    "    print(f\"HVG åˆ—è¡¨å·²ä¿å­˜: {path}\")\n",
    "\n",
    "# --------- å¥å­ç”Ÿæˆç›¸å…³ ----------\n",
    "def topk_indices(x: np.ndarray, k: int):\n",
    "    \"\"\"argpartition å– top-kï¼ˆé™åºï¼‰ï¼Œæ¯”å…¨æ’åºæ›´å¿«ã€‚\"\"\"\n",
    "    k = min(k, x.size)\n",
    "    if k <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    idx = np.argpartition(x, -k)[-k:]\n",
    "    return idx[np.argsort(x[idx])[::-1]]\n",
    "\n",
    "def make_sentences_for_sample(sample_id: str, adata_sample: sc.AnnData,\n",
    "                              hvgs: pd.Index, out_base: str,\n",
    "                              k: int = 50):\n",
    "    out_dir = os.path.join(out_base, f\"{sample_id}_sentences_hvg\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    ad = adata_sample[:, adata_sample.var_names.isin(hvgs)].copy()\n",
    "    if ad.n_vars == 0:\n",
    "        print(f\"âš ï¸ {sample_id}: æ—  HVG äº¤é›†, è·³è¿‡\")\n",
    "        return\n",
    "\n",
    "    obs = ad.obs.copy()\n",
    "    genes = ad.var_names.to_numpy()\n",
    "\n",
    "    # åæ ‡é²æ£’è·å–\n",
    "    if 'pxl_col_in_fullres' not in obs.columns or 'pxl_row_in_fullres' not in obs.columns:\n",
    "        if 'spatial' in ad.obsm:\n",
    "            obs['pxl_col_in_fullres'] = ad.obsm['spatial'][:, 0]\n",
    "            obs['pxl_row_in_fullres'] = ad.obsm['spatial'][:, 1]\n",
    "        else:\n",
    "            obs['pxl_col_in_fullres'] = np.arange(ad.n_obs)\n",
    "            obs['pxl_row_in_fullres'] = 0\n",
    "\n",
    "    for i in range(ad.n_obs):\n",
    "        r, c = obs.iloc[i]['pxl_row_in_fullres'], obs.iloc[i]['pxl_col_in_fullres']\n",
    "        suffix = f\"{i}_0_no_coord\" if (pd.isna(r) or pd.isna(c)) else f\"{int(round(r))}_{int(round(c))}\"\n",
    "        path = os.path.join(out_dir, f\"{sample_id}_{suffix}.txt\")\n",
    "        if os.path.exists(path):\n",
    "            continue\n",
    "\n",
    "        if issparse(ad.X):\n",
    "            vec = ad.X.getrow(i).toarray().ravel()\n",
    "        else:\n",
    "            vec = np.asarray(ad.X[i]).ravel()\n",
    "\n",
    "        idx = topk_indices(vec, k)\n",
    "        toks = genes[idx]\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\" \".join(toks))\n",
    "\n",
    "def validate_sentences_random(hvgs: set, out_base: str, sample_ids: list, n_checks: int = 20):\n",
    "    \"\"\"éšæœºæŠ½æ ·å¥å­æ–‡ä»¶ï¼ŒéªŒè¯æ‰€æœ‰ token å‡âˆˆ HVGã€‚\"\"\"\n",
    "    import random, os\n",
    "    sample_ids = sample_ids[:] if len(sample_ids) <= n_checks else random.sample(sample_ids, n_checks)\n",
    "    ok = 0; bad = []\n",
    "    for sid in sample_ids:\n",
    "        d = os.path.join(out_base, f\"{sid}_sentences_hvg\")\n",
    "        if not (os.path.exists(d) and os.listdir(d)):\n",
    "            continue\n",
    "        fname = random.choice(os.listdir(d))\n",
    "        with open(os.path.join(d, fname), 'r') as f:\n",
    "            toks = f.read().split()\n",
    "        if all(t in hvgs for t in toks):\n",
    "            ok += 1\n",
    "        else:\n",
    "            bad.append((sid, fname, [t for t in toks if t not in hvgs]))\n",
    "    print(f\"å¥å­éªŒè¯ï¼šé€šè¿‡ {ok}/{len(sample_ids)} ä¸ªæ ·æœ¬ã€‚\")\n",
    "    if bad:\n",
    "        print(\"éHVG token ç¤ºä¾‹ï¼ˆæœ€å¤š5ä¾‹ï¼‰ï¼š\")\n",
    "        for sid, fn, notin in bad[:5]:\n",
    "            print(f\"  - {sid}/{fn}: {notin[:10]} ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9d512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGNC è½½å…¥ï¼šå®˜æ–¹ç¬¦å· 44459 ä¸ªï¼›åŒä¹‰/æ—§åæ˜ å°„ 58030 æ¡ã€‚\n"
     ]
    }
   ],
   "source": [
    "# 2.3 NEW: è½½å…¥ HGNC å®˜æ–¹èµ„æº\n",
    "CANONICAL_SET, SYN_MAP, SYMBOL2TYPE = load_hgnc_resources(\n",
    "    HGNC_TSV, keep_status=KEEP_STATUS, keep_locus_types=KEEP_LOCUS_TYPES\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b50232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44459"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CANONICAL_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05d1c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åç§°å¯¹é½å®Œæˆï¼šæ˜ å°„ 4250ï¼›è¿‡æ»¤ 161962ï¼›æœ€ç»ˆåŸºå›  30716\n"
     ]
    }
   ],
   "source": [
    "# 2.4 åç§°å¯¹é½ + åŒä¹‰åæ˜ å°„ + å®˜æ–¹è¿‡æ»¤ + åˆ—èšåˆï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰\n",
    "adata = enforce_hgnc_and_collapse(\n",
    "    adata, canonical_set=CANONICAL_SET, synonym_map=SYN_MAP, audit_dir=CACHE_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a37d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… QCå: spots 1351225â†’1002113, genes 30716â†’20083 (min_cells=1002)\n"
     ]
    }
   ],
   "source": [
    "# 2.5 QC è¿‡æ»¤ + æ ‡å‡†åŒ–\n",
    "adata = qc_and_basic_filters(adata)\n",
    "adata = normalize_log1p(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff802aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¢„å¤„ç†åå¯¹è±¡å·²ç¼“å­˜: /cwStorage/nodecw_group/jijh/hest_sentences_hugo/cache/adata_merged_preprocessed_canonical_collapsed.h5ad\n"
     ]
    }
   ],
   "source": [
    "# 2.6 ç¼“å­˜\n",
    "os.makedirs(os.path.dirname(CACHE_H5AD), exist_ok=True)\n",
    "adata.write(CACHE_H5AD)\n",
    "print(f\"é¢„å¤„ç†åå¯¹è±¡å·²ç¼“å­˜: {CACHE_H5AD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f45447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®¡ç®— HVG ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3_paper'` expects raw count data, but non-integers were found.\n",
      "  return fn(*args_all, **kw)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "HVG ä¸­ä»å«éæ ‡å‡†åï¼ˆå¦‚ -1 åç¼€ï¼‰: ['ERV3-1', 'H1-2', 'H1-3', 'H1-4', 'H1-5', 'NKX2-1', 'NKX3-1']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m hvgs = compute_hvgs(adata.copy(), n_top=N_TOP_HVGS, batch_key=BATCH_KEY, flavor=\u001b[33m'\u001b[39m\u001b[33mseurat_v3_paper\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 2.8 HVG éªŒè¯ + ä¿å­˜ï¼ˆå¯¹ç…§å®˜æ–¹é›†åˆä¸ç±»å‹æ¦‚è§ˆï¼‰\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mvalidate_hvgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhvgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonical_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCANONICAL_SET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol2type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSYMBOL2TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_expected\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TOP_HVGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m save_hvgs(hvgs, HVG_TXT)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHVG å®Œæˆ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hvgs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m genes; ç”¨æ—¶ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_time(time.time()-t0)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 255\u001b[39m, in \u001b[36mvalidate_hvgs\u001b[39m\u001b[34m(hvgs, canonical_set, symbol2type, n_expected)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# æ—  -æ•°å­— åç¼€\u001b[39;00m\n\u001b[32m    254\u001b[39m bad = [g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m hvgs \u001b[38;5;28;01mif\u001b[39;00m re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+$\u001b[39m\u001b[33m\"\u001b[39m, g)]\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bad) == \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHVG ä¸­ä»å«éæ ‡å‡†åï¼ˆå¦‚ -1 åç¼€ï¼‰: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbad[:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m canonical_set:\n\u001b[32m    258\u001b[39m     outside = [g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m hvgs \u001b[38;5;28;01mif\u001b[39;00m g.upper() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m canonical_set]\n",
      "\u001b[31mAssertionError\u001b[39m: HVG ä¸­ä»å«éæ ‡å‡†åï¼ˆå¦‚ -1 åç¼€ï¼‰: ['ERV3-1', 'H1-2', 'H1-3', 'H1-4', 'H1-5', 'NKX2-1', 'NKX3-1']"
     ]
    }
   ],
   "source": [
    "# 2.7 HVGï¼ˆSeurat v3 paper, æ‰¹æ¬¡æ„ŸçŸ¥ï¼‰\n",
    "print(\"è®¡ç®— HVG ...\")\n",
    "hvgs = compute_hvgs(adata.copy(), n_top=N_TOP_HVGS, batch_key=BATCH_KEY, flavor='seurat_v3_paper')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e0fae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_hvgs(hvgs: pd.Index,\n",
    "                  canonical_set: set[str] | None,\n",
    "                  symbol2type: dict[str, str] | None,\n",
    "                  n_expected: int):\n",
    "    # 1) æ— é‡å¤\n",
    "    assert not pd.Index(hvgs).duplicated().any(), \"HVG åˆ—è¡¨å­˜åœ¨é‡å¤åŸºå› å\"\n",
    "\n",
    "    # 2) ä¸å®˜æ–¹é›†åˆå¯¹é½ï¼ˆä¸»åˆ¤æ®ï¼‰\n",
    "    outside = []\n",
    "    if canonical_set:\n",
    "        # æ³¨æ„ï¼šæˆ‘ä»¬çš„ç®¡çº¿å·²ç»Ÿä¸€å¤§å†™ï¼›ä¸ºç¨³å¦¥ä»ä½¿ç”¨ upper() å¯¹é½\n",
    "        outside = [g for g in hvgs if g.upper() not in canonical_set]\n",
    "        if outside:\n",
    "            print(f\"âš ï¸ æœ‰ {len(outside)} ä¸ª HVG ä¸åœ¨ HGNC å®˜æ–¹é›†åˆï¼ˆç¤ºä¾‹ï¼‰ï¼š{outside[:10]}\")\n",
    "\n",
    "    # 3) ä»…å¯¹â€œä¸åœ¨å®˜æ–¹é›†åˆâ€çš„åŸºå› å†åšåç¼€å¯ç–‘æ€§æ£€æµ‹\n",
    "    suspicious_suffix = [g for g in outside if re.search(r\"-\\d+$\", g)]\n",
    "    # â€”â€” å¦‚æœä½ å¸Œæœ›å¯¹è¿™ç±»â€œå¼ºç–‘ä¼¼ make_unique ä¼ªåç¼€â€ä¸¥è‹›å¤„ç†ï¼Œç”¨ assertï¼›å¦åˆ™æ”¹æˆ warningï¼š\n",
    "    assert len(suspicious_suffix) == 0, (\n",
    "        \"HVG ä¸­å­˜åœ¨ç–‘ä¼¼å”¯ä¸€åŒ–ä¼ªåç¼€ï¼ˆä¸åœ¨HGNCä¸”å½¢å¦‚ `-æ•°å­—`ï¼‰çš„åŸºå› ï¼š\"\n",
    "        f\"{suspicious_suffix[:10]}\"\n",
    "    )\n",
    "\n",
    "    # 4) å¯é€‰ï¼šlocus_type åˆ†å¸ƒï¼Œä¾¿äºå®¡è®¡ï¼ˆä¿¡æ¯æ€§è¾“å‡ºï¼Œä¸åšå¼ºæ ¡éªŒï¼‰\n",
    "    if symbol2type:\n",
    "        types = pd.Series([symbol2type.get(g.upper(), \"UNK\") for g in hvgs]).value_counts()\n",
    "        types.to_csv(os.path.join(CACHE_DIR, \"hvg_locus_type_distribution.tsv\"),\n",
    "                     sep='\\t', header=['count'])\n",
    "        print(\"HVG locus_type åˆ†å¸ƒå·²å†™å…¥ cache/hvg_locus_type_distribution.tsv\")\n",
    "\n",
    "    # 5) æ•°é‡æé†’ï¼ˆä¸å¼ºåˆ¶ï¼‰\n",
    "    if len(hvgs) != n_expected:\n",
    "        print(f\"âš ï¸ HVG æ•°é‡ {len(hvgs)} != æœŸæœ› {n_expected}ã€‚è‹¥å‰é¢åšäº†ä¸¥æ ¼è¿‡æ»¤ï¼Œè¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59ad715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVG locus_type åˆ†å¸ƒå·²å†™å…¥ cache/hvg_locus_type_distribution.tsv\n",
      "HVG åˆ—è¡¨å·²ä¿å­˜: /cwStorage/nodecw_group/jijh/hest_sentences_hugo/global_hvgs.txt\n",
      "HVG å®Œæˆ: 5000 genes; ç”¨æ—¶ 55åˆ† 31.36ç§’\n"
     ]
    }
   ],
   "source": [
    "# 2.8 HVG éªŒè¯ + ä¿å­˜ï¼ˆå¯¹ç…§å®˜æ–¹é›†åˆä¸ç±»å‹æ¦‚è§ˆï¼‰\n",
    "validate_hvgs(hvgs, canonical_set=CANONICAL_SET, symbol2type=SYMBOL2TYPE, n_expected=N_TOP_HVGS)\n",
    "save_hvgs(hvgs, HVG_TXT)\n",
    "\n",
    "print(f\"HVG å®Œæˆ: {len(hvgs)} genes; ç”¨æ—¶ {format_time(time.time()-t0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aa3d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹ç”ŸæˆåŸºå› å¥å­ï¼ˆåŸºäº HVGï¼‰ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/jijh/micromamba/envs/gigapath/lib/python3.13/site-packages/anndata/_core/anndata.py:1022: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sub[k] = df_sub[k].cat.remove_unused_categories()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0688105ee240fe8b065e684722f24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥å­ç”Ÿæˆå®Œæ¯•ã€‚\n",
      "å¥å­éªŒè¯ï¼šé€šè¿‡ 20/20 ä¸ªæ ·æœ¬ã€‚\n",
      "å…¨æµç¨‹å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 3) åŸºäº HVG çš„â€œåŸºå› å¥å­â€ç”Ÿæˆï¼ˆNLP åŒ–ï¼‰\n",
    "# =========================================================\n",
    "print(\"\\nå¼€å§‹ç”ŸæˆåŸºå› å¥å­ï¼ˆåŸºäº HVGï¼‰ ...\")\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "sid_list = adata.obs[BATCH_KEY].unique().tolist()\n",
    "\n",
    "def _process_sid(sid: str):\n",
    "    ad_s = adata[adata.obs[BATCH_KEY]==sid].copy()\n",
    "    make_sentences_for_sample(sid, ad_s, hvgs, OUT_BASE, k=N_TOP_GENES_IN_SENT)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=12) as ex:\n",
    "    list(tqdm(ex.map(_process_sid, sid_list), total=len(sid_list)))\n",
    "\n",
    "print(\"å¥å­ç”Ÿæˆå®Œæ¯•ã€‚\")\n",
    "\n",
    "# 3.1 å¥å­éªŒè¯ï¼ˆæŠ½æ ·ï¼‰\n",
    "validate_sentences_random(set(hvgs), OUT_BASE, sid_list, n_checks=min(20, len(sid_list)))\n",
    "\n",
    "print(\"å…¨æµç¨‹å®Œæˆã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ba1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gigapath)",
   "language": "python",
   "name": "gigapath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
