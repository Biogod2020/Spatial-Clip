{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d141f1d2",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:06:09,850 - INFO - --- 1. é‡åŒ–é—®é¢˜: å¯¹æ¯” 'NCBI525' çš„ Spot æ•°é‡ ---\n",
      "2025-11-06 10:06:09,927 - INFO - åŸå§‹ AnnData (.h5ad) ä¸­ 'NCBI525' çš„ Spot æ€»æ•°: 777\n",
      "2025-11-06 10:06:10,323 - INFO - å·²å¤„ç† Parquet ä¸­ 'NCBI525' çš„ Spot æ•°é‡: 112\n",
      "2025-11-06 10:06:10,324 - INFO - æ•°é‡å·®å¼‚: 665 ä¸ª spots è¢«ä¸¢å¼ƒ (æŸå¤±ç‡: 85.59%)\n",
      "2025-11-06 10:06:10,325 - WARNING - ğŸ”¥ æ•°é‡ä¸¥é‡ä¸åŒ¹é…ï¼è¿™æ˜¯é—®é¢˜çš„æ ¸å¿ƒã€‚\n",
      "2025-11-06 10:06:10,326 - INFO - \n",
      "--- 2. å®šä½å¤±è¸ªä¸å¹¸å­˜çš„ Spot ---\n",
      "2025-11-06 10:06:10,327 - INFO - æ‰¾åˆ°äº† 777 ä¸ª 'å¤±è¸ª' çš„ Spotã€‚ç¤ºä¾‹: ['GTGTACCTTGGCTACG-1', 'GTGCTCAAGTACTGTC-1', 'GTGATTCGCCGCTCAA-1', 'CACCTAATCAGTTTAC-1', 'AGCACTTAAGGACGCC-1']\n",
      "2025-11-06 10:06:10,327 - INFO - \n",
      "--- 3. æ ¹æœ¬åŸå› åˆ†æ: æ£€æŸ¥ 'å¤±è¸ª' Spot å¯¹åº”çš„ç“¦ç‰‡æ–‡ä»¶ ---\n",
      "2025-11-06 10:06:10,328 - INFO - ä»å¹¸å­˜çš„ spot æ¨æ–­å‡ºç“¦ç‰‡ç›®å½•: /cpfs01/projects-HDD/cfff-afe2df89e32e_HDD/jjh_19301050235/my_data/hest_data/hest_output/NCBI525_tiles\n",
      "2025-11-06 10:06:10,328 - INFO - ä»å¹¸å­˜çš„ spot æ¨æ–­å‡ºæ–‡ä»¶æ‰©å±•å: .png\n",
      "2025-11-06 10:06:10,328 - INFO - å¯¹ 5 ä¸ª 'å¤±è¸ª' Spot çš„æ–‡ä»¶è¿›è¡Œæ£€æŸ¥...\n",
      "2025-11-06 10:06:10,334 - INFO -   - Spot 'GTGTACCTTGGCTACG-1', è·¯å¾„: /cpfs01/projects-HDD/cfff-afe2df89e32e_HDD/jjh_19301050235/my_data/hest_data/hest_output/NCBI525_tiles/GTGTACCTTGGCTACG-1.png -> âŒ æ–‡ä»¶ä¸å­˜åœ¨\n",
      "2025-11-06 10:06:10,335 - INFO -   - Spot 'GTGCTCAAGTACTGTC-1', è·¯å¾„: /cpfs01/projects-HDD/cfff-afe2df89e32e_HDD/jjh_19301050235/my_data/hest_data/hest_output/NCBI525_tiles/GTGCTCAAGTACTGTC-1.png -> âŒ æ–‡ä»¶ä¸å­˜åœ¨\n",
      "2025-11-06 10:06:10,335 - INFO -   - Spot 'GTGATTCGCCGCTCAA-1', è·¯å¾„: /cpfs01/projects-HDD/cfff-afe2df89e32e_HDD/jjh_19301050235/my_data/hest_data/hest_output/NCBI525_tiles/GTGATTCGCCGCTCAA-1.png -> âŒ æ–‡ä»¶ä¸å­˜åœ¨\n",
      "2025-11-06 10:06:10,335 - INFO -   - Spot 'CACCTAATCAGTTTAC-1', è·¯å¾„: /cpfs01/projects-HDD/cfff-afe2df89e32e_HDD/jjh_19301050235/my_data/hest_data/hest_output/NCBI525_tiles/CACCTAATCAGTTTAC-1.png -> âŒ æ–‡ä»¶ä¸å­˜åœ¨\n",
      "2025-11-06 10:06:10,336 - INFO -   - Spot 'AGCACTTAAGGACGCC-1', è·¯å¾„: /cpfs01/projects-HDD/cfff-afe2df89e32e_HDD/jjh_19301050235/my_data/hest_data/hest_output/NCBI525_tiles/AGCACTTAAGGACGCC-1.png -> âŒ æ–‡ä»¶ä¸å­˜åœ¨\n",
      "2025-11-06 10:06:10,336 - INFO - \n",
      "--- è¯Šæ–­ç»“è®º ---\n",
      "2025-11-06 10:06:10,336 - ERROR - ğŸ”¥ ä¸»è¦é—®é¢˜: è®¸å¤šç“¦ç‰‡æ–‡ä»¶ä»æœªè¢«ç”Ÿæˆæˆ–è·¯å¾„é”™è¯¯ï¼Œå¯¼è‡´å®ƒä»¬åœ¨é¢„å¤„ç†çš„ç¬¬ä¸€æ­¥å°±è¢«è¿‡æ»¤æ‰äº†ã€‚\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ’¡ è¡ŒåŠ¨å»ºè®®:\n",
      "1. è¯·å›åˆ°æ‚¨æœ€åˆçš„åˆ‡ç‰‡è„šæœ¬ (å¯èƒ½åœ¨ `d1_...ipynb` ä¹‹å‰æˆ–ä¹‹ä¸­)ã€‚\n",
      "2. æ£€æŸ¥å¤„ç†æ ·æœ¬ 'NCBI525' æ—¶æ˜¯å¦æœ‰ä»»ä½•é”™è¯¯æ—¥å¿—ã€‚\n",
      "3. å¯èƒ½çš„åŸå› åŒ…æ‹¬ï¼š\n",
      "   - WSI æ–‡ä»¶æœ¬èº«å­˜åœ¨é—®é¢˜ (ä¾‹å¦‚ï¼Œéƒ¨åˆ†åŒºåŸŸæŸå)ã€‚\n",
      "   - å†™å…¥ç“¦ç‰‡æ–‡ä»¶æ—¶é‡åˆ°äº†ç£ç›˜ç©ºé—´ä¸è¶³æˆ–æƒé™é—®é¢˜ã€‚\n",
      "   - è„šæœ¬ä¸­çš„åæ ‡è½¬æ¢é€»è¾‘å¯¹äºè¿™ä¸ªç‰¹å®šæ ·æœ¬å¯èƒ½å­˜åœ¨ bugã€‚\n",
      "4. è§£å†³æ–¹æ¡ˆ: éœ€è¦é‡æ–°è¿è¡Œå¯¹ `NCBI525` æ ·æœ¬çš„ç“¦ç‰‡åˆ‡å‰²è¿‡ç¨‹ï¼Œå¹¶å¯†åˆ‡ç›‘æ§æ—¥å¿—ä»¥ç¡®ä¿æ‰€æœ‰ç“¦ç‰‡éƒ½è¢«æ­£ç¡®ç”Ÿæˆã€‚\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell: Deep Dive Diagnostic for Sample NCBI525 =====\n",
    "# CodeGuardian: This cell is a high-precision diagnostic tool to uncover why\n",
    "# sample NCBI525 has missing data. We will trace the data from its source\n",
    "# to its final state, pinpointing the exact point of failure.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata\n",
    "import logging\n",
    "import os\n",
    "import rootutils\n",
    "\n",
    "# --- 1. é…ç½® (ä¸æ‚¨çš„ç¯å¢ƒä¿æŒä¸€è‡´) ---\n",
    "# Use rootutils to find the project root based on a marker file (e.g., .git)\n",
    "try:\n",
    "    PROJECT_ROOT = rootutils.find_root(search_from=__file__, indicator=\".git\")\n",
    "except NameError:\n",
    "    # Fallback for interactive environments like Jupyter\n",
    "    PROJECT_ROOT = rootutils.find_root(search_from=\".\", indicator=\".git\")\n",
    "\n",
    "if not PROJECT_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨: {PROJECT_ROOT}\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# a. åŸå§‹ HEST æ•°æ®é›†ç›®å½• (ç”¨äºåŠ è½½ .h5ad)\n",
    "RAW_DATA_DIR = PROJECT_ROOT / \"data\" / \"hest_1k_original\"\n",
    "# b. d1 notebook ç”Ÿæˆçš„å·²å¤„ç†æ•°æ®ç›®å½•\n",
    "# åŠ¨æ€æŸ¥æ‰¾åŒ…å« nodes.parquet çš„ç›®å½•ï¼Œé¿å…ç¡¬ç¼–ç è·¯å¾„\n",
    "def find_processed_data_dir(base_path: Path) -> Path | None:\n",
    "    for p in base_path.iterdir():\n",
    "        if p.is_dir() and (p / \"nodes.parquet\").exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "PROCESSED_DATA_DIR = find_processed_data_dir(PROJECT_ROOT / \"data\")\n",
    "if PROCESSED_DATA_DIR is None:\n",
    "    raise FileNotFoundError(f\"åœ¨ '{PROJECT_ROOT / 'data'}' ç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½•åŒ…å« 'nodes.parquet' çš„å·²å¤„ç†æ•°æ®æ–‡ä»¶å¤¹ã€‚\")\n",
    "\n",
    "# c. ç›®æ ‡æ ·æœ¬\n",
    "TARGET_SAMPLE_ID = \"NCBI525\"\n",
    "\n",
    "logger = logging.getLogger(\"ncbi525_diagnostic\")\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "# --- 2. é‡åŒ–é—®é¢˜ï¼šå¯¹æ¯” AnnData ä¸ Parquet ä¸­çš„ Spot æ•°é‡ ---\n",
    "logger.info(f\"--- 1. é‡åŒ–é—®é¢˜: å¯¹æ¯” '{TARGET_SAMPLE_ID}' çš„ Spot æ•°é‡ ---\")\n",
    "adata_path = RAW_DATA_DIR / \"st\" / f\"{TARGET_SAMPLE_ID}.h5ad\"\n",
    "if not adata_path.exists():\n",
    "    raise FileNotFoundError(f\"æœªæ‰¾åˆ° AnnData æ–‡ä»¶: {adata_path}\")\n",
    "\n",
    "nodes_path = PROCESSED_DATA_DIR / \"nodes.parquet\"\n",
    "if not nodes_path.exists():\n",
    "    raise FileNotFoundError(f\"æœªæ‰¾åˆ°å¤„ç†åçš„èŠ‚ç‚¹æ–‡ä»¶: {nodes_path}\")\n",
    "\n",
    "try:\n",
    "    adata = anndata.read_h5ad(adata_path)\n",
    "except Exception as exc:\n",
    "    logger.error(\"âŒ åœ¨è¯»å– AnnData æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "adata_spot_ids = set(adata.obs.index)\n",
    "num_adata_spots = len(adata_spot_ids)\n",
    "logger.info(f\"åŸå§‹ AnnData (.h5ad) ä¸­ '{TARGET_SAMPLE_ID}' çš„ Spot æ€»æ•°: {num_adata_spots}\")\n",
    "\n",
    "try:\n",
    "    nodes_df = pd.read_parquet(nodes_path)\n",
    "except Exception:\n",
    "    logger.error(\"âŒ åœ¨è¯»å– nodes.parquet æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "processed_nodes_for_sample = nodes_df[nodes_df[\"sample_id\"] == TARGET_SAMPLE_ID].copy()\n",
    "if processed_nodes_for_sample.empty:\n",
    "    logger.warning(\"âš ï¸ åœ¨å¤„ç†æ•°æ®é›†ä¸­æœªæ‰¾åˆ°ç›®æ ‡æ ·æœ¬ã€‚åç»­æ£€æŸ¥å°†è¢«è·³è¿‡ã€‚\")\n",
    "else:\n",
    "    num_processed_spots = len(processed_nodes_for_sample)\n",
    "    logger.info(f\"å·²å¤„ç† Parquet ä¸­ '{TARGET_SAMPLE_ID}' çš„ Spot æ•°é‡: {num_processed_spots}\")\n",
    "\n",
    "    discrepancy = num_adata_spots - num_processed_spots\n",
    "    loss_percentage = (discrepancy / num_adata_spots) * 100 if num_adata_spots > 0 else 0.0\n",
    "    logger.info(f\"æ•°é‡å·®å¼‚: {discrepancy} ä¸ª spots è¢«ä¸¢å¼ƒ (æŸå¤±ç‡: {loss_percentage:.2f}%)\")\n",
    "\n",
    "    if discrepancy == 0:\n",
    "        logger.info(\"âœ… æ•°é‡ä¸€è‡´ï¼Œé—®é¢˜å¯èƒ½ä¸åœ¨äºæ•°æ®ä¸¢å¤±ã€‚\")\n",
    "    else:\n",
    "        logger.warning(\"ğŸ”¥ æ•°é‡ä¸¥é‡ä¸åŒ¹é…ï¼è¿™æ˜¯é—®é¢˜çš„æ ¸å¿ƒã€‚\")\n",
    "\n",
    "# --- 3. å®šä½â€œå¤±è¸ªâ€ä¸â€œå¹¸å­˜â€çš„ Spot ---\n",
    "if not processed_nodes_for_sample.empty:\n",
    "    logger.info(\"\\n--- 2. å®šä½å¤±è¸ªä¸å¹¸å­˜çš„ Spot ---\")\n",
    "\n",
    "    def parse_spot_id_from_path_v2(path_str: str | Path) -> str | None:\n",
    "        try:\n",
    "            return Path(path_str).stem\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    valid_image_paths = processed_nodes_for_sample[\"image_path\"].dropna().tolist()\n",
    "    processed_spot_ids_from_path = {\n",
    "        spot_id\n",
    "        for spot_id in (parse_spot_id_from_path_v2(path) for path in valid_image_paths)\n",
    "        if spot_id\n",
    "    }\n",
    "\n",
    "    missing_spot_ids = adata_spot_ids - processed_spot_ids_from_path\n",
    "    logger.info(f\"æ‰¾åˆ°äº† {len(missing_spot_ids)} ä¸ª 'å¤±è¸ª' çš„ Spotã€‚ç¤ºä¾‹: {list(missing_spot_ids)[:5]}\")\n",
    "\n",
    "    # --- 4. å®¡é—®â€œå¤±è¸ªè€…â€ï¼šéªŒè¯å®ƒä»¬å¯¹åº”çš„ç“¦ç‰‡æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ ---\n",
    "    logger.info(\"\\n--- 3. æ ¹æœ¬åŸå› åˆ†æ: æ£€æŸ¥ 'å¤±è¸ª' Spot å¯¹åº”çš„ç“¦ç‰‡æ–‡ä»¶ ---\")\n",
    "    \n",
    "    # ä»å¹¸å­˜çš„ spot æ¨æ–­ç“¦ç‰‡ç›®å½•å’Œæ–‡ä»¶æ‰©å±•å\n",
    "    tile_dir = None\n",
    "    tile_ext = None\n",
    "    if valid_image_paths:\n",
    "        first_path = Path(valid_image_paths[0])\n",
    "        tile_dir = first_path.parent\n",
    "        tile_ext = first_path.suffix\n",
    "        logger.info(f\"ä»å¹¸å­˜çš„ spot æ¨æ–­å‡ºç“¦ç‰‡ç›®å½•: {tile_dir}\")\n",
    "        logger.info(f\"ä»å¹¸å­˜çš„ spot æ¨æ–­å‡ºæ–‡ä»¶æ‰©å±•å: {tile_ext}\")\n",
    "\n",
    "    if not tile_dir or not tile_ext:\n",
    "        logger.warning(\"âš ï¸ æ— æ³•ä» 'nodes.parquet' æ¨æ–­ç“¦ç‰‡ç›®å½•æˆ–æ‰©å±•åï¼Œè·³è¿‡æ–‡ä»¶æ£€æŸ¥ã€‚\")\n",
    "    else:\n",
    "        logger.info(f\"å¯¹ {min(5, len(missing_spot_ids))} ä¸ª 'å¤±è¸ª' Spot çš„æ–‡ä»¶è¿›è¡Œæ£€æŸ¥...\")\n",
    "        num_missing_files = 0\n",
    "        num_empty_files = 0\n",
    "\n",
    "        for spot_id in list(missing_spot_ids)[:5]:\n",
    "            # åŸºäºæ¨æ–­çš„ç›®å½•å’Œæ‰©å±•åæ„å»ºç“¦ç‰‡æ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "            path_to_check = tile_dir / f\"{spot_id}{tile_ext}\"\n",
    "            \n",
    "            if not path_to_check.is_absolute():\n",
    "                path_to_check = (PROJECT_ROOT / path_to_check).resolve()\n",
    "            \n",
    "            log_msg = f\"  - Spot '{spot_id}', è·¯å¾„: {path_to_check}\"\n",
    "            if not path_to_check.exists():\n",
    "                log_msg += \" -> âŒ æ–‡ä»¶ä¸å­˜åœ¨\"\n",
    "                num_missing_files += 1\n",
    "            elif path_to_check.stat().st_size == 0:\n",
    "                log_msg += \" -> âŒ æ–‡ä»¶å¤§å°ä¸º 0\"\n",
    "                num_empty_files += 1\n",
    "            else:\n",
    "                log_msg += f\" -> âœ… æ–‡ä»¶å­˜åœ¨ä¸”å¤§å°ä¸º {path_to_check.stat().st_size} bytes\"\n",
    "            logger.info(log_msg)\n",
    "\n",
    "        logger.info(\"\\n--- è¯Šæ–­ç»“è®º ---\")\n",
    "        if num_missing_files > 0:\n",
    "            logger.error(\"ğŸ”¥ ä¸»è¦é—®é¢˜: è®¸å¤šç“¦ç‰‡æ–‡ä»¶ä»æœªè¢«ç”Ÿæˆæˆ–è·¯å¾„é”™è¯¯ï¼Œå¯¼è‡´å®ƒä»¬åœ¨é¢„å¤„ç†çš„ç¬¬ä¸€æ­¥å°±è¢«è¿‡æ»¤æ‰äº†ã€‚\")\n",
    "        if num_empty_files > 0:\n",
    "            logger.error(\"ğŸ”¥ ä¸»è¦é—®é¢˜: è®¸å¤šç“¦ç‰‡æ–‡ä»¶è¢«ç”Ÿæˆä¸ºç©ºæ–‡ä»¶ (0KB)ï¼ŒåŒæ ·å¯¼è‡´å®ƒä»¬è¢«è¿‡æ»¤ã€‚\")\n",
    "        if num_missing_files == 0 and num_empty_files == 0:\n",
    "            logger.info(\"âœ… 'å¤±è¸ª' spot å¯¹åº”çš„ç“¦ç‰‡æ–‡ä»¶ä¼¼ä¹å­˜åœ¨ä¸”æœ‰æ•ˆã€‚é—®é¢˜å¯èƒ½åœ¨é¢„å¤„ç†çš„ç­›é€‰é€»è¾‘ä¸­ï¼Œè€Œä¸æ˜¯æ–‡ä»¶ä¸¢å¤±ã€‚\")\n",
    "\n",
    "        if num_missing_files > 0 or num_empty_files > 0:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ğŸ’¡ è¡ŒåŠ¨å»ºè®®:\")\n",
    "            print(\"1. è¯·å›åˆ°æ‚¨æœ€åˆçš„åˆ‡ç‰‡è„šæœ¬ (å¯èƒ½åœ¨ `d1_...ipynb` ä¹‹å‰æˆ–ä¹‹ä¸­)ã€‚\")\n",
    "            print(f\"2. æ£€æŸ¥å¤„ç†æ ·æœ¬ '{TARGET_SAMPLE_ID}' æ—¶æ˜¯å¦æœ‰ä»»ä½•é”™è¯¯æ—¥å¿—ã€‚\")\n",
    "            print(\"3. å¯èƒ½çš„åŸå› åŒ…æ‹¬ï¼š\")\n",
    "            print(\"   - WSI æ–‡ä»¶æœ¬èº«å­˜åœ¨é—®é¢˜ (ä¾‹å¦‚ï¼Œéƒ¨åˆ†åŒºåŸŸæŸå)ã€‚\")\n",
    "            print(\"   - å†™å…¥ç“¦ç‰‡æ–‡ä»¶æ—¶é‡åˆ°äº†ç£ç›˜ç©ºé—´ä¸è¶³æˆ–æƒé™é—®é¢˜ã€‚\")\n",
    "            print(\"   - è„šæœ¬ä¸­çš„åæ ‡è½¬æ¢é€»è¾‘å¯¹äºè¿™ä¸ªç‰¹å®šæ ·æœ¬å¯èƒ½å­˜åœ¨ bugã€‚\")\n",
    "            print(f\"4. è§£å†³æ–¹æ¡ˆ: éœ€è¦é‡æ–°è¿è¡Œå¯¹ `{TARGET_SAMPLE_ID}` æ ·æœ¬çš„ç“¦ç‰‡åˆ‡å‰²è¿‡ç¨‹ï¼Œå¹¶å¯†åˆ‡ç›‘æ§æ—¥å¿—ä»¥ç¡®ä¿æ‰€æœ‰ç“¦ç‰‡éƒ½è¢«æ­£ç¡®ç”Ÿæˆã€‚\")\n",
    "            print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f05a3",
   "metadata": {},
   "source": [
    "## éšæœºæŠ½å–åä¸ªæ ·æœ¬è¿›è¡Œç›‘æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:33:19,847 - INFO - --- Starting Widespread Data Loss Diagnostic ---\n",
      "2025-11-06 10:33:21,528 - INFO - Randomly selected 10 samples for diagnosis: ['NCBI574', 'INT20', 'NCBI876', 'TENX115', 'NCBI510', 'MEND51', 'MEND151', 'NCBI783', 'MISC52', 'INT12']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba915161c01a425590f122288fbad721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Diagnosing Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:33:21,909 - INFO - [NCBI574] âœ… All spots accounted for. (1505 spots)\n",
      "2025-11-06 10:33:22,276 - WARNING - [INT20] ğŸ”¥ Mismatch found! AnnData: 4860, Parquet: 4805. Loss: 55 spots (1.13%)\n",
      "2025-11-06 10:33:22,299 - ERROR - [INT20] âŒ At least 5 of 5 checked 'missing' spots correspond to non-existent tile image files.\n",
      "2025-11-06 10:33:22,387 - WARNING - [NCBI876] ğŸ”¥ Mismatch found! AnnData: 1064, Parquet: 348. Loss: 716 spots (67.29%)\n",
      "2025-11-06 10:33:22,390 - ERROR - [NCBI876] âŒ At least 5 of 5 checked 'missing' spots correspond to non-existent tile image files.\n",
      "2025-11-06 10:33:22,490 - WARNING - [TENX115] ğŸ”¥ Mismatch found! AnnData: 3886, Parquet: 1403. Loss: 2483 spots (63.90%)\n",
      "2025-11-06 10:33:22,498 - ERROR - [TENX115] âŒ At least 5 of 5 checked 'missing' spots correspond to non-existent tile image files.\n",
      "2025-11-06 10:33:22,607 - WARNING - [NCBI510] ğŸ”¥ Mismatch found! AnnData: 525, Parquet: 167. Loss: 358 spots (68.19%)\n",
      "2025-11-06 10:33:22,609 - ERROR - [NCBI510] âŒ At least 5 of 5 checked 'missing' spots correspond to non-existent tile image files.\n",
      "2025-11-06 10:33:22,800 - INFO - [MEND51] âœ… All spots accounted for. (416 spots)\n",
      "2025-11-06 10:33:23,112 - WARNING - [MEND151] ğŸ”¥ Mismatch found! AnnData: 2600, Parquet: 2583. Loss: 17 spots (0.65%)\n",
      "2025-11-06 10:33:23,126 - ERROR - [MEND151] âŒ At least 5 of 5 checked 'missing' spots correspond to non-existent tile image files.\n",
      "2025-11-06 10:33:23,241 - WARNING - [NCBI783] ğŸ”¥ Mismatch found! AnnData: 3869, Parquet: 2254. Loss: 1615 spots (41.74%)\n",
      "2025-11-06 10:33:23,253 - ERROR - [NCBI783] âŒ At least 5 of 5 checked 'missing' spots correspond to non-existent tile image files.\n",
      "2025-11-06 10:33:23,442 - WARNING - [MISC52] ğŸ”¥ Mismatch found! AnnData: 229, Parquet: 228. Loss: 1 spots (0.44%)\n",
      "2025-11-06 10:33:23,444 - ERROR - [MISC52] âŒ At least 5 of 5 checked 'missing' spots correspond to non-existent tile image files.\n",
      "2025-11-06 10:33:23,726 - INFO - [INT12] âœ… All spots accounted for. (1451 spots)\n",
      "2025-11-06 10:33:23,733 - ERROR - \n",
      "ğŸ”¥ CRITICAL FINDING: 7 out of 10 checked samples have significant data loss, likely due to missing tile files.\n",
      "2025-11-06 10:33:23,733 - ERROR - This indicates a systemic problem in the initial tile generation/saving process. Please review the upstream scripts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                       Widespread Data Loss Diagnostic Summary\n",
      "================================================================================\n",
      "  sample_id       diagnosis  adata_spots  processed_spots loss_percentage  \\\n",
      "0   NCBI574              OK         1505             1505           0.00%   \n",
      "1     INT20  File Not Found         4860             4805           1.13%   \n",
      "2   NCBI876  File Not Found         1064              348          67.29%   \n",
      "3   TENX115  File Not Found         3886             1403          63.90%   \n",
      "4   NCBI510  File Not Found          525              167          68.19%   \n",
      "5    MEND51              OK          416              416           0.00%   \n",
      "6   MEND151  File Not Found         2600             2583           0.65%   \n",
      "7   NCBI783  File Not Found         3869             2254          41.74%   \n",
      "8    MISC52  File Not Found          229              228           0.44%   \n",
      "9     INT12              OK         1451             1451           0.00%   \n",
      "\n",
      "                                                       details  \n",
      "0                                     All spots accounted for.  \n",
      "1  At least 5 of 5 checked 'missing' spots correspond to no...  \n",
      "2  At least 5 of 5 checked 'missing' spots correspond to no...  \n",
      "3  At least 5 of 5 checked 'missing' spots correspond to no...  \n",
      "4  At least 5 of 5 checked 'missing' spots correspond to no...  \n",
      "5                                     All spots accounted for.  \n",
      "6  At least 5 of 5 checked 'missing' spots correspond to no...  \n",
      "7  At least 5 of 5 checked 'missing' spots correspond to no...  \n",
      "8  At least 5 of 5 checked 'missing' spots correspond to no...  \n",
      "9                                     All spots accounted for.  \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell: Widespread Data Loss Diagnostic (10 Random Samples) =====\n",
    "# CodeGuardian: This cell automates the diagnostic process from the previous cell\n",
    "# for a random subset of samples to determine if the data loss issue is systemic.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import rootutils\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. é…ç½® (ä¸æ‚¨çš„ç¯å¢ƒä¿æŒä¸€è‡´) ---\n",
    "# Use rootutils to find the project root based on a marker file (e.g., .project-root)\n",
    "try:\n",
    "    PROJECT_ROOT = rootutils.find_root(search_from=__file__, indicator=\".project-root\")\n",
    "except NameError:\n",
    "    # Fallback for interactive environments like Jupyter\n",
    "    PROJECT_ROOT = rootutils.find_root(search_from=\".\", indicator=\".project-root\")\n",
    "\n",
    "if not PROJECT_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨: {PROJECT_ROOT}\")\n",
    "\n",
    "# a. åŸå§‹ HEST æ•°æ®é›†ç›®å½• (ç”¨äºåŠ è½½ .h5ad)\n",
    "RAW_DATA_DIR = PROJECT_ROOT / \"data\" / \"hest_1k_original\"\n",
    "# b. d1 notebook ç”Ÿæˆçš„å·²å¤„ç†æ•°æ®ç›®å½•\n",
    "# åŠ¨æ€æŸ¥æ‰¾åŒ…å« nodes.parquet çš„ç›®å½•ï¼Œé¿å…ç¡¬ç¼–ç è·¯å¾„\n",
    "def find_processed_data_dir(base_path: Path) -> Path | None:\n",
    "    # å‡è®¾å·²å¤„ç†æ•°æ®åœ¨ hest_hugo_6nei_correct_parquet_data/train ç›®å½•ä¸‹\n",
    "    candidate_path = base_path / \"hest_hugo_6nei_correct_parquet_data\" / \"train\"\n",
    "    if candidate_path.exists() and (candidate_path / \"nodes.parquet\").exists():\n",
    "        return candidate_path\n",
    "    \n",
    "    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå†è¿›è¡Œé€šç”¨æœç´¢\n",
    "    for p in base_path.iterdir():\n",
    "        if p.is_dir() and (p / \"nodes.parquet\").exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "PROCESSED_DATA_DIR = find_processed_data_dir(PROJECT_ROOT / \"data\")\n",
    "if PROCESSED_DATA_DIR is None:\n",
    "    raise FileNotFoundError(f\"åœ¨ '{PROJECT_ROOT / 'data'}' ç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½•åŒ…å« 'nodes.parquet' çš„å·²å¤„ç†æ•°æ®æ–‡ä»¶å¤¹ã€‚\")\n",
    "\n",
    "# --- 2. å°è£…è¯Šæ–­é€»è¾‘ä¸ºä¸€ä¸ªå‡½æ•° ---\n",
    "logger = logging.getLogger(\"widespread_diagnostic\")\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "def diagnose_sample_data_loss(sample_id: str, raw_data_dir: Path, processed_nodes_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Performs a deep-dive diagnostic for a single sample to check for data loss.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"sample_id\": sample_id,\n",
    "        \"adata_spots\": 0,\n",
    "        \"processed_spots\": 0,\n",
    "        \"loss_percentage\": 0.0,\n",
    "        \"diagnosis\": \"N/A\",\n",
    "        \"details\": \"\"\n",
    "    }\n",
    "    \n",
    "    # 1. åŠ è½½ AnnData\n",
    "    adata_path = raw_data_dir / \"st\" / f\"{sample_id}.h5ad\"\n",
    "    if not adata_path.exists():\n",
    "        result[\"diagnosis\"] = \"Error\"\n",
    "        result[\"details\"] = f\"AnnData file not found at {adata_path}\"\n",
    "        logger.error(f\"[{sample_id}] âŒ {result['details']}\")\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        adata = anndata.read_h5ad(adata_path)\n",
    "        adata_spot_ids = set(adata.obs.index)\n",
    "        result[\"adata_spots\"] = len(adata_spot_ids)\n",
    "    except Exception as e:\n",
    "        result[\"diagnosis\"] = \"Error\"\n",
    "        result[\"details\"] = f\"Failed to read AnnData file: {e}\"\n",
    "        logger.error(f\"[{sample_id}] âŒ {result['details']}\")\n",
    "        return result\n",
    "\n",
    "    # 2. ä»å·²å¤„ç†æ•°æ®ä¸­ç­›é€‰\n",
    "    sample_nodes_df = processed_nodes_df[processed_nodes_df[\"sample_id\"] == sample_id]\n",
    "    result[\"processed_spots\"] = len(sample_nodes_df)\n",
    "\n",
    "    if result[\"adata_spots\"] == 0:\n",
    "        result[\"diagnosis\"] = \"Warning\"\n",
    "        result[\"details\"] = \"Source AnnData has 0 spots.\"\n",
    "        logger.warning(f\"[{sample_id}] âš ï¸ {result['details']}\")\n",
    "        return result\n",
    "        \n",
    "    # 3. è®¡ç®—å·®å¼‚\n",
    "    discrepancy = result[\"adata_spots\"] - result[\"processed_spots\"]\n",
    "    result[\"loss_percentage\"] = (discrepancy / result[\"adata_spots\"]) * 100\n",
    "    \n",
    "    if discrepancy == 0:\n",
    "        result[\"diagnosis\"] = \"OK\"\n",
    "        result[\"details\"] = \"All spots accounted for.\"\n",
    "        logger.info(f\"[{sample_id}] âœ… {result['details']} ({result['adata_spots']} spots)\")\n",
    "        return result\n",
    "\n",
    "    logger.warning(\n",
    "        f\"[{sample_id}] ğŸ”¥ Mismatch found! \"\n",
    "        f\"AnnData: {result['adata_spots']}, Parquet: {result['processed_spots']}. \"\n",
    "        f\"Loss: {discrepancy} spots ({result['loss_percentage']:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # 4. æ ¹æœ¬åŸå› åˆ†æ\n",
    "    valid_image_paths = sample_nodes_df[\"image_path\"].dropna().tolist()\n",
    "    if not valid_image_paths:\n",
    "        result[\"diagnosis\"] = \"Critical\"\n",
    "        result[\"details\"] = \"No valid image paths found in processed data, but spots exist in AnnData.\"\n",
    "        logger.error(f\"[{sample_id}] âŒ {result['details']}\")\n",
    "        return result\n",
    "    \n",
    "    # ä»è·¯å¾„ä¸­è§£æå‡º spot_id\n",
    "    processed_spot_ids_from_path = {Path(p).stem for p in valid_image_paths}\n",
    "    missing_spot_ids = adata_spot_ids - processed_spot_ids_from_path\n",
    "\n",
    "    # æ¨æ–­ç“¦ç‰‡ç›®å½•å’Œæ‰©å±•å\n",
    "    first_path = Path(valid_image_paths[0])\n",
    "    tile_dir, tile_ext = first_path.parent, first_path.suffix\n",
    "\n",
    "    # æŠ½æŸ¥å‡ ä¸ªä¸¢å¤±çš„ spot\n",
    "    num_missing_files = 0\n",
    "    for spot_id in list(missing_spot_ids)[:5]: # Check up to 5 missing spots\n",
    "        path_to_check = tile_dir / f\"{spot_id}{tile_ext}\"\n",
    "        if not path_to_check.exists():\n",
    "            num_missing_files += 1\n",
    "            \n",
    "    if num_missing_files > 0:\n",
    "        result[\"diagnosis\"] = \"File Not Found\"\n",
    "        result[\"details\"] = f\"At least {num_missing_files} of 5 checked 'missing' spots correspond to non-existent tile image files.\"\n",
    "        logger.error(f\"[{sample_id}] âŒ {result['details']}\")\n",
    "    else:\n",
    "        result[\"diagnosis\"] = \"Logic/Filter Issue\"\n",
    "        result[\"details\"] = \"'Missing' spots seem to have valid image files. The issue is likely in the data filtering logic.\"\n",
    "        logger.warning(f\"[{sample_id}] âš ï¸ {result['details']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# --- 3. ä¸»æ‰§è¡Œé€»è¾‘ ---\n",
    "logger.info(\"--- Starting Widespread Data Loss Diagnostic ---\")\n",
    "\n",
    "# a. åŠ è½½å®Œæ•´çš„èŠ‚ç‚¹æ•°æ®\n",
    "nodes_path = PROCESSED_DATA_DIR / \"nodes.parquet\"\n",
    "if not nodes_path.exists():\n",
    "    raise FileNotFoundError(f\"æœªæ‰¾åˆ°å¤„ç†åçš„èŠ‚ç‚¹æ–‡ä»¶: {nodes_path}\")\n",
    "full_nodes_df = pd.read_parquet(nodes_path)\n",
    "\n",
    "# b. è·å–æ‰€æœ‰æ ·æœ¬IDå¹¶éšæœºæŠ½æ ·\n",
    "all_sample_ids = full_nodes_df['sample_id'].unique().tolist()\n",
    "num_samples_to_check = min(10, len(all_sample_ids)) # Check up to 10 samples\n",
    "selected_samples = random.sample(all_sample_ids, num_samples_to_check)\n",
    "\n",
    "logger.info(f\"Randomly selected {len(selected_samples)} samples for diagnosis: {selected_samples}\")\n",
    "\n",
    "# c. å¾ªç¯è¯Šæ–­\n",
    "diagnostic_results = []\n",
    "for sample_id in tqdm(selected_samples, desc=\"Diagnosing Samples\"):\n",
    "    diagnostic_results.append(\n",
    "        diagnose_sample_data_loss(sample_id, RAW_DATA_DIR, full_nodes_df)\n",
    "    )\n",
    "\n",
    "# d. æ±‡æ€»å¹¶å±•ç¤ºç»“æœ\n",
    "summary_df = pd.DataFrame(diagnostic_results)\n",
    "summary_df['loss_percentage'] = summary_df['loss_percentage'].map('{:.2f}%'.format)\n",
    "\n",
    "# è°ƒæ•´åˆ—é¡ºåºä»¥ä¾¿æŸ¥çœ‹\n",
    "display_cols = ['sample_id', 'diagnosis', 'adata_spots', 'processed_spots', 'loss_percentage', 'details']\n",
    "summary_df = summary_df[display_cols]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                       Widespread Data Loss Diagnostic Summary\")\n",
    "print(\"=\"*80)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "print(summary_df)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# e. æœ€ç»ˆç»“è®º\n",
    "critical_issues = summary_df[summary_df['diagnosis'].isin(['File Not Found', 'Critical'])]\n",
    "if not critical_issues.empty:\n",
    "    logger.error(f\"\\nğŸ”¥ CRITICAL FINDING: {len(critical_issues)} out of {num_samples_to_check} checked samples have significant data loss, likely due to missing tile files.\")\n",
    "    logger.error(\"This indicates a systemic problem in the initial tile generation/saving process. Please review the upstream scripts.\")\n",
    "else:\n",
    "    logger.info(\"\\nâœ… All randomly checked samples appear to be healthy or have minor issues. The problem might be isolated to specific samples like NCBI525.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:39:03,025 - INFO - --- å…¨é¢æ£€æŸ¥ `nodes.parquet` ä¸­çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„ ---\n",
      "2025-11-06 10:39:03,477 - INFO - å°†è¦æ£€æŸ¥ 919173 ä¸ªå”¯ä¸€çš„æ–‡ä»¶è·¯å¾„...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e6d87993f140328917145daa4e9eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "æ­£åœ¨éªŒè¯æ–‡ä»¶è·¯å¾„:   0%|          | 0/919173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:39:55,525 - INFO - âœ… æˆåŠŸï¼æ‰€æœ‰ 919173 ä¸ªæ–‡ä»¶è·¯å¾„å‡æœ‰æ•ˆä¸”æ–‡ä»¶å­˜åœ¨ã€‚\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                       æ–‡ä»¶è·¯å¾„å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "\n",
    "# Check whether all the paths in the nodesdf exist\n",
    "logger.info(\"--- å…¨é¢æ£€æŸ¥ `nodes.parquet` ä¸­çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„ ---\")\n",
    "\n",
    "# ç¡®ä¿ full_nodes_df å·²ä»å‰ä¸€ä¸ªå•å…ƒæ ¼åŠ è½½\n",
    "if 'full_nodes_df' not in locals():\n",
    "    logger.error(\"âŒ `full_nodes_df` æœªå®šä¹‰ã€‚è¯·å…ˆè¿è¡Œä¸Šé¢çš„å•å…ƒæ ¼ã€‚\")\n",
    "else:\n",
    "    # è·å–æ‰€æœ‰å”¯ä¸€çš„ã€éç©ºçš„å›¾åƒè·¯å¾„\n",
    "    image_paths_to_check = full_nodes_df['image_path'].dropna().unique()\n",
    "    total_paths = len(image_paths_to_check)\n",
    "    logger.info(f\"å°†è¦æ£€æŸ¥ {total_paths} ä¸ªå”¯ä¸€çš„æ–‡ä»¶è·¯å¾„...\")\n",
    "\n",
    "    def check_path_exists(path_str):\n",
    "        \"\"\"æ£€æŸ¥å•ä¸ªæ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›è·¯å¾„å­—ç¬¦ä¸²ã€‚\"\"\"\n",
    "        absolute_path = PROJECT_ROOT / path_str\n",
    "        if not absolute_path.exists():\n",
    "            return path_str\n",
    "        return None\n",
    "\n",
    "    missing_paths = []\n",
    "    # ä½¿ç”¨ ThreadPoolExecutor å¹¶è¡Œæ£€æŸ¥æ–‡ä»¶\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count() or 4) as executor:\n",
    "        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "        results = list(tqdm(executor.map(check_path_exists, image_paths_to_check), total=total_paths, desc=\"æ­£åœ¨éªŒè¯æ–‡ä»¶è·¯å¾„\"))\n",
    "    \n",
    "    # ä»ç»“æœä¸­è¿‡æ»¤å‡ºæ‰€æœ‰é None çš„é¡¹ï¼ˆå³ç¼ºå¤±çš„è·¯å¾„ï¼‰\n",
    "    missing_paths = [path for path in results if path is not None]\n",
    "\n",
    "    # --- æŠ¥å‘Šç»“æœ ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"                       æ–‡ä»¶è·¯å¾„å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š\")\n",
    "    print(\"=\"*80)\n",
    "    if not missing_paths:\n",
    "        logger.info(f\"âœ… æˆåŠŸï¼æ‰€æœ‰ {total_paths} ä¸ªæ–‡ä»¶è·¯å¾„å‡æœ‰æ•ˆä¸”æ–‡ä»¶å­˜åœ¨ã€‚\")\n",
    "    else:\n",
    "        num_missing = len(missing_paths)\n",
    "        logger.error(f\"ğŸ”¥ å¤±è´¥ï¼åœ¨ {total_paths} ä¸ªè·¯å¾„ä¸­å‘ç° {num_missing} ä¸ªç¼ºå¤±æ–‡ä»¶ã€‚\")\n",
    "        logger.error(\"è¿™è¡¨æ˜åœ¨æ•°æ®å¤„ç†çš„æŸä¸ªé˜¶æ®µï¼Œç“¦ç‰‡æ–‡ä»¶ä¸¢å¤±æˆ–è·¯å¾„è®°å½•é”™è¯¯ã€‚\")\n",
    "        \n",
    "        # æ‰“å°ä¸€äº›ç¼ºå¤±æ–‡ä»¶çš„ä¾‹å­\n",
    "        print(\"\\nç¼ºå¤±æ–‡ä»¶ç¤ºä¾‹ (æœ€å¤šæ˜¾ç¤º10ä¸ª):\")\n",
    "        for p in missing_paths[:10]:\n",
    "            print(f\"  - {p}\")\n",
    "        if num_missing > 10:\n",
    "            print(f\"  ... ä»¥åŠå…¶ä»– {num_missing - 10} ä¸ªæ–‡ä»¶ã€‚\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
