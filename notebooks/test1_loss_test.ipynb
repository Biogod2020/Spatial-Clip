{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9519a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'open_clip_torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip_torch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'open_clip_torch'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692108a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nodes: /cwStorage/nodecw_group/jijh/yuanspace_data/artifacts/nodes.parquet\n",
      "Loading edges: /cwStorage/nodecw_group/jijh/yuanspace_data/artifacts/edges.parquet\n",
      "[Check1] Checked 500 rows. mean=1.00000000, std=0.00000000, min=1.00000000, max=1.00000000\n",
      "[Check1] ✅ All sampled rows sum to 1 within tolerance.\n"
     ]
    }
   ],
   "source": [
    "# --- Check 1: Row-sum == 1 (main positive + neighbors, row-normalized) ---\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === 配置 ===\n",
    "ARTIFACTS_DIR = Path(\"/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts\")\n",
    "NODES_PATH = ARTIFACTS_DIR / \"nodes.parquet\"\n",
    "EDGES_PATH = ARTIFACTS_DIR / \"edges.parquet\"\n",
    "\n",
    "# 抽样的 anchor 数量（建议 200~1000 视内存）\n",
    "NUM_ANCHORS_TO_CHECK = 500\n",
    "\n",
    "# “主正样本权重=1 + 邻居 alpha 后归一化”的默认策略\n",
    "# 如果你在训练里使用了“锐化/温度”处理（例如 alpha^gamma 或主正样本权重 τ），可以在这里同步设置：\n",
    "USE_GAMMA = False\n",
    "GAMMA = 1.0      # 若 USE_GAMMA=True，则对 alpha 使用 alpha ** GAMMA\n",
    "USE_TAU = False\n",
    "TAU = 1.0        # 若 USE_TAU=True，则主正样本权重设为 TAU（替代 1）\n",
    "\n",
    "print(f\"Loading nodes: {NODES_PATH}\")\n",
    "nodes = pd.read_parquet(NODES_PATH, columns=[\"tile_id\"])\n",
    "tile_ids = nodes[\"tile_id\"].to_numpy()\n",
    "\n",
    "print(f\"Loading edges: {EDGES_PATH}\")\n",
    "edges = pd.read_parquet(EDGES_PATH, columns=[\"src_tile_id\", \"nbr_tile_id\", \"alpha\"])\n",
    "\n",
    "# 为了加速随机抽样，这里只用存在出边的 src 集合\n",
    "src_unique = edges[\"src_tile_id\"].unique()\n",
    "rng = np.random.default_rng(2025)\n",
    "sampled_src = rng.choice(src_unique, size=min(NUM_ANCHORS_TO_CHECK, len(src_unique)), replace=False)\n",
    "\n",
    "row_sums = []\n",
    "bad_rows = []\n",
    "\n",
    "# 将边按 src 分组，便于快速检索\n",
    "grp = edges.groupby(\"src_tile_id\", sort=False)\n",
    "\n",
    "for src in sampled_src:\n",
    "    if src not in grp.groups:\n",
    "        # 该 src 没有邻居边（极少数情况），直接跳过\n",
    "        continue\n",
    "    sub = grp.get_group(src)\n",
    "\n",
    "    # 邻居 alpha\n",
    "    alpha = sub[\"alpha\"].to_numpy(dtype=np.float64)\n",
    "\n",
    "    # 可选：对 alpha 做幂次锐化\n",
    "    if USE_GAMMA and GAMMA != 1.0:\n",
    "        alpha = np.power(alpha, GAMMA)\n",
    "\n",
    "    # 主正样本权重\n",
    "    main_w = TAU if USE_TAU else 1.0\n",
    "\n",
    "    total = main_w + alpha.sum()\n",
    "    if total <= 0:\n",
    "        # 不应出现；防御性处理\n",
    "        rs = np.nan\n",
    "    else:\n",
    "        # 归一化后的行和应为 1\n",
    "        rs = (main_w / total) + (alpha / total).sum()\n",
    "\n",
    "    row_sums.append(rs)\n",
    "    if not np.isfinite(rs) or abs(rs - 1.0) > 1e-6:\n",
    "        bad_rows.append((src, rs))\n",
    "\n",
    "row_sums = np.array(row_sums, dtype=np.float64)\n",
    "print(f\"[Check1] Checked {len(row_sums)} rows. mean={row_sums.mean():.8f}, \"\n",
    "      f\"std={row_sums.std():.8f}, min={row_sums.min():.8f}, max={row_sums.max():.8f}\")\n",
    "if bad_rows:\n",
    "    print(f\"[Check1][WARN] Found {len(bad_rows)} rows with |sum-1|>1e-6. Show first 5:\")\n",
    "    for a,b in bad_rows[:5]:\n",
    "        print(f\"  src_tile_id={a}, row_sum={b}\")\n",
    "else:\n",
    "    print(\"[Check1] ✅ All sampled rows sum to 1 within tolerance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921a6d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check2] ✅ Mapping assertion passed for all tested ranks.\n"
     ]
    }
   ],
   "source": [
    "# --- Check 2: Mapping assertion for main positives (simulated) ---\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ARTIFACTS_DIR = Path(\"/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts\")\n",
    "NODES_PATH = ARTIFACTS_DIR / \"nodes.parquet\"\n",
    "\n",
    "# === 模拟分布式设置 ===\n",
    "WORLD_SIZE = 3\n",
    "N_LOCAL = 16      # 每个 rank 的本地 batch 大小（用于模拟），可改小一点以便可视化\n",
    "RANKS_TO_TEST = [0, 1, 2]  # 要测试的 rank 集合\n",
    "\n",
    "nodes = pd.read_parquet(NODES_PATH, columns=[\"tile_id\"])\n",
    "tile_ids = nodes[\"tile_id\"].to_numpy()\n",
    "assert len(tile_ids) >= WORLD_SIZE * N_LOCAL, \"数据量不足以构造模拟批次，请减小 N_LOCAL 或 WORLD_SIZE\"\n",
    "\n",
    "# 简单的“轮转分配”方式模拟 DistributedSampler（真实实现有 shuffle，但索引公式一致）\n",
    "# 我们取前 WORLD_SIZE * N_LOCAL 个样本，切成 WORLD_SIZE 片，每片 N_LOCAL 大小\n",
    "local_batches = []\n",
    "for r in range(WORLD_SIZE):\n",
    "    start = r * N_LOCAL\n",
    "    end = (r + 1) * N_LOCAL\n",
    "    local_img_ids = tile_ids[start:end].copy()  # 作为 image_tile_ids\n",
    "    local_txt_ids = tile_ids[start:end].copy()  # 作为 text_tile_ids（对称）\n",
    "    local_batches.append((local_img_ids, local_txt_ids))\n",
    "\n",
    "# 模拟 all_gather 后的“全局列域”\n",
    "global_tile_ids = np.concatenate([b[1] for b in local_batches], axis=0)  # 用 text 端决定列域\n",
    "txt_id_to_idx = {int(tid): idx for idx, tid in enumerate(global_tile_ids)}\n",
    "\n",
    "# 对每个 rank 做断言：txt_id_to_idx[image_tile_ids[i]] == rank*N_LOCAL + i\n",
    "bad = []\n",
    "for rank in RANKS_TO_TEST:\n",
    "    image_tile_ids, _ = local_batches[rank]\n",
    "    for i in range(N_LOCAL):\n",
    "        tid = int(image_tile_ids[i])\n",
    "        col = txt_id_to_idx.get(tid, None)\n",
    "        expect = rank * N_LOCAL + i\n",
    "        if col != expect:\n",
    "            bad.append((rank, i, tid, col, expect))\n",
    "\n",
    "if bad:\n",
    "    print(f\"[Check2][FAIL] Found {len(bad)} mismatches. Show first 10:\")\n",
    "    for r,i,tid,col,exp in bad[:10]:\n",
    "        print(f\"  rank={r}, i={i}, tile_id={tid}, mapped_col={col}, expected={exp}\")\n",
    "else:\n",
    "    print(\"[Check2] ✅ Mapping assertion passed for all tested ranks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76831e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check3] Anchor tile_id=3\n",
      "  pos_col (should be rank*N_LOCAL + local_index): 3\n",
      "  neighbors in-batch = 0 / total neighbors = 6\n",
      "  (Optional) main+in-batch-neighbors sum before normalization = 1.000000 (应 < 1+Σ所有邻居；仅用于辅助理解)\n"
     ]
    }
   ],
   "source": [
    "# --- Check 3: Print pos_col and neighbor_cols (simulated batch domain) ---\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ARTIFACTS_DIR = Path(\"/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts\")\n",
    "NODES_PATH = ARTIFACTS_DIR / \"nodes.parquet\"\n",
    "EDGES_PATH = ARTIFACTS_DIR / \"edges.parquet\"\n",
    "\n",
    "WORLD_SIZE = 3\n",
    "N_LOCAL = 16      # 与检查 2 相同，保持一致\n",
    "RANK = 0          # 选择一个 rank 来展示\n",
    "ANCHOR_INDEX_IN_LOCAL = 3  # 选择该 rank 的第几个样本作为 anchor\n",
    "\n",
    "# 1) 读取数据\n",
    "nodes = pd.read_parquet(NODES_PATH, columns=[\"tile_id\"])\n",
    "edges = pd.read_parquet(EDGES_PATH, columns=[\"src_tile_id\", \"nbr_tile_id\", \"distance\", \"alpha\"])\n",
    "tile_ids = nodes[\"tile_id\"].to_numpy()\n",
    "assert len(tile_ids) >= WORLD_SIZE * N_LOCAL\n",
    "\n",
    "# 2) 构造模拟批次与列域（与检查 2 相同）\n",
    "local_batches = []\n",
    "for r in range(WORLD_SIZE):\n",
    "    start = r * N_LOCAL\n",
    "    end = (r + 1) * N_LOCAL\n",
    "    local_img_ids = tile_ids[start:end].copy()\n",
    "    local_txt_ids = tile_ids[start:end].copy()\n",
    "    local_batches.append((local_img_ids, local_txt_ids))\n",
    "\n",
    "global_tile_ids = np.concatenate([b[1] for b in local_batches], axis=0)  # 列域由 text 端确定\n",
    "txt_id_to_idx = {int(tid): idx for idx, tid in enumerate(global_tile_ids)}\n",
    "\n",
    "# 3) 选择 anchor（取 image 端的第 ANCHOR_INDEX_IN_LOCAL 个）\n",
    "image_tile_ids, text_tile_ids = local_batches[RANK]\n",
    "anchor_tile_id = int(image_tile_ids[ANCHOR_INDEX_IN_LOCAL])\n",
    "\n",
    "# 4) 取该 anchor 的邻居（按距离从近到远）\n",
    "nbr_df = edges[edges[\"src_tile_id\"] == anchor_tile_id].sort_values(\"distance\").copy()\n",
    "# 只保留“本批次列域里存在”的邻居\n",
    "in_batch_mask = nbr_df[\"nbr_tile_id\"].isin(global_tile_ids)\n",
    "nbr_in = nbr_df[in_batch_mask]\n",
    "\n",
    "pos_col = txt_id_to_idx.get(anchor_tile_id, None)\n",
    "neighbor_cols = [txt_id_to_idx.get(int(t), None) for t in nbr_in[\"nbr_tile_id\"].tolist()]\n",
    "\n",
    "print(f\"[Check3] Anchor tile_id={anchor_tile_id}\")\n",
    "print(f\"  pos_col (should be rank*N_LOCAL + local_index): {pos_col}\")\n",
    "print(f\"  neighbors in-batch = {len(nbr_in)} / total neighbors = {len(nbr_df)}\")\n",
    "\n",
    "# 打印前若干邻居\n",
    "MAX_SHOW = 12\n",
    "for idx, row in nbr_in.head(MAX_SHOW).iterrows():\n",
    "    t = int(row[\"nbr_tile_id\"])\n",
    "    col = txt_id_to_idx.get(t, None)\n",
    "    dist = float(row[\"distance\"])\n",
    "    alpha = float(row[\"alpha\"])\n",
    "    print(f\"    nbr_tile_id={t:>10d} | col={col:>5} | dist={dist:8.3f} | alpha={alpha:6.3f}\")\n",
    "\n",
    "# 5) 可选：计算“主正样本+邻居（仅批内）”的归一化行和，辅助人工核验\n",
    "main_w = 1.0\n",
    "alpha_vec = nbr_in[\"alpha\"].to_numpy(dtype=float)\n",
    "row_sum = (main_w + alpha_vec.sum())\n",
    "print(f\"  (Optional) main+in-batch-neighbors sum before normalization = {row_sum:.6f} \"\n",
    "      f\"(应 < 1+Σ所有邻居；仅用于辅助理解)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b614a",
   "metadata": {},
   "source": [
    "# 测试改正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4af1d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 在 Notebook 里运行：为你的 Dataset 增补索引加速结构 ====\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def dataset_build_fast_indices(dataset, k_neighbors: int = 6):\n",
    "    \"\"\"\n",
    "    为 SpatiallyAwareDataset 构建快速索引与向量化邻居矩阵。\n",
    "    需要 dataset 拥有:\n",
    "      - dataset.tile_ids: np.ndarray shape [N]\n",
    "      - dataset.sample_ids: np.ndarray shape [N] 或等价的列表\n",
    "      - dataset.edges_map: dict[int, list[int]]  # src_tile_id -> nbr_tile_id(长度<=k)\n",
    "    生成:\n",
    "      - dataset.id2idx: dict[tile_id] -> dataset index\n",
    "      - dataset.sample_to_indices: dict[sample_id] -> np.ndarray of indices\n",
    "      - dataset.nbr_index: np.ndarray [N, k_neighbors]，元素为索引，缺失填 -1\n",
    "    \"\"\"\n",
    "    assert hasattr(dataset, \"tile_ids\") and hasattr(dataset, \"sample_ids\"), \"Dataset 缺少 tile_ids 或 sample_ids\"\n",
    "    assert hasattr(dataset, \"edges_map\"), \"Dataset 需要有 edges_map (src_tile_id -> [nbr_tile_id...])\"\n",
    "    tile_ids = np.asarray(dataset.tile_ids)\n",
    "    sample_ids = np.asarray(dataset.sample_ids)\n",
    "\n",
    "    N = tile_ids.shape[0]\n",
    "    id2idx = {int(t): int(i) for i, t in enumerate(tile_ids)}\n",
    "    dataset.id2idx = id2idx\n",
    "\n",
    "    # 每个样本的索引列表（向量化）\n",
    "    sample_to_indices = defaultdict(list)\n",
    "    for i, sid in enumerate(sample_ids):\n",
    "        sample_to_indices[sid].append(i)\n",
    "    dataset.sample_to_indices = {sid: np.asarray(idxs, dtype=np.int64) for sid, idxs in sample_to_indices.items()}\n",
    "\n",
    "    # 邻居“索引矩阵” [N, K]，向量化映射 tile_id->index\n",
    "    K = int(k_neighbors)\n",
    "    nbr_index = np.full((N, K), -1, dtype=np.int64)\n",
    "    for i in range(N):\n",
    "        src_tid = int(tile_ids[i])\n",
    "        nbr_tids = dataset.edges_map.get(src_tid, [])\n",
    "        # 只取前K个\n",
    "        if len(nbr_tids) > K:\n",
    "            nbr_tids = nbr_tids[:K]\n",
    "        # 映射为索引，映射不到的置 -1\n",
    "        mapped = [id2idx.get(int(t), -1) for t in nbr_tids]\n",
    "        if mapped:\n",
    "            nbr_index[i, :len(mapped)] = np.asarray(mapped, dtype=np.int64)\n",
    "    dataset.nbr_index = nbr_index\n",
    "    print(f\"[build_fast_indices] N={N}, K={K}; sample buckets={len(dataset.sample_to_indices)}; done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801abda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 在 Notebook 里运行：邻域感知批采样器 ====\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SpatialBucketBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    邻域感知 BatchSampler：\n",
    "    - 每个 batch 由若干“中心锚点” + 这些锚点的邻居（也作为锚点）组成；\n",
    "    - 只产出索引列表（不改 DataLoader 的 __getitem__ 行为）→ IO 不增加；\n",
    "    - DDP 下按 sample_id 分桶给 rank（hash 分配），各 rank 扫描不同样本集合；\n",
    "    - 保证每个 rank 的 batch 数等长（循环补齐或丢弃多余）。\n",
    "\n",
    "    依赖 dataset 成员：\n",
    "      dataset.sample_to_indices: dict[sample_id] -> np.ndarray of indices\n",
    "      dataset.nbr_index: np.ndarray[N, K] 邻居索引矩阵（-1 表示无效）\n",
    "      dataset.sample_ids: np.ndarray[N]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size: int,\n",
    "        world_size: int = 1,\n",
    "        rank: int = 0,\n",
    "        centers_per_batch: int = 16,\n",
    "        max_neighbors_per_center: int = 4,\n",
    "        same_sample_only: bool = True,\n",
    "        drop_last: bool = True,\n",
    "        seed: int = 2025\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.world_size = int(world_size)\n",
    "        self.rank = int(rank)\n",
    "        self.centers_per_batch = int(centers_per_batch)\n",
    "        self.max_neighbors_per_center = int(max_neighbors_per_center)\n",
    "        self.same_sample_only = bool(same_sample_only)\n",
    "        self.drop_last = bool(drop_last)\n",
    "        self.seed = int(seed)\n",
    "\n",
    "        assert hasattr(dataset, \"sample_to_indices\") and hasattr(dataset, \"nbr_index\"), \\\n",
    "            \"请先调用 dataset_build_fast_indices(dataset) 生成 sample_to_indices / nbr_index\"\n",
    "        self.N = len(dataset.sample_ids)\n",
    "        self.sample_ids = np.asarray(dataset.sample_ids)\n",
    "\n",
    "        # 1) 将样本桶按哈希分配给各 rank，避免 DDP 重叠\n",
    "        all_samples = list(dataset.sample_to_indices.keys())\n",
    "        all_samples.sort(key=lambda x: str(x))  # 稳定顺序\n",
    "        self.assigned_samples = [s for s in all_samples if (hash(str(s)) % self.world_size) == self.rank]\n",
    "        assert len(self.assigned_samples) > 0, \"本 rank 被分配的样本桶为空，请检查数据或 world_size 设置。\"\n",
    "\n",
    "        # 2) 计算每个 epoch 的批次数，保证各 rank 步数一致\n",
    "        #    用全局 N 估算：每个 rank 生成 floor(N / (B * world_size)) 个 batch\n",
    "        self.batches_per_epoch = self._compute_batches_per_epoch()\n",
    "\n",
    "        # 3) 为每个样本桶准备一个“游标指针”与乱序索引\n",
    "        self._rng = random.Random(self.seed)\n",
    "        self._epoch = 0\n",
    "        self._reset_per_sample_state()\n",
    "\n",
    "    def _compute_batches_per_epoch(self):\n",
    "        # 按总样本量与 batch_size/world_size 估算一个“全局统一”的步数\n",
    "        est = self.N // (self.batch_size * self.world_size)\n",
    "        return max(1, int(est))\n",
    "\n",
    "    def _reset_per_sample_state(self):\n",
    "        self._per_sample_order = {}\n",
    "        self._per_sample_ptr = {}\n",
    "        for s in self.assigned_samples:\n",
    "            idxs = self.dataset.sample_to_indices[s]\n",
    "            # 打乱\n",
    "            order = idxs.copy()\n",
    "            self._rng.shuffle(order.tolist())  # 原地洗牌（list 更快）\n",
    "            self._per_sample_order[s] = order\n",
    "            self._per_sample_ptr[s] = 0\n",
    "\n",
    "        # 给样本桶一个稳定但洗牌后的遍历顺序\n",
    "        self._sample_cycle = self.assigned_samples.copy()\n",
    "        self._rng.shuffle(self._sample_cycle)\n",
    "\n",
    "    def set_epoch(self, epoch: int):\n",
    "        self._epoch = int(epoch)\n",
    "        # 以 epoch 为种子扰动，确保每个 epoch 的顺序不同但各 rank 一致可复现\n",
    "        self._rng.seed(self.seed + self._epoch)\n",
    "        self._reset_per_sample_state()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = self._rng\n",
    "        ds = self.dataset\n",
    "        nbr_index = ds.nbr_index\n",
    "        sample_ids = self.sample_ids\n",
    "\n",
    "        # round-robin 扫描样本桶，产生固定数量的 batch\n",
    "        num_yield = 0\n",
    "        sample_cursor = 0\n",
    "        while num_yield < self.batches_per_epoch:\n",
    "            s = self._sample_cycle[sample_cursor]\n",
    "            sample_cursor = (sample_cursor + 1) % len(self._sample_cycle)\n",
    "\n",
    "            order = self._per_sample_order[s]\n",
    "            ptr = self._per_sample_ptr[s]\n",
    "            if ptr >= len(order):\n",
    "                # 该桶耗尽，重置该桶（循环使用），以避免不同 rank 步数不一致\n",
    "                self._rng.shuffle(order.tolist())\n",
    "                self._per_sample_ptr[s] = 0\n",
    "                ptr = 0\n",
    "\n",
    "            batch = []\n",
    "            used = set()\n",
    "\n",
    "            # 1) 选中心锚点\n",
    "            centers = []\n",
    "            centers_take = min(self.centers_per_batch, len(order) - ptr)\n",
    "            if centers_take <= 0:\n",
    "                continue\n",
    "            centers = order[ptr:ptr + centers_take]\n",
    "            self._per_sample_ptr[s] += centers_take\n",
    "\n",
    "            # 2) 将中心加入 batch\n",
    "            for c in centers:\n",
    "                if len(batch) >= self.batch_size:\n",
    "                    break\n",
    "                if c in used:\n",
    "                    continue\n",
    "                batch.append(int(c))\n",
    "                used.add(int(c))\n",
    "\n",
    "                # 3) 拉取该中心的邻居索引，限制同样本、去重，最多取 max_neighbors_per_center\n",
    "                nbrs = nbr_index[int(c)]\n",
    "                # 过滤无效\n",
    "                nbrs = nbrs[nbrs >= 0]\n",
    "                if self.same_sample_only:\n",
    "                    nbrs = nbrs[sample_ids[nbrs] == s]\n",
    "                # 去重自己/已选\n",
    "                if len(nbrs) > 0:\n",
    "                    # 打乱邻居次序，避免总是取前 K 个\n",
    "                    nbrs = nbrs.copy()\n",
    "                    rng.shuffle(nbrs.tolist())\n",
    "                    for nb in nbrs:\n",
    "                        if len(batch) >= self.batch_size:\n",
    "                            break\n",
    "                        nb = int(nb)\n",
    "                        if nb in used:\n",
    "                            continue\n",
    "                        batch.append(nb)\n",
    "                        used.add(nb)\n",
    "                        if len(batch) >= self.batch_size or \\\n",
    "                           (len(batch) - len(centers)) >= self.max_neighbors_per_center * len(centers):\n",
    "                            break  # 控制邻居预算\n",
    "\n",
    "            # 4) 若 batch 未满，继续从同样本补齐随机样本（不保证是邻居）\n",
    "            if len(batch) < self.batch_size:\n",
    "                remain = self.batch_size - len(batch)\n",
    "                # 从该桶剩余索引中补\n",
    "                ptr2 = self._per_sample_ptr[s]\n",
    "                if ptr2 + remain <= len(order):\n",
    "                    fill = order[ptr2:ptr2 + remain]\n",
    "                    self._per_sample_ptr[s] += remain\n",
    "                    for f in fill:\n",
    "                        fi = int(f)\n",
    "                        if fi in used:\n",
    "                            continue\n",
    "                        batch.append(fi)\n",
    "                        used.add(fi)\n",
    "                else:\n",
    "                    # 不够就循环补\n",
    "                    take = 0\n",
    "                    while len(batch) < self.batch_size:\n",
    "                        if self._per_sample_ptr[s] >= len(order):\n",
    "                            self._rng.shuffle(order.tolist())\n",
    "                            self._per_sample_ptr[s] = 0\n",
    "                        fi = int(order[self._per_sample_ptr[s]])\n",
    "                        self._per_sample_ptr[s] += 1\n",
    "                        if fi in used:\n",
    "                            continue\n",
    "                        batch.append(fi)\n",
    "                        used.add(fi)\n",
    "\n",
    "            # 5) 产出\n",
    "            if self.drop_last and len(batch) < self.batch_size:\n",
    "                continue\n",
    "            num_yield += 1\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c451de6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip_train\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpatiallyAwareDataset\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1) 准备 dataset（你已有）\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dataset = \u001b[43mSpatiallyAwareDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_neighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 假定 dataset 已经有 tile_ids / sample_ids / edges_map\u001b[39;00m\n\u001b[32m      9\u001b[39m dataset_build_fast_indices(dataset, k_neighbors=\u001b[32m6\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/public/home/jijh/diffusion_project/git_repo/yuanspace/src/open_clip_train/spatial_data.py:25\u001b[39m, in \u001b[36mSpatiallyAwareDataset.__init__\u001b[39m\u001b[34m(self, artifacts_dir, k_neighbors)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.k = k_neighbors\n\u001b[32m     23\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitializing SpatiallyAwareDataset with k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk_neighbors\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m nodes_path = \u001b[43martifacts_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnodes.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     26\u001b[39m edges_path = artifacts_dir / \u001b[33m\"\u001b[39m\u001b[33medges.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m nodes_path.exists() \u001b[38;5;129;01mand\u001b[39;00m edges_path.exists(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNodes or Edges file not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifacts_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# ==== 在 Notebook 里运行示例（或粘贴到你的 train 脚本里） ====\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "from open_clip_train.spatial_data import SpatiallyAwareDataset\n",
    "\n",
    "# 1) 准备 dataset（你已有）\n",
    "dataset = SpatiallyAwareDataset(artifacts_dir=\"/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts\", k_neighbors=6)\n",
    "# 假定 dataset 已经有 tile_ids / sample_ids / edges_map\n",
    "dataset_build_fast_indices(dataset, k_neighbors=6)\n",
    "\n",
    "# 2) 构造 batch_sampler（DDP 环境下）\n",
    "if dist.is_available() and dist.is_initialized():\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank()\n",
    "else:\n",
    "    world_size = 1\n",
    "    rank = 0\n",
    "\n",
    "batch_sampler = SpatialBucketBatchSampler(\n",
    "    dataset=dataset,\n",
    "    batch_size=1600,\n",
    "    world_size=world_size,\n",
    "    rank=rank,\n",
    "    centers_per_batch=16,           # 可调：中心锚点个数\n",
    "    max_neighbors_per_center=4,     # 可调：每个中心带的邻居上限\n",
    "    same_sample_only=True,\n",
    "    drop_last=True,\n",
    "    seed=2025,\n",
    ")\n",
    "\n",
    "# 3) DataLoader（注意：不要再传 batch_size / shuffle / sampler）\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=batch_sampler,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    "    collate_fn=getattr(dataset, \"collate_fn\", None)  # 若你有自定义 collate_fn 就传入\n",
    ")\n",
    "\n",
    "# 4) 训练循环中，每个 epoch 开始前设置 epoch（确保可复现洗牌）\n",
    "for epoch in range(num_epochs):\n",
    "    if hasattr(batch_sampler, \"set_epoch\"):\n",
    "        batch_sampler.set_epoch(epoch)\n",
    "    # for step, batch in enumerate(loader):\n",
    "    #     ... 你的训练逻辑 ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85927285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spatial_clip)",
   "language": "python",
   "name": "spatial_clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
