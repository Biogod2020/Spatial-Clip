{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb022bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# --- æ ¸å¿ƒé…ç½® ---\n",
    "# ğŸ›¡ï¸ å®‰å…¨ç¬¬ä¸€: è®¾ä¸º False ä»¥å®é™…æ‰§è¡Œæ–‡ä»¶å†™å…¥\n",
    "DRYRUN = False\n",
    "\n",
    "# --- è·¯å¾„é…ç½® ---\n",
    "# è¾“å…¥: åŒ…å«é¢„å¤„ç†åæ•°æ®çš„ç›®å½•\n",
    "INPUT_ARTIFACTS_DIR = Path(\"/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts\")\n",
    "# è¾“å‡º: å°†å­˜æ”¾ train/ å’Œ val/ å­ç›®å½•çš„åŸºå‡†ç›®å½•\n",
    "OUTPUT_SPLIT_DIR = Path(\"/cwStorage/nodecw_group/jijh/yuanspace_data/dataset_split\")\n",
    "\n",
    "# å®šä¹‰åŸå§‹æ¸…å•æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè·å– sample_id\n",
    "ORIGINAL_MANIFEST_FILE = Path(\"/cwStorage/nodecw_group/jijh/hest_1k/HEST_v1_1_0.csv\") \n",
    "\n",
    "# æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "assert INPUT_ARTIFACTS_DIR.exists(), f\"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {INPUT_ARTIFACTS_DIR}\"\n",
    "assert ORIGINAL_MANIFEST_FILE.exists(), f\"åŸå§‹æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {ORIGINAL_MANIFEST_FILE}\"\n",
    "for filename in [\"nodes.parquet\", \"edges.parquet\", \"image_embeds.npy\", \"text_embeds.npy\"]:\n",
    "    assert (INPUT_ARTIFACTS_DIR / filename).exists(), f\"è¾“å…¥æ–‡ä»¶ç¼ºå¤±: {filename}\"\n",
    "\n",
    "# --- æ—¥å¿—é…ç½® ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    stream=sys.stdout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a686927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_sample_ids(manifest_path: Path) -> Set[str]:\n",
    "    \"\"\"\n",
    "    æ ¹æ®é¢„å®šä¹‰çš„ç ”ç©¶é¡¹ç›®æ ‡é¢˜ï¼Œä»åŸå§‹æ¸…å•ä¸­è·å–éªŒè¯é›†çš„ sample_id åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    df_manifest = pd.read_csv(manifest_path)\n",
    "    \n",
    "    VALIDATION_TITLES = [\n",
    "        \"Spatial deconvolution of HER2-positive breast cancer delineates tumor-associated cell type interactions\",\n",
    "        \"Characterization of immune cell populations in the tumor microenvironment of colorectal cancer using high definition spatial profiling\"\n",
    "    ]\n",
    "    \n",
    "    validation_ids = df_manifest[df_manifest['dataset_title'].isin(VALIDATION_TITLES)]['id'].unique()\n",
    "    \n",
    "    logging.info(f\"ä»æ¸…å•ä¸­ç¡®å®šäº† {len(validation_ids)} ä¸ªç”¨äºéªŒè¯çš„ç‹¬ç«‹æ ·æœ¬ IDã€‚\")\n",
    "    return set(validation_ids)\n",
    "\n",
    "def split_and_save_dataset(\n",
    "    input_dir: Path, \n",
    "    output_dir: Path, \n",
    "    validation_sample_ids: Set[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    åŠ è½½é¢„å¤„ç†çš„æ•°æ®ï¼ŒæŒ‰ sample_id æ‹†åˆ†ä¸ºè®­ç»ƒ/éªŒè¯é›†ï¼Œå¹¶ä¿å­˜åˆ°æ–°ç›®å½•ã€‚\n",
    "    \"\"\"\n",
    "    logging.info(f\"å¼€å§‹ä» '{input_dir}' åŠ è½½æ•°æ®...\")\n",
    "    nodes_df = pd.read_parquet(input_dir / \"nodes.parquet\")\n",
    "    edges_df = pd.read_parquet(input_dir / \"edges.parquet\")\n",
    "    all_img_embeds = np.load(input_dir / \"image_embeds.npy\")\n",
    "    all_txt_embeds = np.load(input_dir / \"text_embeds.npy\")\n",
    "    logging.info(\"æ‰€æœ‰è¾“å…¥æ–‡ä»¶åŠ è½½å®Œæ¯•ã€‚\")\n",
    "\n",
    "    # 1. æ‹†åˆ† Nodes å’Œ Embeddings\n",
    "    val_nodes_mask = nodes_df['sample_id'].isin(validation_sample_ids)\n",
    "    \n",
    "    train_nodes_df = nodes_df[~val_nodes_mask].copy()\n",
    "    val_nodes_df = nodes_df[val_nodes_mask].copy()\n",
    "    \n",
    "    # ä½¿ç”¨åŸå§‹ç´¢å¼•æ¥åˆ‡ç‰‡ numpy æ•°ç»„\n",
    "    train_indices = train_nodes_df.index.to_numpy()\n",
    "    val_indices = val_nodes_df.index.to_numpy()\n",
    "    \n",
    "    train_img_embeds = all_img_embeds[train_indices]\n",
    "    val_img_embeds = all_img_embeds[val_indices]\n",
    "    \n",
    "    train_txt_embeds = all_txt_embeds[train_indices]\n",
    "    val_txt_embeds = all_txt_embeds[val_indices]\n",
    "    \n",
    "    # 2. æ‹†åˆ† Edges\n",
    "    train_tile_ids = set(train_nodes_df['tile_id'])\n",
    "    val_tile_ids = set(val_nodes_df['tile_id'])\n",
    "    \n",
    "    train_edges_mask = edges_df['src_tile_id'].isin(train_tile_ids) & edges_df['nbr_tile_id'].isin(train_tile_ids)\n",
    "    val_edges_mask = edges_df['src_tile_id'].isin(val_tile_ids) & edges_df['nbr_tile_id'].isin(val_tile_ids)\n",
    "    \n",
    "    train_edges_df = edges_df[train_edges_mask].copy()\n",
    "    val_edges_df = edges_df[val_edges_mask].copy()\n",
    "\n",
    "    # 3. æŠ¥å‘Šç»Ÿè®¡æ•°æ®\n",
    "    logging.info(\"--- æ‹†åˆ†ç»Ÿè®¡ ---\")\n",
    "    logging.info(f\"è®­ç»ƒé›†: {len(train_nodes_df)} tiles, {len(train_edges_df)} edges\")\n",
    "    logging.info(f\"éªŒè¯é›†: {len(val_nodes_df)} tiles, {len(val_edges_df)} edges\")\n",
    "    logging.info(\"--------------------\")\n",
    "\n",
    "    # 4. å®‰å…¨å†™å…¥æ–‡ä»¶\n",
    "    if not DRYRUN:\n",
    "        train_dir = output_dir / \"train\"\n",
    "        val_dir = output_dir / \"val\"\n",
    "        train_dir.mkdir(parents=True, exist_ok=True)\n",
    "        val_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logging.info(f\"æ­£åœ¨å‘ '{train_dir}' å†™å…¥è®­ç»ƒé›†æ–‡ä»¶...\")\n",
    "        pq.write_table(pa.Table.from_pandas(train_nodes_df, preserve_index=False), train_dir / \"nodes.parquet\")\n",
    "        pq.write_table(pa.Table.from_pandas(train_edges_df, preserve_index=False), train_dir / \"edges.parquet\")\n",
    "        np.save(train_dir / \"image_embeds.npy\", train_img_embeds)\n",
    "        np.save(train_dir / \"text_embeds.npy\", train_txt_embeds)\n",
    "        \n",
    "        logging.info(f\"æ­£åœ¨å‘ '{val_dir}' å†™å…¥éªŒè¯é›†æ–‡ä»¶...\")\n",
    "        pq.write_table(pa.Table.from_pandas(val_nodes_df, preserve_index=False), val_dir / \"nodes.parquet\")\n",
    "        pq.write_table(pa.Table.from_pandas(val_edges_df, preserve_index=False), val_dir / \"edges.parquet\")\n",
    "        np.save(val_dir / \"image_embeds.npy\", val_img_embeds)\n",
    "        np.save(val_dir / \"text_embeds.npy\", val_txt_embeds)\n",
    "        \n",
    "        logging.info(\"æ‰€æœ‰æ–‡ä»¶å·²æˆåŠŸå†™å…¥ç£ç›˜ã€‚\")\n",
    "    else:\n",
    "        logging.warning(\"å½“å‰ä¸ºé¢„æ¼”æ¨¡å¼ (DRYRUN=True)ï¼Œæœªæ‰§è¡Œä»»ä½•å†™ç›˜æ“ä½œã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54be59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_split(output_dir: Path):\n",
    "    \"\"\"\n",
    "    åŠ è½½æ‹†åˆ†åçš„æ•°æ®å¹¶æ‰§è¡Œä¸€ç³»åˆ—æ£€æŸ¥ä»¥ç¡®ä¿å…¶å®Œæ•´æ€§å’Œæ­£ç¡®æ€§ã€‚\n",
    "    \"\"\"\n",
    "    logging.info(\"--- å¼€å§‹éªŒè¯æ‹†åˆ†åçš„æ•°æ®é›† ---\")\n",
    "    train_dir = output_dir / \"train\"\n",
    "    val_dir = output_dir / \"val\"\n",
    "    \n",
    "    if not train_dir.exists() or not val_dir.exists():\n",
    "        logging.error(\"éªŒè¯å¤±è´¥: è¾“å‡ºç›®å½• train/ æˆ– val/ ä¸å­˜åœ¨ã€‚\")\n",
    "        return\n",
    "\n",
    "    # åŠ è½½æ•°æ®\n",
    "    train_nodes = pd.read_parquet(train_dir / \"nodes.parquet\")\n",
    "    val_nodes = pd.read_parquet(val_dir / \"nodes.parquet\")\n",
    "    train_edges = pd.read_parquet(train_dir / \"edges.parquet\")\n",
    "    val_edges = pd.read_parquet(val_dir / \"edges.parquet\")\n",
    "    train_img_embeds = np.load(train_dir / \"image_embeds.npy\")\n",
    "    val_img_embeds = np.load(val_dir / \"image_embeds.npy\")\n",
    "\n",
    "    # æ£€æŸ¥ 1: æ•°æ®æ³„éœ² (æœ€é‡è¦ï¼)\n",
    "    train_samples = set(train_nodes['sample_id'].unique())\n",
    "    val_samples = set(val_nodes['sample_id'].unique())\n",
    "    common_samples = train_samples.intersection(val_samples)\n",
    "    \n",
    "    assert not common_samples, f\"éªŒè¯å¤±è´¥ï¼šå‘ç°æ•°æ®æ³„éœ²ï¼ä»¥ä¸‹ sample_id åŒæ—¶å­˜åœ¨äºè®­ç»ƒé›†å’ŒéªŒè¯é›†: {common_samples}\"\n",
    "    logging.info(\"âœ… PASSED: è®­ç»ƒé›†ä¸éªŒè¯é›†ä¹‹é—´æ— æ ·æœ¬IDäº¤é›† (æ— æ•°æ®æ³„éœ²)ã€‚\")\n",
    "    \n",
    "    # æ£€æŸ¥ 2: å†…éƒ¨ä¸€è‡´æ€§\n",
    "    assert len(train_nodes) == train_img_embeds.shape[0], \"è®­ç»ƒé›†èŠ‚ç‚¹æ•°ä¸åµŒå…¥æ•°ä¸åŒ¹é…ã€‚\"\n",
    "    assert len(val_nodes) == val_img_embeds.shape[0], \"éªŒè¯é›†èŠ‚ç‚¹æ•°ä¸åµŒå…¥æ•°ä¸åŒ¹é…ã€‚\"\n",
    "    logging.info(\"âœ… PASSED: èŠ‚ç‚¹æ•°ä¸åµŒå…¥æ•°åœ¨å„å­é›†ä¸­ä¿æŒä¸€è‡´ã€‚\")\n",
    "\n",
    "    # æ£€æŸ¥ 3: è¾¹å¼•ç”¨å®Œæ•´æ€§\n",
    "    train_tile_ids = set(train_nodes['tile_id'])\n",
    "    val_tile_ids = set(val_nodes['tile_id'])\n",
    "    assert train_edges['src_tile_id'].isin(train_tile_ids).all(), \"è®­ç»ƒé›†è¾¹è¡¨åŒ…å«æ— æ•ˆçš„æºèŠ‚ç‚¹IDã€‚\"\n",
    "    assert train_edges['nbr_tile_id'].isin(train_tile_ids).all(), \"è®­ç»ƒé›†è¾¹è¡¨åŒ…å«æ— æ•ˆçš„é‚»å±…èŠ‚ç‚¹IDã€‚\"\n",
    "    assert val_edges['src_tile_id'].isin(val_tile_ids).all(), \"éªŒè¯é›†è¾¹è¡¨åŒ…å«æ— æ•ˆçš„æºèŠ‚ç‚¹IDã€‚\"\n",
    "    assert val_edges['nbr_tile_id'].isin(val_tile_ids).all(), \"éªŒè¯é›†è¾¹è¡¨åŒ…å«æ— æ•ˆçš„é‚»å±…èŠ‚ç‚¹IDã€‚\"\n",
    "    logging.info(\"âœ… PASSED: è¾¹è¡¨çš„å¼•ç”¨å®Œæ•´æ€§åœ¨å„å­é›†ä¸­å¾—åˆ°ä¿æŒã€‚\")\n",
    "    \n",
    "    logging.info(\"ğŸ‰ å…¨éƒ¨éªŒè¯æˆåŠŸï¼æ•°æ®é›†å·²å‡†å¤‡å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fd6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:49:58,528 - INFO - ä»æ¸…å•ä¸­ç¡®å®šäº† 44 ä¸ªç”¨äºéªŒè¯çš„ç‹¬ç«‹æ ·æœ¬ IDã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- ä¸»æµç¨‹ ---\n",
    "# 1. ä»åŸå§‹æ¸…å•ä¸­è·å–æƒå¨çš„éªŒè¯é›† sample_id åˆ—è¡¨\n",
    "validation_sample_ids = get_validation_sample_ids(ORIGINAL_MANIFEST_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90e5bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPA119',\n",
       " 'SPA120',\n",
       " 'SPA121',\n",
       " 'SPA122',\n",
       " 'SPA123',\n",
       " 'SPA124',\n",
       " 'SPA125',\n",
       " 'SPA126',\n",
       " 'SPA127',\n",
       " 'SPA128',\n",
       " 'SPA129',\n",
       " 'SPA130',\n",
       " 'SPA131',\n",
       " 'SPA132',\n",
       " 'SPA133',\n",
       " 'SPA134',\n",
       " 'SPA135',\n",
       " 'SPA136',\n",
       " 'SPA137',\n",
       " 'SPA138',\n",
       " 'SPA139',\n",
       " 'SPA140',\n",
       " 'SPA141',\n",
       " 'SPA142',\n",
       " 'SPA143',\n",
       " 'SPA144',\n",
       " 'SPA145',\n",
       " 'SPA146',\n",
       " 'SPA147',\n",
       " 'SPA148',\n",
       " 'SPA149',\n",
       " 'SPA150',\n",
       " 'SPA151',\n",
       " 'SPA152',\n",
       " 'SPA153',\n",
       " 'SPA154',\n",
       " 'TENX147',\n",
       " 'TENX148',\n",
       " 'TENX149',\n",
       " 'TENX152',\n",
       " 'TENX153',\n",
       " 'TENX154',\n",
       " 'TENX155',\n",
       " 'TENX156'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c6b61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:50:08,787 - INFO - å¼€å§‹ä» '/cwStorage/nodecw_group/jijh/yuanspace_data/artifacts' åŠ è½½æ•°æ®...\n",
      "2025-09-13 21:50:24,688 - INFO - æ‰€æœ‰è¾“å…¥æ–‡ä»¶åŠ è½½å®Œæ¯•ã€‚\n",
      "2025-09-13 21:50:28,168 - INFO - --- æ‹†åˆ†ç»Ÿè®¡ ---\n",
      "2025-09-13 21:50:28,171 - INFO - è®­ç»ƒé›†: 919173 tiles, 5515038 edges\n",
      "2025-09-13 21:50:28,172 - INFO - éªŒè¯é›†: 33735 tiles, 202410 edges\n",
      "2025-09-13 21:50:28,173 - INFO - --------------------\n",
      "2025-09-13 21:50:28,177 - INFO - æ­£åœ¨å‘ '/cwStorage/nodecw_group/jijh/yuanspace_data/dataset_split/train' å†™å…¥è®­ç»ƒé›†æ–‡ä»¶...\n",
      "2025-09-13 21:50:50,925 - INFO - æ­£åœ¨å‘ '/cwStorage/nodecw_group/jijh/yuanspace_data/dataset_split/val' å†™å…¥éªŒè¯é›†æ–‡ä»¶...\n",
      "2025-09-13 21:50:51,906 - INFO - æ‰€æœ‰æ–‡ä»¶å·²æˆåŠŸå†™å…¥ç£ç›˜ã€‚\n",
      "2025-09-13 21:50:52,976 - INFO - --- å¼€å§‹éªŒè¯æ‹†åˆ†åçš„æ•°æ®é›† ---\n",
      "2025-09-13 21:50:56,651 - INFO - âœ… PASSED: è®­ç»ƒé›†ä¸éªŒè¯é›†ä¹‹é—´æ— æ ·æœ¬IDäº¤é›† (æ— æ•°æ®æ³„éœ²)ã€‚\n",
      "2025-09-13 21:50:56,652 - INFO - âœ… PASSED: èŠ‚ç‚¹æ•°ä¸åµŒå…¥æ•°åœ¨å„å­é›†ä¸­ä¿æŒä¸€è‡´ã€‚\n",
      "2025-09-13 21:50:57,073 - INFO - âœ… PASSED: è¾¹è¡¨çš„å¼•ç”¨å®Œæ•´æ€§åœ¨å„å­é›†ä¸­å¾—åˆ°ä¿æŒã€‚\n",
      "2025-09-13 21:50:57,075 - INFO - ğŸ‰ å…¨éƒ¨éªŒè¯æˆåŠŸï¼æ•°æ®é›†å·²å‡†å¤‡å°±ç»ªã€‚\n"
     ]
    }
   ],
   "source": [
    "# 2. æ‰§è¡Œæ‹†åˆ†å’Œä¿å­˜\n",
    "split_and_save_dataset(\n",
    "    input_dir=INPUT_ARTIFACTS_DIR,\n",
    "    output_dir=OUTPUT_SPLIT_DIR,\n",
    "    validation_sample_ids=validation_sample_ids\n",
    ")\n",
    "\n",
    "# 3. è¿è¡ŒéªŒè¯è„šæœ¬\n",
    "if not DRYRUN:\n",
    "    validate_split(OUTPUT_SPLIT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400c47f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spatial_clip)",
   "language": "python",
   "name": "spatial_clip"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
